# 연구노트

---

## Week 01 - 교수님 미팅

# 1주차 상담내용 (09/02)

- 문서보다는 코드를 위주로 보는 것이 낫다.
- 리눅스 커널은 요구사항이 너무 많으므로 (특히 디바이스 드라이버), 원하면 교수님께서 미니멀 커널을 제공해 주실것이다.
- 하지만 리눅스 커널보다는 **xv6**나 pintos를 사용 하는 것이 좋다.
- xv6는 박스(Bochs)로 실행할 수 있고, pintos는 qemu로 실행 가능하다. 다만, 꼭 정해져 있는 건 아니기에 바꿔서 써도 괜찮다.
- 우선 xv6을 박스로 컴파일해보자. 인터넷에 검색하면 방법이 잘 나올 것이다. 잘 된다면, 이제 내가 만든 VMM을 이용해 해보고 문제 없으면 되는거다.
- 중간/기말, 소프트콘 제출 등을 고려하면 실질적인 개발 주수는 12주 남짓이다. 따라서 시간이 날 때 달려야 한다.
- 연구노트 작성은 미리미리. 매주 작성하고 꼭 끝에 todo를 작성해서 다음주의 목표를 정하자.
- qemu 코드는 너무 크고, architecture independent이기 때문에 코드가 방대하다. 따라서 qemu보다는 Bochs의 코드를 참고하는 것이 가장 좋은 방법이다.
- 교수님께 질문/도움요청은 슬랙으로.
- 러스트도 공부해보자. 나중에 끝날때쯤 교수님께 러스트로 만들어진 커널을 받아서 돌리는 것도 목표 중 하나로 좋을듯.

## Week 01 - 연구 진행 내용

# 1주차 연구내용

목표: 문서 수집 및 학습

## 연구 내용

이번주 목표는 '문서 수집 및 학습'입니다. 하지만 바로 문서만 읽으면 구체적으로 어떤 부분이 부족한지, 뭘 더 배워야 할 지 모를 것 같다는 생각이 들었습니다.

따라서 교수님께서 제안해주신 xv6 체험을 먼저 진행하며, 과정 중에 생기는 구체적인 궁금증들을 해결하는 과정으로 진행하기로 결정했습니다.

### xv6

우선 xv6는 [github](https://github.com/mit-pdos/xv6-public)에 소스 코드가 공개되어 있습니다. 그러나 README.md를 통해 할 수 있듯, 약 5년 전에 x86 버전은 개발이 중단되었고 현재에는 RISC-V 버전만 개발되고 있습니다. 그럼에도 제 프로젝트의 목표는 KVM을 통해 AMD64 환경에서의 가상화를 구현하는 것이기에, x86 코드를 그대로 활용하기로 결정했습니다.

README.md의 'BUILDING AND RUNNING XV6' 부분을 보면, 컴파일과 부팅을 하는 방법이 명시되어 있습니다. x86 크로스컴파일이 가능한 gcc가 설치되어 있는 환경에서 `make` 명령어를 실행하면 컴파일이 되고, `make qemu`를 하면 QEMU 환경에서 실행된다고 써 있습니다.

다만, 코드가 너무 방대한 QEMU보다는 교수님께서 제안하신 Bochs를 레퍼런스로 공부하고 싶기에, `make bochs`로 부팅하면 될 것 같습니다.

#### 컴파일

우선 xv6-public 레포를 로컬로 클론했습니다.

```bash
git clone https://github.com/mit-pdos/xv6-public
```

이후, 폴더에 들어간 후 `make`를 실행했으나 다음과 같은 에러가 발생했습니다:

```
In function ‘mpconfig’,
    inlined from ‘mpinit’ at mp.c:101:14:
mp.c:83:10: error: array subscript -48806446 is outside array bounds of ‘void[2147483647]’ [-Werror=array-bounds=]
   83 |   if(conf->version != 1 && conf->version != 4)
      |      ~~~~^~~~~~~~~
mp.c:78:34: note: at offset -2147483648 into object ‘*<unknown>.physaddr’ of size [0, 2147483647]
   78 |   if((mp = mpsearch()) == 0 || mp->physaddr == 0)
      |                                ~~^~~~~~~~~~
mp.c:85:28: error: array subscript -48806446 is outside array bounds of ‘void[2147483647]’ [-Werror=array-bounds=]
   85 |   if(sum((uchar*)conf, conf->length) != 0)
      |                        ~~~~^~~~~~~~
mp.c:78:34: note: at offset -2147483648 into object ‘*<unknown>.physaddr’ of size [0, 2147483647]
   78 |   if((mp = mpsearch()) == 0 || mp->physaddr == 0)
      |                                ~~^~~~~~~~~~
mp.c: In function ‘mpinit’:
mp.c:104:22: error: array subscript -48806446 is outside array bounds of ‘struct mpconf[48806446]’ [-Werror=array-bounds=]
  104 |   lapic = (uint*)conf->lapicaddr;
      |                      ^~
In function ‘mpconfig’,
    inlined from ‘mpinit’ at mp.c:101:14:
mp.c:78:34: note: at offset -2147483648 into object ‘*<unknown>.physaddr’ of size [0, 2147483647]
   78 |   if((mp = mpsearch()) == 0 || mp->physaddr == 0)
      |                                ~~^~~~~~~~~~
mp.c: In function ‘mpinit’:
mp.c:105:46: error: array subscript -48806446 is outside array bounds of ‘struct mpconf[48806446]’ [-Werror=array-bounds=]
  105 |   for(p=(uchar*)(conf+1), e=(uchar*)conf+conf->length; p<e; ){
      |                                              ^~
In function ‘mpconfig’,
    inlined from ‘mpinit’ at mp.c:101:14:
mp.c:78:34: note: at offset -2147483648 into object ‘*<unknown>.physaddr’ of size [0, 2147483647]
   78 |   if((mp = mpsearch()) == 0 || mp->physaddr == 0)
      |                                ~~^~~~~~~~~~
In function ‘sum’,
    inlined from ‘mpconfig’ at mp.c:85:6,
    inlined from ‘mpinit’ at mp.c:101:14:
mp.c:25:16: error: array subscript [-2147483648, -2147418114] is outside array bounds of ‘void[2147483647]’ [-Werror=array-bounds=]
   25 |     sum += addr[i];
      |            ~~~~^~~
In function ‘mpconfig’,
    inlined from ‘mpinit’ at mp.c:101:14:
mp.c:78:34: note: at offset [-2147483648, -2147418114] into object ‘*<unknown>.physaddr’ of size [0, 2147483647]
   78 |   if((mp = mpsearch()) == 0 || mp->physaddr == 0)
      |                                ~~^~~~~~~~~~
cc1: all warnings being treated as errors
make: *** [<내장>: mp.o] 오류 1
```

에러 코드를 읽어보니, 자료형 변환 부분에서 경고가 발생하는 것 같았습니다. 그리고 `cc1: all warnings being treated as errors`라는 라인을 보았을 때, `-Werror` 플래그가 모든 경고를 에러로 받아들이고 있다는 사실을 알 수 있었습니다.

(이 부분이 실제 코드의 결함일 수도 있고, 제 컴파일러의 버전이 맞지 않는 것일 수도 있습니다. 실제로 코드에 결함이 있다면, 따로 레포를 포크해 개선할 수 있다면 더 좋을 것 같습니다.)
(제 환경은 Fedora 42이고, GCC 버전은 15.2.1 입니다.)

그래서 Makefile을 수정해 `-Werror` 부분을 삭제하고, `make clean` 이후 다시 컴파일했더니, 컴파일 되었습니다. (정상적으로 된 건지는 잘 모르곘습니다.)

#### 부팅

이후 `make bochs`로 부팅해보았으나, 다음과 같은 에러와 함께 실패했습니다:

```
========================================================================
                        Bochs x86 Emulator 3.0
            Built from GitHub snapshot on February 16, 2025
                Timestamp: Sun Feb 16 10:00:00 CET 2025
========================================================================
00000000000i[      ] BXSHARE not set. using compile time default '/usr/share/bochs'
00000000000i[      ] reading configuration from .bochsrc
00000000000p[      ] >>PANIC<< .bochsrc:497: directive 'vga_update_interval' not understood
00000000000e[SIM   ] notify called, but no bxevent_callback function is registered
00000000000e[SIM   ] notify called, but no bxevent_callback function is registered
========================================================================
Bochs is exiting with the following message:
[      ] .bochsrc:497: directive 'vga_update_interval' not understood
========================================================================
00000000000i[SIM   ] quit_sim called with exit code 1
make: *** [Makefile:211: bochs] 오류 1
```

다시 확인해보니, 그냥 bochs를 실행해도 다음과 같은 에러가 뜹니다. .bochsrc 파일이 설정되지 않아서 생기는 문제인 것 같습니다.

([Notes 파일](https://github.com/mit-pdos/xv6-public/blob/master/Notes)에 bochs 관련 정보들이 있는데, 아직은 bochs를 잘 몰라서 어떻게 설정해야 하는지 잘 모르겠습니다.)

따라서, 우선 bochs가 제대로 설정되기 전까지는 xv6가 정상적으로 컴파일되었는지 확인하기 위해 QEMU를 사용하기로 결정했습니다.

`make qemu`, `make qemu-nox`로도 테스트해봤으나, 별다른 에러 메시지 없이, 다음 출력 외에는 몇분동안 아무 반응이 없었습니다.

```
SeaBIOS (version 1.17.0-5.fc42)


iPXE (https://ipxe.org) 00:03.0 CA00 PCI2.10 PnP PMM+1EFCAEC0+1EF0AEC0 CA00



Booting from Hard Disk..
```

컴파일 과정의 문제로 의심이 되어, 검색을 통해 정보를 찾아봤습니다. 그 결과, 한 [Youtube 영상](https://www.youtube.com/watch?v=TLiV_sK77jg)을 발견했습니다.

이 영상에서는 lubuntu 12.04 32bit를 사용하지만, 다운로드 링크가 만료되어있습니다.

64bit 환경인 게 이 문제의 원인이라면, 영상처럼 32bit OS를 활용하면 가능할 것이므로, 데스크탑에 데비안 12를 기반으로 하는 LMDE 6 가상환경을 구성했습니다. (CPU: `kvm32`로 설정해야 virtio 인터넷 연결이 가능합니다.)

이후, 필요한 패키지들을 설치하고 xv6 레포를 클론했습니다.

```bash
sudo apt install git qemu-system qemu-user bochs gcc gcc-multilib build-essential make vim
git clone https://github.com/mit-pdos/xv6-public
```

바로 `make`를 돌려봤습니다.

![alt text](image.png)

그 결과, 똑같이 make 과정에서 경고(에러)가 발생했습니다. 이를 보아, xv6 코드 자체의 문제인 것 같습니다.

앞선 해결방법과 똑같이, Makefile에서 `-Werror` 플래그를 삭제하고 `make`로 컴파일한 후, `make qemu`로 실행해보았습니다.

![alt text](image-1.png)

잘 실행되는 모습을 볼 수 있었습니다.

![alt text](image-2.png)

`make bochs`는 여전히 작동하지 않았습니다 (앞서 확인했듯이, .bochsrc가 설정되지 않았습니다.)

#### Fedora 42(x64)에서는 작동하지 않았는데, LMDE 6(x86)에서는 작동한 이유가 무엇인지에 대한 분석

##### 가설 1. 컴파일러의 문제이다.

만약 컴파일러가 문제였다면, LMDE에서 컴파일한 이미지를 Fedora로 넘겨서 qemu로 돌렸을 때 정상적으로 실행되어야 합니다.

따라서, LMDE에서 `make`로 컴파일한 파일을 Fedora로 복사해 `make qemu`로 부팅해봤습니다.

![alt text](image-3.png)

그 결과, qemu로 정상적으로 부팅되는 모습을 볼 수 있었습니다. 이를 통해, 컴파일 과정에서 문제가 발생했음을 확인할 수 있었습니다. 따라서 가설 1이 맞다는 것을 검증할 수 있었습니다.

##### 그러면 Fedora의 컴파일러는 뭐가 잘못된 것이었을까?

우선, [xv6 레포](https://github.com/mit-pdos/xv6-public)의 README에는 다음과 같이 쓰여 있습니다.

```
BUILDING AND RUNNING XV6

To build xv6 on an x86 ELF machine (like Linux or FreeBSD), run
"make". On non-x86 or non-ELF machines (like OS X, even on x86), you
will need to install a cross-compiler gcc suite capable of producing
x86 ELF binaries (see https://pdos.csail.mit.edu/6.828/).
Then run "make TOOLPREFIX=i386-jos-elf-". Now install the QEMU PC
simulator and run "make qemu".
```

그러니까 x86 ELF 환경에서는 그냥 `make`로 컴파일이 되고, 그게 아닌 경우에는 크로스컴파일러를 설정한 후 `make TOOLPREFIX=i386-jos-elf-`를 하면 된다는 것 같습니다. 여기서, "non-x86"에 x86-64가 포함되는건지 아닌건지는 잘 모르겠습니다. 우선 x86-64가 non-x86이 맞다고 가정하고 문제를 해결해보려 했습니다.

##### Fedora에 gcc-multilib 설치

우선 dnf search gcc로 관련 패키지들을 검색해봤습니다.

그러나 어떤게 gcc-multilib에 해당하는지 알 수 없어, 검색을 통해 정보를 찾던 도중 [스택오버플로우 글](https://stackoverflow.com/questions/65863241/gcc-multilib-for-fedora)을 발견해, 다음과 같은 명령어로 제안하는 패키지들을 설치해보았습니다.

```bash
sudo dnf install glibc-devel.i686 libstdc++-devel.i686
```

이후 `make`, `make qemu`를 했으나 여전히 결과는 같았습니다. 글에서 `-static.i686` 접미사 달린 패키지들도 설치하라는 말을 발견했습니다. 그것이 효과가 있을지 보기 위해 `Makefile`을 보니 정말 `-static` 컴파일 옵션이 있는 걸 볼 수 있었습니다. 따라서 "정적 링킹을 위한 glibc 파일이 없어서 문제가 발생했다" 라는 추론을 할 수 있었습니다. 따라서 `dnf search glibc`로 관련 패키지들을 전부 조회해, `glibc-static.i686`이라는 패키지를 발견해 설치했습니다. 그러나, 여전히 문제가 해결되지는 않았습니다.

만약 64비트인게 문제가 아니라 그냥 Fedora인게 문제라면, 추후 distrobox를 활용해 데비안/우분투 기반으로 다시 컴파일 했을 때 될 수도 있겠습니다.

#### 컴파일에 관한 결정

문제가 아직 해결되지 않아서 마음이 불편한 부분이 있지만, 우선 LMDE에서 컴파일한 파일이 있으므로 당분간은 그걸 계속 사용하기로 결정했습니다. 따라서 복사한 레포 + 컴파일된 이미지를 .zip파일로 묶어 이 프로젝트 레포의 archive 폴더에서 관리할 수 있도록 했습니다.

### Bochs

앞서 xv6 컴파일 및 부팅 과정에서 파악한 현재의 문제는 대략 두가지입니다.

1. 컴파일 환경의 불안정성 (LMDE에서 컴파일해 해결.)
2. Bochs 사용법에 대한 이해 부족

1번은 어느정도 해결했으므로 큰 문제는 아닙니다. 이 프로젝트의 주제가 xv6같은 운영체제 그 자체가 아닌, qemu나 bochs같은 하이퍼바이저이기 때문입니다. 따라서 xv6가 실행될 수 있도록 컴파일된 현재의 상황에서는 큰 문제가 되지 않습니다.

하지만, 2번은 확실한 문제입니다. 따라서 지금 집중해야 하는 부분은 Bochs에 대한 문서를 수집하고, `.bochsrc`를 가져오거나 작성하는 것, 그리고 `make bochs`를 성공시키는 것입니다. 이를 성공하면 2주차 과정에서 구조를 설계하는 데 큰 도움이 될 것입니다.

#### 문서 수집 및 학습

주요 문서들은 메인 README.md에 정리했습니다.

우선 [GitHub](https://github.com/bochs-emu/Bochs)의 README.md를 읽어보면, Bochs는 C++로 짜여진 x86 **에뮬레이터**입니다. 즉, VMware나 Virtualbox에서 구현하는 가상화 또는 JVM같은 JIT 컴파일러와 다릅니다. 에뮬레이터라는 특성 덕분에, x86 뿐만 아니라 ARM, MIPS 등의 다양한 아키텍처 위에서도 x86 환경을 에뮬레이팅 할 수 있습니다.

제가 진행할 프로젝트는 KVM을 이용한 가상화에 초점을 맞췄기에, 이런 부분에서 Bochs와 차이점을 가집니다.

README 아래에는 베이징 ISCA-35에서 발표된, Portable VM에 관한 발표의 [PPT 파일](https://bochs.sourceforge.io/VirtNoJit.pdf)이 있습니다. 이 자료에서는 여러 가상화/에뮬레이션 기술들을 소개합니다. 대부분의 내용을 이해하지는 못했지만, 이해한 부분을 정리하자면:

- JIT나 하드웨어 기반 가상화가 아닌, 한줄씩 번역하는 인터프리터 방식을 차용한다.
  - 따라서 디버깅이 쉽고, 이식성이 좋다. 이를 통해 CPU와의 완전한 격리를 이뤄낼 수 있다.
- fetch-decode-dispatch 순서대로 명령을 처리한다.
  - Bochs는 50% 이상의 시간을 이 사이클에 소요한다.
  - decode된 명령어는 i-cache에 저장된다.
- Lazy Flag 방식을 사용한다.

#### xv6 구동하기

앞선 단계에서, `make bochs`는 에러를 출력하며 구동되지 않았습니다. 그리고 그 문제는 `.bochsrc` 파일이 없어서 그런 것 같습니다.

QEMU는 구동 옵션을 인라인으로 다 넣기 때문에 따로 설정파일을 만들지 않아도 작동한 것 같지만, Bochs는 그렇지 않아 `.bochsrc` 파일을 만들거나, 또는 누군가 만들어 놓은 설정 파일을 가져와야 할 것 같습니다.

우선 검색을 통해 다음 설정 파일을 찾았습니다: [dot-bochsrc by Thomas Nguyen](https://gitlab07.cs.washington.edu/tomn/lvisor-18wi/-/blob/master/tests/xv6-src/dot-bochsrc)

해당 파일을 복사해 그대로 .bochsrc로 만들어보았으나, 여전히 같은 에러를 출력하며 잘 작동하지 않았습니다.

추가적으로 정보를 더 찾아봤으나 마땅한 설정 파일을 찾지 못하여, 2차시에 Bochs에 대해 공부하면서 .bochsrc 또한 [공식 Documentation](https://bochs.sourceforge.io/doc/docbook/user/bochsrc.html)과 같이 공부하겠습니다.

## 다음주 todo:

- `.bochsrc` 설정 파일 세팅하기
- `Bochs`를 레퍼런스로, 실질적인 VMM 구조 설계하기
- 개발 환경 구축 (C)
- (시간이 남는다면,) Fedora에서 xv6이 컴파일되지 않은 이유 조사. (만약 Fedora가 문제라면 Distrobox를 이용해 컨테이너 위 컴파일 진행)
- (시간이 남는다면,) xv6 소스 코드를 포크하고 개선해 `-Werror` 플래그를 지우지 않고도 성공적으로 컴파일되도록 개선

---

## Week 02 - 교수님 미팅

# 2주차 상담내용 (09/09)

- 32비트 할건지, 64비트 할건지 확실히 하자.
- xv6를 사용하는 다른 학교, 학과에서 어떻게 하고 있는지 알아보자.
  - 아주대 전자과
  - 카이스트 (OSLAB 말고)
  - 서울대
  - 연세대
- 컴파일러의 컴파일 방식이 달라서 문제가 발생했을 것이다.
- 문제가 되었던 .bochsrc 파일은 dot-bochsrc라는 이름으로 xv6 레포에 있으니, 잘 수정해서 써보면 된다. 메모리, 디스크(ata) 등을 설정하는 게 중요하다.
- Paravirtualization 부분은 QEMU 코드를 보는 게 답이다.
- 그래도 Bochs가 가상 머신 자체는 만들어 줄테니, 기본 베이스를 Bochs로 잡고 CPU 구현만 QEMU 코드를 이용하면 될 것이다. `CONFIG_KVM`를 코드에서 검색해서 찾아보도록 하자.
- 컴파일 에러 해결보다는 Bochs 문제 해결을 먼저 하도록 하자.
- Bochs와 QEMU처럼, SeaBIOS를 그대로 사용하도록 하자.

## Week 02 - 연구 진행 내용

# 2주차 연구내용

목표: 구조 설계, 개발 환경 구축, Git 저장소 생성

저번주 todo:

- `.bochsrc` 설정 파일 세팅하기
- `Bochs`를 레퍼런스로, 실질적인 VMM 구조 설계하기
- 개발 환경 구축 (C)
- Fedora에서 xv6이 컴파일되지 않은 이유 조사. (만약 Fedora가 문제라면 Distrobox를 이용해 컨테이너 위 컴파일 진행)
- xv6 소스 코드를 포크하고 개선해 `-Werror` 플래그를 지우지 않고도 성공적으로 컴파일되도록 개선

## 연구 내용

이번주 목표는 '코드 구조 설계 및 개발 환경 구축'입니다. 또한 저번에 해결하지 못한 bochs 부팅까지 해결할 계획입니다.

### xv6 컴파일 문제 해결하기

#### 문제점 분석

저번주에 발생한 xv6 컴파일 관련 문제는 다음 두가지입니다.

1. 컴파일 과정 중 생기는 경고가 `-Werror` 플래그에 걸려 컴파일이 실패하는 것.
2. 컴파일 후, `make qemu`가 정상적으로 실행되지 않는것.

1번 문제의 경우에는, 저번주에 임시로 `-Werror` 플래그를 제거해서 해결했습니다. 그러나, 교수님께서 '컴파일러 버전에 따라 처리 방식이 다르다'고 조언해주셨으니, 다양한 버전으로 테스트해보는 과정이 필요할 것 같습니다.

2번 문제의 경우에는, LMDE 32bit 환경에서 컴파일해 해결했습니다. Fedora의 multilib 컴파일러가 제대로 작동하지 않은 것 같으니, 다른 배포판으로도 테스트를 해봐야겠습니다.

따라서, 1번과 2번을 동시에 해결하기 위해 다양한 배포판들을 [distrobox](https://distrobox.it/)로 테스트하기로 결정했습니다. distrobox는 도커 기반의 리눅스 컨테이너를 간단하게 생성할 수 있도록 해주는 툴으로, 권한 설정과 홈 폴더 공유와 같은 번거로운 작업을 간단하게 해결해줍니다.

#### 다른 학교의 사례 분석

무작정 배포판들을 테스트해보기보단, 우선 교수님께서 조언해주신 대로 여러 학교들의 사례를 찾아보았습니다.

- 서울대, 연세대: xv6-riscv 버전을 사용중인 것으로 확인됨 (해당없음)
- KAIST: [KAIST OSLAB](https://oslab.kaist.ac.kr/xv6-install/), 데비안 기반 (우분투로 추정됨)의 운영체제를 사용함.
- 아주대: 공개된 정보 없음.

따라서 KAIST 관련 정보로 검색하다, 다음 [블로그 글](https://velog.io/@uooee8633/M1-%EB%A7%A5%EB%B6%81%EC%97%90%EC%84%9C-XV6qemu-%EB%B9%8C%EB%93%9C%ED%95%98%EA%B8%B0)을 확인했습니다.

해당 글에서는 Ubuntu 22.04 (64bit)에서 컴파일이 성공한 것을 확인할 수 있었습니다.

따라서, distrobox로 Ubuntu 22.04 환경을 먼저 구성해보았습니다.

#### Ubuntu 22.04 테스트

우선 distrobox로 Ubuntu 22.04 이미지를 다운로드해, 컨테이너를 생성하고 실행했습니다.

```bash
distrobox create -i quay.io/toolbx/ubuntu-toolbox:22.04
distrobox enter ubuntu-toolbox-22-04
```

이후, 필요한 패키지들을 설치했습니다. `make`와 `gcc-multilib`같은 패키지들은 `build-essential` 메타패키지로 한번에 설치됩니다.

```bash
sudo apt install build-essential qemu-system -y
```

xv6 레포를 git으로 클론해줬습니다. git은 컨테이너에 기본적으로 설치되어있었습니다.

```bash
git clone https://github.com/mit-pdos/xv6-public
cd xv6-public
```

바로 `make`로 컴파일해보았습니다.

![alt text](image.png)

컴파일 에러 없이 잘 컴파일 된 것을 볼 수 있었습니다. 이로서, 앞선 1번 `-Werror` 문제는 컴파일러 버전(배포판)을 바꿔서 해결할 수 있었습니다.

따라서, 바로 `make qemu`로 실행도 해보았습니다.

![alt text](image-1.png)

문제없이 바로 실행되었습니다... 이로서 2번 문제도 해결했습니다.

그냥 처음부터 우분투 기준으로 해볼걸... 하는 생각이 들지만, 이 과정을 컴파일러 버전의 중요성을 배울 수 있었습니다.

##### xv6 fork

프로젝트 관리를 위해, xv6 레포지토리를 fork했습니다. 그리고 README에 distrobox를 이용한 컴파일 가이드를 작성했습니다. 앞으로 xv6 코드에 변경사항이 생기면 이 레포지토리로 관리할 것입니다.

[https://github.com/seolcu/xv6-public](https://github.com/seolcu/xv6-public)

### Bochs를 이용한 부팅 도전

우선 컨테이너에 bochs를 설치해줬습니다.

```bash
sudo apt install bochs
```

그리고 `make bochs`를 실행했습니다. 1주차와 같은 오류가 발생했습니다. 1주차에 교수님께서 .bochsrc 설정법을 설명해 주셨으니, 그에 따라 고쳐보려고 합니다.

![alt text](image-2.png)

우선 아래의 Makefile을 보면, 아래 부분에서 `dot-bochsrc` 파일을 가르키는 소프트링크로 `.bochsrc` 파일을 생성하는 것을 볼 수 있습니다. 따라서 `dot-bochsrc`를 수정하려 합니다.

```makefile
bochs : fs.img xv6.img
	if [ ! -e .bochsrc ]; then ln -s dot-bochsrc .bochsrc; fi
	bochs -q
```

#### 환경변수와 bochsbios

dot-bochsrc를 읽어보기 전에, 앞선 `make bochs`에서의 출력 메시지에서 환경변수 설정을 볼 수 있었습니다.

```
00000000000i[      ] LTDL_LIBRARY_PATH not set. using compile time default '/usr/lib/x86_64-linux-gnu/bochs/plugins'
========================================================================
                        Bochs x86 Emulator 2.7
              Built from SVN snapshot on August  1, 2021
                Timestamp: Sun Aug  1 10:07:00 CEST 2021
========================================================================
00000000000i[      ] BXSHARE not set. using compile time default '/usr/share/bochs'
```

`LTDL_LIBRARY_PATH`은 `/usr/lib/x86_64-linux-gnu/bochs/plugins`로 설정되고,

`BXSHARE`는 `/usr/share/bochs`로 설정되는 것을 볼 수 있었습니다.

컨테이너에서 확인하니, 두 경로 모두 정상적으로 존재하는 것을 볼 수 있었습니다.

그러나, dot-bochsrc에서 다음과 같은 라인을 발견할 수 있었습니다.

```
romimage: file=$BXSHARE/BIOS-bochs-latest
```

이를 보면, `/usr/share/bochs/BIOS-bochs-latest`라는 파일이 존재해야 하는 것을 알 수 있습니다. 그러나, 실제로는 해당 위치에 파일이 존재하지 않았습니다. 따라서 `apt search bochs`를 하니, 몇몇 관련 패키지들이 설치되지 않았다는 사실을 알 수 있었습니다.

따라서 `sudo apt install bochsbios`를 진행하니, 해당 위치에 BIOS-bochs-latest 파일이 정상적으로 생겨났습니다.

비슷하게, `bochs-term`, `vgabios` 등도 설치가 필요했습니다.

#### Ubuntu의 Bochs 패키지로 실행 실패

앞선 패키지들을 모두 설치했음에도, 다음과 같은 에러가 계속 발생했습니다.

```
00000000000p[      ] >>PANIC<< .bochsrc:497: directive 'vga_update_interval' not understood
00000000000e[SIM   ] notify called, but no bxevent_callback function is registered
00000000000e[SIM   ] notify called, but no bxevent_callback function is registered
========================================================================
Bochs is exiting with the following message:
[      ] .bochsrc:497: directive 'vga_update_interval' not understood
========================================================================
00000000000i[SIM   ] quit_sim called with exit code 1
make: *** [Makefile:211: bochs] Error 1
```

에러에 해당하는 부분을 전부 주석처리해보았으나, 메모리 부분에서 계속 커널 패닉이 발생하며 실행에 실패했습니다.

따라서, 잠시 멈춰서 문제의 원인을 떠올려봤습니다.

생각해보니, xv6의 코드는 약 15년 전에 작성되었으므로, 그 당시의 Bochs의 설정 파일 형식과 현재의 설정 파일 형식이 믾이 다르겠다는 생각이 들었습니다.

그래서 16.04 컨테이너를 새로 만들어 해봤으나, 여전히 같은 문제에 부딪혔습니다.

그러다, 1주차에서 보았던 Notes 파일이 떠올랐습니다.

#### Bochs 2.2.6

xv6 레포에는 Notes라는 파일이 있고, 그 파일 안에는 이런 내용이 있습니다.

```
bochs 2.2.6:
./configure --enable-smp --enable-disasm --enable-debugger --enable-all-optimizations --enable-4meg-pages --enable-global-pages --enable-pae --disable-reset-on-triple-fault
bochs CVS after 2.2.6:
./configure --enable-smp --enable-disasm --enable-debugger --enable-all-optimizations --enable-4meg-pages --enable-global-pages --enable-pae
```

따라서, 이를 참고해 2.2.6 버전을 사용해보기로 했습니다.

해당 버전은 GitHub에는 존재하지 않아서, [Sourceforge](https://sourceforge.net/projects/bochs/files/bochs/)에서 [Bochs 2.2.6](https://master.dl.sourceforge.net/project/bochs/bochs/2.2.6/bochs-2.2.6.tar.gz)을 다운받을 수 있었습니다.

```bash
wget https://master.dl.sourceforge.net/project/bochs/bochs/2.2.6/bochs-2.2.6.tar.gz
tar -xvf bochs-2.2.6.tar.gz
rm bochs-2.2.6.tar.gz
cd bochs-2.2.6
```

그대로 `./configure ...`를 하니, 다음과 같은 에러가 발생했습니다.

```
ERROR: X windows gui was selected, but X windows libraries were not found.
```

Distrobox 컨테이너는 minimal이기에 xorg는 기본적으로 설치되지 않습니다. 따라서, `xorg-dev` 패키지를 설치해 라이브러리를 충족해주고, `./configure ...`를 실행했습니다.

```bash
sudo apt install xorg-dev
./configure --enable-smp --enable-disasm --enable-debugger --enable-all-optimizations --enable-4meg-pages --enable-global-pages --enable-pae --disable-reset-on-triple-fault
```

`./configure ...`는 성공적으로 진행되었습니다.

![alt text](image-3.png)

하지만, `make` 단계에서 컴파일 에러가 발생해 또 막히게 되었습니다.

![alt text](image-4.png)

컴파일러 버전 문제를 의심하여, Ubuntu 16.04에서 다시 해 보았으나 여전히 같은 문제가 발생했습니다.

따라서 우선 이 문제는 보류해두고, 오늘의 주 목표인 코드 구조 설계 및 개발 환경 구축을 진행했습니다.

### 코드 구조 설계

Bochs의 코드를 참고해 코드를 작성할 것이므로, 우선 Bochs 2.2.6의 코드를 읽어보았습니다.

Bochs 코드를 컴파일해 실행해보지는 못해도, 전체적인 로직을 보는 것은 가능하겠다고 생각했습니다.

우선 그 전에, Bochs 2.2.6 코드를 온라인으로 볼 수 있는 곳이 적어, 편한 브라우징과 기록을 위해 코드를 [GitHub](https://github.com/seolcu/bochs-2.2.6)에 올렸습니다. (LGPL 라이선스 파일도 같이 올렸으니 괜찮을 것 같습니다.)

#### `main.cc`

우선 코드의 가장 중심인 `main.cc`를 살펴봤습니다.

그 중 `main` 함수는 아래와 같습니다:

```C
int main (int argc, char *argv[])
{
  bx_startup_flags.argc = argc;
  bx_startup_flags.argv = argv;
#if BX_WITH_SDL && defined(WIN32)
  // if SDL/win32, try to create a console window.
  RedirectIOToConsole ();
#endif
#if defined(WIN32)
  SetConsoleTitle("Bochs for Windows - Console");
#endif
  return bxmain ();
}
```

처음에 입력하는 인자를 `bx_startup_flags`라는 구조체 인스턴스에 저장하는 것을 볼 수 있습니다. 해당 인스턴스는 전역변수로 선언되어있습니다. 이후 `bxmain` 함수를 실행합니다.

이어지는 `bxmain` 함수는 다음과 같습니다.

```C
int bxmain () {
#ifdef HAVE_LOCALE_H
  // Initialize locale (for isprint() and other functions)
  setlocale (LC_ALL, "");
#endif
  bx_user_quit = 0;
  bx_init_siminterface ();   // create the SIM object
  static jmp_buf context;
  if (setjmp (context) == 0) {
    SIM->set_quit_context (&context);
    if (bx_init_main (bx_startup_flags.argc, bx_startup_flags.argv) < 0)
      return 0;
    // read a param to decide which config interface to start.
    // If one exists, start it.  If not, just begin.
    bx_param_enum_c *ci_param = SIM->get_param_enum (BXP_SEL_CONFIG_INTERFACE);
    char *ci_name = ci_param->get_choice (ci_param->get());
    if (!strcmp(ci_name, "textconfig")) {
#if BX_USE_TEXTCONFIG
      init_text_config_interface ();   // in textconfig.h
#else
      BX_PANIC(("configuration interface 'textconfig' not present"));
#endif
    }
#if BX_WITH_WX
    else if (!strcmp(ci_name, "wx")) {
      PLUG_load_plugin(wx, PLUGTYPE_CORE);
    }
#endif
    else {
      BX_PANIC (("unsupported configuration interface '%s'", ci_name));
    }
    int status = SIM->configuration_interface (ci_name, CI_START);
    if (status == CI_ERR_NO_TEXT_CONSOLE)
      BX_PANIC (("Bochs needed the text console, but it was not usable"));
    // user quit the config interface, so just quit
  } else {
    // quit via longjmp
  }
  SIM->set_quit_context (NULL);
#if defined(WIN32)
  if (!bx_user_quit) {
    // ask user to press ENTER before exiting, so that they can read messages
    // before the console window is closed. This isn't necessary after pressing
    // the power button.
    fprintf (stderr, "\nBochs is exiting. Press ENTER when you're ready to close this window.\n");
    char buf[16];
    fgets (buf, sizeof(buf), stdin);
  }
#endif
  return SIM->get_exit_code ();
}
```

초반부에서 `bx_init_siminterface` 함수를 실행해 시뮬레이터를 세팅합니다. 이와 관련한 주석을 `gui/siminterface.cc`에서 발견할 수 있었습니다.

```C
bx_simulator_interface_c *SIM = NULL;
logfunctions *siminterface_log = NULL;
#define LOG_THIS siminterface_log->

// bx_simulator_interface just defines the interface that the Bochs simulator
// and the gui will use to talk to each other.  None of the methods of
// bx_simulator_interface are implemented; they are all virtual.  The
// bx_real_sim_c class is a child of bx_simulator_interface_c, and it
// implements all the methods.  The idea is that a gui needs to know only
// definition of bx_simulator_interface to talk to Bochs.  The gui should
// not need to include bochs.h.
//
// I made this separation to ensure that all guis use the siminterface to do
// access bochs internals, instead of accessing things like
// bx_keyboard.s.internal_buffer[4] (or whatever) directly. -Bryce
//
```

이를 읽어보면, `bx_simulator_interface`라는 구조체는 Bochs와 GUI의 통신을 담당하는 인터페이스라고 합니다. 그래서 기본적으로 `bx_simulator_interface` 구조체에는 실제 메서드가 구현되어 있지 않고, 비어있습니다.
실제 메서드들은 `bx_real_sim_c`라는 클래스를 통해 구현되는데, 이를 생성하는 과정은 방금 본 `bx_init_siminterface` 함수에서 발견할 수 있습니다. `bx_init_siminterface` 함수는 아래와 같습니다.

```C
void bx_init_siminterface ()
{
  siminterface_log = new logfunctions ();
  siminterface_log->put ("CTRL");
  siminterface_log->settype(CTRLLOG);
  if (SIM == NULL)
    SIM = new bx_real_sim_c();
}
```

이를 통해 보면, `bxmain` 함수에서 `bx_init_siminterface`를 실행하는 순간부터 `SIM` 인스턴스가 실제로 사용 가능한 형태가 된다는 사실을 알 수 있습니다.

그 아래를 보면 `bx_init_main` 함수를 실행하는 것을 볼 수 있습니다.

```C
    if (bx_init_main (bx_startup_flags.argc, bx_startup_flags.argv) < 0)
      return 0;
```

리턴값이 0보다 작은 경우에 0을 리턴하는 것을 보아, `bx_init_main` 함수에서 에러가 발생하면 즉시 종료하는 것 같습니다.

따라서 `bx_init_main` 함수를 살펴보기로 했습니다.

```C
  int norcfile = 1;

  if (load_rcfile) {
    /* parse configuration file and command line arguments */
#ifdef WIN32
    int length;
    if (bochsrc_filename != NULL) {
      lstrcpy(bx_startup_flags.initial_dir, bochsrc_filename);
      length = lstrlen(bx_startup_flags.initial_dir);
      while ((length > 1) && (bx_startup_flags.initial_dir[length-1] != 92)) length--;
      bx_startup_flags.initial_dir[length] = 0;
    } else {
      bx_startup_flags.initial_dir[0] = 0;
    }
#endif
    if (bochsrc_filename == NULL) bochsrc_filename = bx_find_bochsrc ();
    if (bochsrc_filename)
      norcfile = bx_read_configuration (bochsrc_filename);
  }

  if (norcfile) {
    // No configuration was loaded, so the current settings are unusable.
    // Switch off quick start so that we will drop into the configuration
    // interface.
    if (SIM->get_param_enum(BXP_BOCHS_START)->get() == BX_QUICK_START) {
      if (!SIM->test_for_text_console ())
        BX_PANIC(("Unable to start Bochs without a bochsrc.txt and without a text console"));
      else
        BX_ERROR (("Switching off quick start, because no configuration file was found."));
    }
    SIM->get_param_enum(BXP_BOCHS_START)->set (BX_LOAD_START);
  }

  // parse the rest of the command line.  This is done after reading the
  // configuration file so that the command line arguments can override
  // the settings from the file.
  if (bx_parse_cmdline (arg, argc, argv)) {
    BX_PANIC(("There were errors while parsing the command line"));
    return -1;
  }
  // initialize plugin system. This must happen before we attempt to
  // load any modules.
  plugin_startup();
  return 0;
}
```

코드가 길어서 조금씩 나눠서 보기로 했습니다. 앞부분에서는 옵션 설정 등 기본적인 init 과정을 거칩니다.

주목한 점은 끝부분에서 `.bochsrc`를 처리한다는 점입니다. 이 과정을 담당하는 함수들은 `bx_find_bochsrc`, `bx_read_configuration`, `bx_parse_cmdline` 등으로 보입니다. 그리고 이 모든 함수들은 `config.cc`에 있습니다. 이 다음으로는 `config.cc`를 살펴보면 될 것 같습니다.

그러나, `config.cc`는 3000줄이 넘어갈 정도로 길어서, `main.cc`에서 했던 것처럼 하나하나 뜯어보며 읽기는 어려울 것 같습니다.

따라서, 3주차부터는 코드를 리뷰하는데 있어서 좀 다른 방법을 생각해봐야 할 것 같습니다.

## 다음주 todo:

- Bochs 코드 리뷰
- 프로젝트 코드 구조 설계
- (KVM) VM 생성 및 메모리 할당 코드 작성

---

## Week 03 - 교수님 미팅

# 3주차 상담내용 (09/16)

- `./configure` 옵션을 바꿔보자. 아래와 같은 옵션을 추가해보자.
  - `--disable-vga`
  - `--with-term`
  - `--with-nogui`
- 기존 `./configure` 옵션에서, 아래와 같은 옵션은 빼도 될 것 같다.
  - `--enable-smp`: 싱글코어로 할 것이다.
  - `--enable-all-optimizations --enable-4meg-pages --enable-global-pages`: 지금은 부가적인 기능은 빼는 게 좋다.
- 코드 부분에서는, `config.cc`를 전부 읽기보다는 필요한 것만 남기고 최대한 다 빼는 게 좋다.
  - ata: ata0 master 하나만 있어도 될 것 같음.
  - floppy: 제거
  - vga: 제거
  - serial mode: `term` 또는 `file`로
  - usb: 제거
  - mouse: 제거
  - 기타 디스플레이: 제거
  - 네트워크: 제거
  - 클럭: 제거
  - pci 디바이스: 제거

## Week 03 - 연구 진행 내용

# 3주차 연구내용

목표: VM 생성 및 메모리 할당 코드 작성

저번주 todo:

- Bochs 코드 리뷰
- 프로젝트 코드 구조 설계
- (KVM) VM 생성 및 메모리 할당 코드 작성

## 연구 내용

### Bochs 2.2.6 컴파일

#### Configure

우선 저번주에 이어서 Bochs 2.2.6 컴파일을 이어서 해봤습니다.

저번주에 사용한 configure 명령은 아래와 같았습니다. (xv6 레포의 Notes 파일에서 발견된 명령어입니다.)

```bash
./configure --enable-smp --enable-disasm --enable-debugger --enable-all-optimizations --enable-4meg-pages --enable-global-pages --enable-pae --disable-reset-on-triple-fault
```

그리고 이를 컴파일하면 다음와 같은 에러가 발생했었습니다.

```
ERROR: X windows gui was selected, but X windows libraries were not found.
```

저번주에는 xorg를 설치해 해결하려 했으나, 교수님과 상담 후 `--with-term` 옵션을 주기로 결정했습니다. 또한, 멀티코어를 활성화하는 `--enable-smp` 옵션과, 최적화 및 페이징과 관련된 옵션들인 `--enable-all-optimizations --enable-4meg-pages --enable-global-pages --enable-pae`도 모두 삭제하기로 했습니다.

```bash
./configure --enable-disasm --enable-debugger --disable-reset-on-triple-fault --with-term
```

그랬더니, 다음과 같은 에러가 발생했습니다.

```
Curses library not found: tried curses, ncurses, termlib and pdcurses.
```

Curses 관련 라이브러리가 없다고 떠서, 아래 명령어로 설치해줬습니다. (Ubuntu 22.04 기준)

```bash
sudo apt install libncurses-dev
```

이후 같은 옵션으로 configure가 정상적으로 진행되었습니다. (X window 관련 문제가 발생하지 않는 걸 보니, term을 켜면 vga는 저절로 꺼지는 것 같습니다.) 따라서 바로 `make`를 진행해봤습니다. 그랬더니 수많은 warning과 함께, 아래와 같은 에러가 발생했습니다.

![alt text](image.png)

아무래도 GCC 버전이 달라 C++ 표준이 달라져서 생긴 에러인 것 같습니다.

#### GCC 버전 낮추기

따라서 시스템의 GCC 버전을 더 낮추기로 했습니다. 그래서 더 찾아보던 중, [C++과 GCC에 관한 블로그 글](https://dulidungsil.tistory.com/entry/GCC-%EB%B2%84%EC%A0%84%EA%B3%BC-C-%EB%B2%84%EC%A0%84-%EB%A7%A4%EC%B9%AD)을 하나 발견했습니다. 이 글에는 이렇게 쓰여있습니다:

> 1998년에 첫 번째 표준인 C++ 98이 공개된 후 오랜 기간동안 정체기를 거치다가 2011년이 되어서야 새로운 개념들이 추가된 버전이 공개되었습니다. C++ 11부터 Modern C++ 이라고 부릅니다.
>
> - trusty: Ubuntu 14.04, xenial: Ubuntu 16.04, bionic: Ubuntu 18.04
> - focal: Ubuntu 20.04,
> - jammy: Ubuntu 22.04, kinetic: Ubuntu 22.10
> - lunar: Ubuntu 23.04

우선 현재 사용중이던 Ubuntu 22.04에는 gcc 4.8 미만을 설치할 수 없었습니다.

따라서 distrobox에서 제공하는 가장 낮은 Ubuntu 버전인 Ubuntu 16.04를 설치하고 build-essential을 설치해봤습니다.

```bash
distrobox create -i quay.io/toolbx/ubuntu-toolbox:16.04
```

```bash
sudo apt install build-essential
```

이후 gcc 버전을 확인해보았습니다.

```
gcc (Ubuntu 5.4.0-6ubuntu1~16.04.12) 5.4.0 20160609
Copyright (C) 2015 Free Software Foundation, Inc.
This is free software; see the source for copying conditions.  There is NO
warranty; not even for MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.
```

기본 설치된 gcc 버전은 5.4.0이었습니다. 따라서 apt로 gcc-4.7을 직접 설치해보았습니다.

```bash
sudo apt install gcc-4.7 gcc-4.7-multilib g++-4.7 g++-4.7-multilib
```

그리고 gcc 4.7을 기본 컴파일러로 사용하기 위해, [update-alternatives와 관련된 블로그 글](https://blog.koriel.kr/gcc-g-dareun-beojeon-cugahago-paekiji-gwanrihagi/)을 통해 gcc 4.7을 기본으로 설정했습니다.

```bash
sudo update-alternatives --install /usr/bin/gcc gcc /usr/bin/gcc-4.7 10
sudo update-alternatives --install /usr/bin/g++ g++ /usr/bin/g++-4.7 10
```

이제 gcc 4.7이 설치되었습니다:

```
$ gcc --version
gcc (Ubuntu/Linaro 4.7.4-3ubuntu12) 4.7.4
Copyright (C) 2012 Free Software Foundation, Inc.
This is free software; see the source for copying conditions.  There is NO
warranty; not even for MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.
```

따라서 바로 configure 후 make 해보았습니다:

![alt text](image-1.png)

여전히 같은 문제로 컴파일에 실패했습니다.

#### hdimage.h 고치기

컴파일러 버전을 최대한 낮췄음에도 컴파일이 되지 않으니, 직접 코드를 수정해 컴파일되도록 만들기로 했습니다.

컴파일 에러는 다음과 같습니다:

```
hdimage.h: At global scope:
hdimage.h:277:8: error: extra qualification ‘sparse_image_t::’ on member ‘get_physical_offset’ [-fpermissive]
hdimage.h:282:8: error: extra qualification ‘sparse_image_t::’ on member ‘set_virtual_page’ [-fpermissive]
```

해당하는 부분은 `iodev/hdimage.h` 파일입니다. 문제가 되는 부분은 아래와 같습니다:

```C
off_t
#ifndef PARANOID
       sparse_image_t::
#endif
                       get_physical_offset();
 void
#ifndef PARANOID
       sparse_image_t::
#endif
                       set_virtual_page(Bit32u new_virtual_page);
```

`sparse_image_t` 클래스 안에서 그 클래스를 스스로 호출해 생긴 문제인 것 같습니다. 이 코드를 다음과 같이 고쳤습니다:

```C
off_t get_physical_offset();
void set_virtual_page(Bit32u new_virtual_page);
```

그랬더니, 다음과 같은 에러가 발생했습니다:

![alt text](image-2.png)

#### symbols.cc 고치기

에러 메시지는 다음과 같았습니다:

```
symbols.cc: At global scope:
symbols.cc:135:10: error: ‘hash_map’ does not name a type
symbols.cc:143:1: error: ‘hash_map’ does not name a type
symbols.cc: In constructor ‘context_t::context_t(Bit32u)’:
symbols.cc:150:5: error: ‘map’ was not declared in this scope
symbols.cc: In static member function ‘static context_t* context_t::get_context(Bit32u)’:
symbols.cc:171:12: error: ‘map’ was not declared in this scope
Makefile:72: recipe for target 'symbols.o' failed
make[1]: *** [symbols.o] Error 1
make[1]: Leaving directory '/home/seolcu/문서/코드/bochs-2.2.6/bx_debug'
Makefile:264: recipe for target 'bx_debug/libdebug.a' failed
make: *** [bx_debug/libdebug.a] Error 2
```

문제가 되는 부분은 `bx_debug/symbols.cc` 파일이었습니다. 라인 135 근처를 확인했습니다.

```C
private:
  static hash_map<int,context_t*>* map;
  // Forvard references (find name by address)
  set<symbol_entry_t*,lt_symbol_entry_t>* syms;
  // Reverse references (find address by name)
  set<symbol_entry_t*,lt_rsymbol_entry_t>* rsyms;
  Bit32u id;
};

hash_map<int,context_t*>* context_t::map = new hash_map<int,context_t*>;
```

hash_map이라는 것을 타입을 사용하지만, 컴파일러가 이를 못 찾는 것 같습니다.

따라서 이 타입을 불러오는 include 부분을 살펴봤습니다.

```C
/* Haven't figured out how to port this code to OSF1 cxx compiler.
   Until a more portable solution is found, at least make it easy
   to disable the template code:  just set BX_HAVE_HASH_MAP=0
   in config.h */
#if BX_HAVE_HASH_MAP
#include <hash_map>
#elif BX_HAVE_HASH_MAP_H
#include <hash_map.h>
#endif
```

OSF1 cxx 컴파일러와의 호환성을 위해 임시로 사용한 전역변수의 흔적이 남아있습니다. 또한 `hash_map`이라는 타입이, `unordered_map`이 등장하며 비표준으로 옮겨졌으므로, `<ext/hash_map>`으로 바꿔줬습니다.

```C
#include <ext/hash_map>
using namespace __gnu_cxx;
```

make 해보니 다음과 같은 에러가 발생했습니다:

```
yacc -p bx -d parser.y
make[1]: yacc: Command not found
Makefile:104: recipe for target 'parser.c' failed
make[1]: *** [parser.c] Error 127
make[1]: Leaving directory '/home/seolcu/문서/코드/bochs-2.2.6/bx_debug'
Makefile:264: recipe for target 'bx_debug/libdebug.a' failed
make: *** [bx_debug/libdebug.a] Error 2
```

yacc가 설치되지 않은 것 같아, 터미널에 입력하니 `bison` 패키지 설치를 권유받았습니다. 따라서 `bison`을 설치하고 `make`를 진행해보았더니, 드디어 정상적으로 컴파일되었습니다. 따라서 바로 `sudo make install`로 설치까지 진행했습니다.

그리고, 지금까지의 과정을 통해 알아낸 Bochs 2.2.6 컴파일 방법을 [seolcu/bochs-2.2.6](https://github.com/seolcu/bochs-2.2.6) 레포에 가이드로 작성했습니다.

### Bochs 2.2.6으로 xv6 부팅

바로 xv6를 실행하기 위해, `make` 후 `make bochs` 해봤습니다.

![alt text](image-3.png)

CPU 개수 관련 에러가 발생했습니다. 역시 `.bochsrc` 파일을 수정할 필요가 있어보입니다.

#### CPU 설정

원래 설정:

```
cpu: count=2, ips=10000000
```

CPU 개수 1개로 조정:

```
cpu: count=1, ips=10000000
```

#### ATA 설정

CPU 설정 이후 다음과 같은 에러가 발생했습니다:

```
========================================================================
                       Bochs x86 Emulator 2.2.6
              Build from CVS snapshot on January 29, 2006
========================================================================
00000000000i[     ] reading configuration from .bochsrc
00000000000i[     ] installing term module as the Bochs GUI
00000000000i[     ] Warning: no rc file specified.
00000000000i[     ] using log file bochsout.txt
========================================================================
Bochs is exiting with the following message:
[HD   ] ata0/1 image size doesn't match specified geometry
========================================================================
Makefile:210: recipe for target 'bochs' failed
make: *** [bochs] Error 1
```

이 부분과 관련된 설정은 다음 부분인 것 같았습니다:

```
ata0-master: type=disk, mode=flat, path="xv6.img", cylinders=100, heads=10, spt=10
ata0-slave: type=disk, mode=flat, path="fs.img", cylinders=1024, heads=1, spt=1
```

뒤에 있는 cylinders, heads, spt 값이 올바르지 않아서 부팅에 문제가 생긴건지 확인하기 위해, [bochsrc 공식 문서](https://bochs.sourceforge.io/doc/docbook/user/bochsrc.html)와 [위키피디아 CHS 문서](https://ko.wikipedia.org/wiki/%EC%8B%A4%EB%A6%B0%EB%8D%94-%ED%97%A4%EB%93%9C-%EC%84%B9%ED%84%B0)를 읽으며 값을 계산헀습니다.

디스크 사이즈 계산식은 다음과 같습니다:

```
총 크기 (바이트) = 실린더 수 × 헤드 수 × 트랙당 섹터 수 × 섹터당 바이트 수(512)
```

따라서, 다음과 같이 정리가 가능합니다:

```
총 크기 (바이트) = cylinders * heads * spt(secter per track) * 512
```

##### ata0-master

따라서 ata0-master의 디스크 사이즈 기대값은, 100 \* 10 \* 10 \* 512 = 5120000입니다. 이 값이 xv6.img의 크기와 일치해야합니다. 확인해보면:

```
$ ls -l xv6.img
-rw-r--r--. 1 seolcu seolcu 5120000  9월 23 12:08 xv6.img
```

이미지 사이즈가 5120000이므로, ata0-master의 디스크 사이즈는 맞습니다.

##### ata0-slave

다음으로 ata0-slave의 디스크 사이즈 기대값은, 1024 \* 1 \* 1 \* 512 = 524288입니다. fs.img를 확인해보면:

```
$ ls -l fs.img
-rw-r--r--. 1 seolcu seolcu 512000  9월 23 12:08 fs.img
```

이미지 사이즈가 512000으로, 일치하지 않는 것을 확인할 수 있었습니다.

이를 해결하기 위해 cylinders를 1024에서 1000으로 조정했습니다.:

```
ata0-master: type=disk, mode=flat, path="xv6.img", cylinders=100, heads=10, spt=10
ata0-slave: type=disk, mode=flat, path="fs.img", cylinders=1000, heads=1, spt=1
```

#### 디버거 때문? 인풋 안 들어감

이후 make bochs를 하니, 아래와 같이 뭔가 이상하게 돌아갔습니다. 아무 입력이 들어가지 않았습니다.

![alt text](image-4.png)

bochs 컴파일할 때 `--enable-debugger` 옵션을 켜서 디버거로 들어간건가 하는 생각에, 그 플래그를 빼고 다시 컴파일하고 실행해보았습니다.

#### SMP

![alt text](image-5.png)

SMP가 꺼져 있어서 커널 패닉이 발생한 것 같습니다. xv6에서 듀얼코어 이상을 요구하는 것 같습니다. 우선은 다음주로 넘겨둬야겠습니다.

## 다음주 todo

- 전체 점검

---

## Week 04 - 교수님 미팅

# 4주차 상담내용 (09/23)

다음 순서대로 할 일을 처리하면 된다:

1. SMP 문제 해결하기
2. bochs에 gdb 붙여보기 -> CPU와 메모리를 어떻게 구현하는지 관찰하기
3. xv6에 gdb 붙이기
4. 하이퍼콜: qemu와 xv6가 KVM을 어떻게 처리하는지 관찰하기
5. qemu의 KVM 기능 구현하기

## Week 04 - 연구 진행 내용

# 4주차 연구내용

목표: 계획 점검, 오픈소스 코드 리뷰, 추가 학습

저번주 todo:

- 전체 점검

## 연구내용

### Bochs에서 SMP 지원 해제하기

저번주에 Bochs 2.2.6을 컴파일하고 xv6를 디버거 없이 부팅해보았을때, 다음과 같은 에러가 발생했습니다.

![alt text](image.png)

```
Booting from Hard Disk...
lapicid 0: panic: Expect to run on an SMP
 801031a1 80102e80 0 0 0 0 0 0 0
```

교수님께서 xv6가 듀얼코어를 요구할 것 같지는 않다고 하셨기에, .bochsrc 수정 없이 bochs 2.2.6 컴파일 옵션만 수정해보기로 했습니다.

따라서 `--enable-smp` 옵션을 주어 컴파일해보았습니다.

```bash
./configure --enable-smp --enable-disasm --disable-reset-on-triple-fault --with-term
```

컴파일 이후 `make bochs` xv6를 부팅해보니, 되는듯 하더니 다음과 같은 에러가 발생했습니다.

```
========================================================================
                       Bochs x86 Emulator 2.2.6
              Build from CVS snapshot on January 29, 2006
========================================================================
00000000000i[     ] reading configuration from .bochsrc
00000000000i[     ] installing term module as the Bochs GUI
00000000000i[     ] using log file bochsout.txt
========================================================================
Bochs is exiting with the following message:
[CPU0 ] exception(): 3rd (14) exception with no resolution
========================================================================
Makefile:210: recipe for target 'bochs' failed
make: *** [bochs] Error 1
```

xv6가 아닌 bochs 에러로 보여 bochs 소스 코드에서 `exception with no resolution`를 검색해보니, `cpu/exception.cc` 파일에서 다음 코드를 발견할 수 있었습니다:

```C++
#if BX_RESET_ON_TRIPLE_FAULT
    BX_ERROR(("exception(): 3rd (%d) exception with no resolution, shutdown status is %02xh, resetting", vector, DEV_cmos_get_reg(0x0f)));
    debug(BX_CPU_THIS_PTR prev_eip);
    bx_pc_system.Reset(BX_RESET_SOFTWARE);
#else
    BX_PANIC(("exception(): 3rd (%d) exception with no resolution", vector));
    BX_ERROR(("WARNING: Any simulation after this point is completely bogus."));
#endif
#if BX_DEBUGGER
    bx_guard.special_unwind_stack = true;
#endif
    longjmp(BX_CPU_THIS_PTR jmp_buf_env, 1); // go back to main decode loop
  }
```

`BX_RESET_ON_TRIPLE_FAULT` 이라는 상수값에 따라 명령이 분기되는데, 이 옵션은 bochs configure 과정에서 사용한 옵션인 `--disable-reset-on-triple-fault` 옵션과 관련이 있어 보였습니다. 뭔가 CPU에 문제가 발생하는 상황을 가정한 것 같은데, 이 옵션을 넣으면 무조건 else문으로 빠져 작동을 멈추도록 설계된 것 같습니다.

그래서 우선 configure에서 해당 플래그를 삭제하고 다시 컴파일해보았습니다.

```bash
./configure --enable-smp --enable-disasm --with-term
```

컴파일 이후 xv6를 부팅해보니, 부팅 과정에서 막혔습니다.

![alt text](image-1.png)

무슨 상황인가 싶어 configure 옵션에 디버거를 추가해 컴파일하고 xv6를 부팅해보았습니다.

```bash
./configure --enable-smp --enable-disasm --with-term --enable-debugger
```

![alt text](image-2.png)

저번에 디버거 붙였을 때랑 같은 에러가 발생했습니다. 아무래도 configure 옵션을 더 살펴봐야겠습니다.

### Bochs gdb stub 활성화하기

`./configure --help`로 `--enable-debugger`의 설명을 읽어보았습니다.

```
  --enable-debugger                 compile in support for Bochs internal debugger
```

Bochs의 내부 디버거를 활용한다고 되어있습니다. 저번에 교수님께서 디버거 붙일 때 다른 프로세스에서 gdb 붙이는 방식을 제안하셨으므로, 다른 옵션을 써야할 것 같았습니다. 따라서 검색 중, bochs 문서에서 [GDB Stub 관련 항목](https://bochs.sourceforge.io/doc/docbook/user/debugging-with-gdb.html)을 발견했습니다.

해당 문서에 따르면, configure에서 `--enable-gdb-stub` 옵션을 주고 bochsrc에 [gdbstub 관련 옵션](https://bochs.sourceforge.io/doc/docbook/user/bochsrc.html#BOCHSOPT-GDBSTUB)을 주면 설정한 포트로 gdb를 대기한다고 합니다. 따라서 바로 시도해보았습니다.

```bash
./configure --enable-smp --enable-disasm --with-term --enable-gdb-stub
```

이후 make를 했더니, 다음과 같은 에러가 발생했습니다.

```
$ make
cd iodev && \
make  libiodev.a
make[1]: Entering directory '/home/seolcu/문서/코드/bochs-2.2.6/iodev'
g++ -c  -I.. -I./.. -I../instrument/stubs -I./../instrument/stubs -g -O2 -D_FILE_OFFSET_BITS=64 -D_LARGE_FILES   devices.cc -o devices.o
In file included from iodev.h:32:0,
                 from devices.cc:30:
../bochs.h:381:2: error: #error GDB stub was written for single processor support. If multiprocessor support is added, then we can remove this check.
 #error GDB stub was written for single processor support.  If multiprocessor support is added, then we can remove this check.
  ^
In file included from iodev.h:32:0,
                 from devices.cc:30:
../bochs.h: In member function ‘char* iofunctions::getaction(int)’:
../bochs.h:307:64: warning: deprecated conversion from string constant to ‘char*’ [-Wwrite-strings]
     static char *name[] = { "ignore", "report", "ask", "fatal" };
                                                                ^
../bochs.h:307:64: warning: deprecated conversion from string constant to ‘char*’ [-Wwrite-strings]
../bochs.h:307:64: warning: deprecated conversion from string constant to ‘char*’ [-Wwrite-strings]
../bochs.h:307:64: warning: deprecated conversion from string constant to ‘char*’ [-Wwrite-strings]
devices.cc: In constructor ‘bx_devices_c::bx_devices_c()’:
devices.cc:50:12: warning: deprecated conversion from string constant to ‘char*’ [-Wwrite-strings]
   put("DEV");
            ^
Makefile:120: recipe for target 'devices.o' failed
make[1]: *** [devices.o] Error 1
make[1]: Leaving directory '/home/seolcu/문서/코드/bochs-2.2.6/iodev'
Makefile:259: recipe for target 'iodev/libiodev.a' failed
make: *** [iodev/libiodev.a] Error 2
```

멀티코어에서는 GDB stub이 작동하지 않는 것 같습니다. 따라서 SMP를 꺼야 할 것 같습니다. SMP를 껐을 때 또 xv6에서 에러가 발생할까봐 걱정되지만, 우선 SMP를 다시 끄고 컴파일하기로 했습니다.

```bash
./configure --enable-disasm --with-term --enable-gdb-stub
```

역시 SMP를 빼고 나니 정상적으로 컴파일되었습니다. 이제 bochsrc에 다음과 같은 gdbstub 옵션을 넣어보았습니다.

```
gdbstub: enabled=1, port=1234, text_base=0, data_base=0, bss_base=0
```

이후 바로 `make bochs`로 xv6를 부팅해보았습니다. 다음과 같이 뜨며 1234 포트에서 gdb 연결 대기가 시작되었습니다.

```
Waiting for gdb connection on localhost:1234
```

다른 터미널에서 바로 gdb를 켜서 `target remote localhost:1234` 명령어로 연결해보았습니다.

```bash
(gdb) target remote localhost:1234
Remote debugging using localhost:1234
warning: unrecognized item "ENN" in "qSupported" response
qTStatus: Target returns error code 'NN'.
0x0000fff0 in ?? ()
qTStatus: Target returns error code 'NN'.
```

뭔가 경고가 뜨긴 하지만 되는 것 같습니다. `c` 명령어로 계속 실행하도록 했습니다.

![alt text](image-3.png)

```
Booting from Hard Disk...
lapicid 0: panic: Expect to run on an SMP
 801031a1 80102e80 0 0 0 0 0 0 0
```

여전히 xv6에서 SMP 관련 에러가 발생합니다. 아무래도 xv6에서 SMP 요구 자체를 끌 필요가 있을 것 같습니다.

### 멀티코어 요구 제거를 위한 xv6 코드 수정

main.c 파일에서 멀티코어와 관련되어보이는 함수 `mpinit()`와 `startothers()` 등의 함수를 비활성화하면서 멀티코어 처리를 끄려고 해 보았으나, bochs에서 잘 작동하지 않았습니다. 따라서 코드는 다시 되돌렸습니다.

### qemu에서 gdb 붙여보기

qemu 기준으로 xv6를 다시 보도록 했습니다.

xv6 부팅에서 qemu는 다음과 같은 옵션을 사용합니다.

```makefile
ifndef CPUS
CPUS := 2
endif
QEMUOPTS = -drive file=fs.img,index=1,media=disk,format=raw -drive file=xv6.img,index=0,media=disk,format=raw -smp $(CPUS) -m 512 $(QEMUEXTRA)
```

CPUS로 CPU 개수를 설정한 후, qemu 옵션으로 바로 넣는 모습입니다. 여기서 사용된 CPUS 변수는 qemu에서만 사용됩니다. 여기서 우선 CPUS를 1로 바꾸고 돌려보았습니다.

![alt text](image-4.png)

아무 문제 없이 부팅되었습니다.

`make qemu-gdb` 옵션도 있어서, 시도해보았습니다.

```bash
$ make qemu-gdb
sed "s/localhost:1234/localhost:26000/" < .gdbinit.tmpl > .gdbinit
*** Now run 'gdb'.
qemu-system-i386 -serial mon:stdio -drive file=fs.img,index=1,media=disk,format=raw -drive file=xv6.img,index=0,media=disk,format=raw -smp 1 -m 512  -S -gdb tcp::26000
```

포트 26000을 열어주어서, gdb로 attach 해보았습니다.

![alt text](image-5.png)

바로 gdb가 붙었습니다. `c`를 입력하니 정상적으로 작동합니다.

gdb에 편의성 기능들을 추가해주는 `pwndbg`를 사용하니 gdbinit을 자동으로 읽어 attach 해주고 symbol도 가져와줬습니다.

Bochs에서 안 돼서 아쉽지만, 우선 이 방식이라도 [xv6-public 레포](https://github.com/seolcu/xv6-public)의 README.md에 정리해뒀습니다.

### 프로젝트 되돌아보기

프로젝트에서 너무 문제 해결에 몰두하다 보면, 원래의 목적에서 벗어나 문제 해결에만 몰두하는, 주객이 전도되는 상황이 일어나곤 합니다. 이를 방지하기 위해, 4주차에는 프로젝트를 되돌아보는 시간을 가졌습니다.

#### 프로젝트의 목표는 무엇이었을까

이 프로젝트의 최종 목표는 `리눅스 KVM API를 이용한 초소형 가상 머신 모니터 (VMM) 개발` 입니다.

#### OS

프로젝트를 위해선 VMM 위에서 돌아갈 OS가 필요했습니다. 처음에는 리눅스를 생각했지만, 리눅스는 디바이스 드라이버 요구사항이 너무 많아 조금 더 원시적인 OS가 필요했습니다.

따라서 교육용 OS인 `xv6`를 선택했습니다. 현재 `xv6`는 RISC-V로 개발중이기에, x86 기반으로 개발된 이전 버전을 사용하기로 했습니다.

바로 로컬 환경에서 `xv6`를 컴파일해보았는데, 컴파일러 버전이 너무 높아 문제가 발생했습니다. 따라서 distrobox로 Ubuntu 16.04 환경을 만들어 컴파일에 성공했습니다.

#### VMM

아무런 레퍼런스 없이 VMM을 스스로 만들기는 어렵기에, 다른 오픈소스 VMM을 참고하기로 했습니다.

대표적인 VMM으로는 QEMU와 Bochs가 있었습니다. 여기에서 QEMU는 너무 방대하기에, Bochs를 틀로 사용하기로 했습니다. 다만 Bochs는 KVM 등의 하드웨어 지원을 사용하지 않고, 명령어를 하나하나 번역하는 인터프리터 방식이었기에 KVM과 관련된 부분은 QEMU의 코드를 참고하기로 했습니다.

그러나 Ubuntu 16.04의 Bochs 버전이 너무 높아, xv6가 잘 작동하지 않았습니다. 따라서 xv6 메모에 명시된 대로 Bochs 2.2.6를 직접 컴파일해 쓰기로 했습니다. 이 과정에서 SMP를 활성화하면 GDB를 못 붙이고, SMP를 비활성화하면 xv6가 안 돌아가는 진퇴양난의 상황을 마주하게 됩니다.

반면, QEMU는 코어 설정과 상관없이 잘 실행되었고, GDB도 잘 붙었습니다.

### 앞으로 뭘 하면 좋을까?

교수님과 상담을 통해, 앞으로의 계획을 더 탄탄히 하고 싶습니다.

---

## Week 05 - 교수님 미팅

# 5주차 상담내용 (09/30)

## 진행 상황 검토

- Bochs를 컴파일하고 xv6를 실행하려는 시도가 한 달째 이어지고 있으나, 컴파일 플래그를 바꿔보는 수준의 trial-and-error 방식으로는 진전이 거의 없었음.
- 프로젝트의 핵심은 단순히 Bochs를 빌드하는 것이 아니라, Bochs가 KVM 인터페이스를 사용하도록 만드는 것인데, 현재 부팅조차 되지 않는 상태.

## 새로운 전략 제안

교수님은 현재의 접근 방식이 비효율적이라고 판단하고, 남은 시간(약 1.5개월)을 고려하여 새로운 전략을 제안하셨습니다.

1.  **Bochs 접근 방식 포기**: Bochs 2.2.6이 너무 오래되었고, 컴파일 및 KVM 연동 문제 해결에 시간이 과도하게 소요되고 있어 이 방식은 포기하는 것으로 결정.

2.  **QEMU 또는 새로운 VMM 기반 접근**: 대안으로 QEMU를 활용하거나, 더 간단한 예제를 통해 하이퍼바이저의 핵심 원리를 학습하는 방향을 제안.

## 다음 단계 (Action Items)

- **RISC-V와 Rust를 이용한 VMM 학습**: x86의 복잡성을 피해, 더 단순하고 깔끔한 RISC-V 아키텍처를 기반으로 한 VMM 개발 튜토리얼을 학습하기로 함.
    -   교수님께서 약 1000줄 분량의 Rust로 작성된 RISC-V 하이퍼바이저 예제 코드를 추천하셨음.
    -   이 튜토리얼을 통해 VMM의 핵심 기능(VM 시작, 게스트로 제어 전달, 하이퍼콜 처리 등)을 학습하는 것을 목표로 함.

- **학습 후 x86에 적용**: RISC-V 예제에서 얻은 지식을 바탕으로, `ioctl`을 통해 호스트 KVM과 통신하는 간단한 x86 하이퍼바이저를 구현하는 것을 최종 목표로 설정.

- **향후 일정**: 다음 2주(추석 연휴 포함) 동안 해당 튜토리얼을 진행하고, 다음 상담 시간에 진행 상황을 검토하기로 함.

## 교수님께서 주신 참고 링크

- [Hypervisor in 1,000 Lines](https://1000hv.seiya.me/en/)
- [OSDev](https://wiki.osdev.org/Expanded_Main_Page)
- [Hypervisor From Scratch](https://rayanfam.com/topics/hypervisor-from-scratch-part-1/)

## Week 05-06 - 연구 진행 내용

# 5-6주차 연구내용

목표: vCPU 생성 및 제어, 메모리 입출력, VM 종료 코드 작성

저번주 todo:

- 전체 점검

## 연구 내용

4주차까지 Bochs를 이용해 xv6를 부팅하려는 시도를 했으나, 여러 문제로 인해 큰 진전이 없었습니다. 교수님과의 상담을 통해, Bochs 접근 방식을 포기하고 새로운 전략을 시도하기로 했습니다.

교수님께서 x86은 너무 복잡하니, RISC-V로 하이퍼바이저 개념을 먼저 배우고 나중에 x86 KVM으로 전환하는 방법을 추천해 주셨습니다. 그래서 [RISC-V 기반 VMM 개발 튜토리얼](https://1000hv.seiya.me/en/)을 따라가기로 했습니다.

#### 튜토리얼 구성 확인

해당 튜토리얼을 훑어보니, RISC-V 아키텍처 기반으로 Rust를 사용해 bare-metal 하이퍼바이저를 만드는 방법을 설명하고 있습니다. 목표는 약 1000줄 정도의 코드로 Linux 커널을 부팅하는 것이고, QEMU 에뮬레이터를 사용한다고 합니다.

튜토리얼은 총 13개 챕터로 구성되어 있습니다:

1. Getting Started - 개발 환경 구성
2. Boot - 하이퍼바이저 부팅
3. Hello World - 기본 출력
4. Memory Allocation - 메모리 할당
5. Guest Mode - 게스트 모드 진입
6. Guest Page Table - 페이지 테이블
7. Hello from Guest - 게스트 실행
8. Build Linux Kernel - 리눅스 빌드
9. Boot Linux - 리눅스 부팅
10. Supervisor Binary Interface - SBI
11. Memory Mapped I/O - MMIO
12. Interrupt Injection - 인터럽트
13. Outro - 마무리

이 중 챕터 8부터는 미완성인 듯 합니다.

### Chapter 1-2: 개발 환경 및 부팅 구조 만들기

#### Rust 툴체인 설치

패키지 매니저 대신 튜토리얼에서 권장하는 Rustup으로 설치했습니다:

```bash
curl --proto '=https' --tlsv1.2 -sSf https://sh.rustup.rs | sh
```

QEMU는 이미 설치되어 있었기 때문에 넘어갔습니다.

#### Rust 프로젝트 생성

새 프로젝트를 만들었습니다:

```bash
cargo init --bin hypervisor
```

`rust-toolchain.toml` 파일을 만들어서 툴체인을 지정했습니다:

```toml
[toolchain]
channel = "stable"
targets = ["riscv64gc-unknown-none-elf"]
```

#### 최소 부팅 코드 작성

`src/main.rs`를 작성했습니다. `#![no_std]`와 `#![no_main]` 속성을 사용해 표준 라이브러리 없이 bare-metal 환경에서 실행되도록 했습니다:

```rust
#![no_std]
#![no_main]

use core::arch::asm;

#[unsafe(no_mangle)]
#[unsafe(link_section = ".text.boot")]
pub extern "C" fn boot() -> ! {
    unsafe {
        asm!(
            "la sp, __stack_top",
            "j {main}",
            main = sym main,
            options(noreturn)
        );
    }
}

fn main() -> ! {
    // BSS 초기화
    unsafe {
        let bss_start = &raw mut __bss;
        let bss_size = (&raw mut __bss_end as usize) - (&raw mut __bss as usize);
        core::ptr::write_bytes(bss_start, 0, bss_size);
    }

    loop {}
}
```

#### 링커 스크립트 작성

`hypervisor.ld`를 만들어 메모리 레이아웃을 정의했습니다:

```ld
ENTRY(boot)

SECTIONS {
    . = 0x80200000;  /* OpenSBI가 커널을 로드하는 주소 */

    .text :{
        KEEP(*(.text.boot));
        *(.text .text.*);
    }

    .rodata : ALIGN(8) {
        *(.rodata .rodata.*);
    }

    .data : ALIGN(8) {
        *(.data .data.*);
    }

    .bss : ALIGN(8) {
        __bss = .;
        *(.bss .bss.* .sbss .sbss.*);
        __bss_end = .;
    }

    . = ALIGN(16);
    . += 1024 * 1024; /* 1MB 스택 */
    __stack_top = .;
}
```

#### Panic Handler 추가

빌드하니 panic handler가 없다는 에러가 발생했습니다:

```
error: `#[panic_handler]` function required, but not found
```

`no_std` 환경에서는 panic handler를 직접 구현해야 합니다:

```rust
use core::panic::PanicInfo;

#[panic_handler]
pub fn panic_handler(_info: &PanicInfo) -> ! {
    loop {
        unsafe {
            core::arch::asm!("wfi");
        }
    }
}
```

#### 빌드 및 실행

`run.sh` 스크립트를 작성했습니다:

```bash
#!/bin/sh
set -ev

RUSTFLAGS="-C link-arg=-Thypervisor.ld -C linker=rust-lld" \
  cargo build --bin hypervisor --target riscv64gc-unknown-none-elf

cp target/riscv64gc-unknown-none-elf/debug/hypervisor hypervisor.elf

qemu-system-riscv64 \
    -machine virt \
    -cpu rv64 \
    -bios default \
    -smp 1 \
    -m 128M \
    -nographic \
    -d cpu_reset,unimp,guest_errors,int -D qemu.log \
    -serial mon:stdio \
    --no-reboot \
    -kernel hypervisor.elf
```

실행해봤습니다:

```bash
chmod +x run.sh
./run.sh
```

OpenSBI 부팅 메시지가 출력된 후 멈췄습니다. 아직 아무것도 출력하지 않지만, `main()` 함수의 무한 루프가 실행되고 있는 듯 합니다.

### Chapter 3: Hello World

#### SBI를 이용한 콘솔 출력

튜토리얼을 보니, OpenSBI 펌웨어가 제공하는 SBI(Supervisor Binary Interface)를 사용할 수 있다고 합니다. `src/print.rs`를 만들어서 SBI putchar 함수를 구현했습니다:

```rust
use core::arch::asm;

pub fn sbi_putchar(ch: u8) {
    unsafe {
        asm!(
            "ecall",
            in("a6") 0,
            in("a7") 1,
            inout("a0") ch as usize => _,
            out("a1") _
        );
    }
}
```

`ecall` 명령을 inline assembly로 호출합니다. `a7`=1은 Console Putchar extension입니다.

#### println! 매크로 구현

Rust의 `fmt::Write` 트레이트를 구현해서 `println!` 매크로를 만들었습니다:

```rust
pub struct Printer;

impl core::fmt::Write for Printer {
    fn write_str(&mut self, s: &str) -> core::fmt::Result {
        for byte in s.bytes() {
            sbi_putchar(byte);
        }
        Ok(())
    }
}

#[macro_export]
macro_rules! println {
    ($($arg:tt)*) => {{
        use core::fmt::Write;
        let _ = writeln!($crate::print::Printer, $($arg)*);
    }};
}
```

#### Trap Handler 구현

CPU가 exception이나 interrupt를 만나면 trap handler가 호출됩니다. `src/trap.rs`를 만들었습니다:

```rust
#[macro_export]
macro_rules! read_csr {
    ($csr:expr) => {{
        let mut value: u64;
        unsafe {
            ::core::arch::asm!(concat!("csrr {}, ", $csr), out(reg) value);
        }
        value
    }};
}

#[unsafe(link_section = ".text.stvec")]
pub fn trap_handler() -> ! {
    let scause = read_csr!("scause");
    let sepc = read_csr!("sepc");
    let stval = read_csr!("stval");
    
    panic!("trap: scause={:#x} at {:#x} (stval={:#x})", scause, sepc, stval);
}
```

Panic handler도 수정해서 panic 정보를 출력하도록 했습니다:

```rust
#[panic_handler]
pub fn panic_handler(info: &PanicInfo) -> ! {
    println!("panic: {}", info);
    loop {
        unsafe {
            core::arch::asm!("wfi");
        }
    }
}
```

#### main() 수정

`src/main.rs`를 수정해서 trap handler를 등록하고 부팅 메시지를 출력했습니다:

```rust
#[macro_use]
mod print;
mod trap;

fn main() -> ! {
    unsafe {
        let bss_start = &raw mut __bss;
        let bss_size = (&raw mut __bss_end as usize) - (&raw mut __bss as usize);
        core::ptr::write_bytes(bss_start, 0, bss_size);

        asm!("csrw stvec, {}", in(reg) trap::trap_handler as usize);
    }

    println!("\nBooting hypervisor...");
    loop {}
}
```

#### 실행 결과

실행해보니:

```
Booting hypervisor...
```

드디어 출력에 성공했습니다.

### Chapter 4: Memory Allocation

#### Bump Allocator 구현

`no_std` 환경에서 `Vec`, `Box` 같은 자료구조를 쓰려면 메모리 할당자를 직접 구현해야 합니다. 가장 단순한 bump allocator를 구현했습니다.

`spin` 크레이트를 추가합니다.

```bash
cargo add spin
```

`src/allocator.rs`를 만듭니다.

```rust
use core::alloc::{GlobalAlloc, Layout};
use spin::Mutex;

pub struct BumpAllocator {
    next: usize,
    end: usize,
}

unsafe impl GlobalAlloc for BumpAllocator {
    unsafe fn alloc(&self, layout: Layout) -> *mut u8 {
        let mut next = self.next;
        let align = layout.align();
        let size = layout.size();
        
        // Align
        next = (next + align - 1) & !(align - 1);
        
        let new_next = next + size;
        if new_next > self.end {
            return core::ptr::null_mut();
        }
        
        self.next = new_next;
        next as *mut u8
    }

    unsafe fn dealloc(&self, _ptr: *mut u8, _layout: Layout) {
        // Bump allocator는 해제를 지원하지 않음
    }
}

#[global_allocator]
static ALLOCATOR: Mutex<BumpAllocator> = Mutex::new(BumpAllocator {
    next: 0,
    end: 0,
});
```

링커 스크립트에 힙 영역을 추가합니다.

```ld
    . = ALIGN(16);
    __heap = .;
    . += 8 * 1024 * 1024; /* 8MB 힙 */
    __heap_end = .;
```

`main()`에서 할당자를 초기화했습니다:

```rust
unsafe extern "C" {
    static mut __heap: u8;
    static mut __heap_end: u8;
}

fn main() -> ! {
    // ... BSS 초기화 ...
    
    unsafe {
        let heap_start = &raw mut __heap as usize;
        let heap_size = (&raw mut __heap_end as usize) - heap_start;
        allocator::init(heap_start, heap_size);
    }
    
    // ...
}
```

#### Page Allocator 구현

Bump allocator 위에 4KB 단위로 메모리를 할당하는 page allocator를 만들었습니다:

```rust
extern crate alloc;
use alloc::vec::Vec;

const PAGE_SIZE: usize = 4096;

pub fn alloc_pages(size: usize) -> *mut u8 {
    let pages = (size + PAGE_SIZE - 1) / PAGE_SIZE;
    let mut v = Vec::with_capacity(pages * PAGE_SIZE);
    v.resize(pages * PAGE_SIZE, 0);
    let ptr = v.as_mut_ptr();
    core::mem::forget(v);
    ptr
}
```

#### 테스트

`Vec`을 테스트해봤습니다

```rust
extern crate alloc;
use alloc::vec::Vec;

fn main() -> ! {
    // ... 초기화 ...
    
    println!("\nBooting hypervisor...");
    
    let mut v = Vec::new();
    for i in 0..10 {
        v.push(i);
    }
    println!("vec test: {:?}", v);
    
    loop {}
}
```

다음과 같은 실행 결과가 출력되었습니다:

```
Booting hypervisor...
vec test: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]
```

잘 작동합니다.

### Chapter 5-6: Guest Mode 및 Page Table

#### 게스트 모드 진입 시도

RISC-V 가상화 모드를 활성화하려면 H-extension을 켜야 합니다. `run.sh`에 `-cpu rv64,h=true`를 추가했습니다:

```bash
qemu-system-riscv64 \
    -machine virt \
    -cpu rv64,h=true \
    ...
```

게스트 모드 진입 코드를 작성했습니다:

```rust
fn main() -> ! {
    // ... 초기화 ...
    
    println!("\nBooting hypervisor...");
    
    let guest_code: [u8; 4] = [0x73, 0x00, 0x00, 0x00]; // wfi 명령
    
    unsafe {
        // hstatus 설정
        let mut hstatus: u64 = 0;
        hstatus |= 2 << 32; // VSXL=2 (64-bit mode)
        hstatus |= 1 << 7;  // SPV=1 (virtualization mode)
        asm!("csrw hstatus, {}", in(reg) hstatus);
        
        // sstatus 설정
        let mut sstatus: u64 = 0;
        sstatus |= 1 << 8; // SPP=1 (return to supervisor mode)
        asm!("csrw sstatus, {}", in(reg) sstatus);
        
        // sepc 설정
        asm!("csrw sepc, {}", in(reg) guest_code.as_ptr() as u64);
        
        // 게스트로 진입
        asm!("sret");
    }
    
    loop {}
}
```

실행하니 바로 트랩이 발생했습니다:

```
panic: trap: scause=0xc at 0x80200b7a (stval=0x80200b7a)
```

`scause=0xc`는 instruction page fault입니다. 게스트 코드가 직접 실행되려 했지만 주소 변환이 안 돼서 발생한 에러입니다.

#### Guest Page Table 구현

2-stage address translation을 구현해야 합니다. `src/guest_page_table.rs`를 만듭니다.

```rust
use crate::allocator::alloc_pages;

pub const PTE_V: u64 = 1 << 0;
pub const PTE_R: u64 = 1 << 1;
pub const PTE_W: u64 = 1 << 2;
pub const PTE_X: u64 = 1 << 3;

pub struct GuestPageTable {
    root: *mut u64,
}

impl GuestPageTable {
    pub fn new() -> Self {
        let root = alloc_pages(4096) as *mut u64;
        unsafe {
            core::ptr::write_bytes(root, 0, 4096);
        }
        Self { root }
    }
    
    pub fn map(&mut self, guest_pa: u64, host_pa: u64, flags: u64) {
        // 4-level page table (Sv48x4) 구현
        // ...
    }
    
    pub fn hgatp(&self) -> u64 {
        let mode = 9u64 << 60; // Sv48x4 mode
        mode | ((self.root as u64) >> 12)
    }
}
```

Page table 구현은 4단계로 나뉩니다:

1. VPN[3]으로 L3 테이블 인덱스
2. VPN[2]로 L2 테이블 인덱스
3. VPN[1]로 L1 테이블 인덱스
4. VPN[0]으로 L0 테이블 인덱스

각 단계에서 PTE가 없으면 새 페이지를 할당하고, 마지막 단계에서 host PA를 매핑합니다.

#### 게스트 코드 컴파일

간단한 어셈블리 코드를 작성했습니다:

```asm
.section .text
.global guest_boot
guest_boot:
    j guest_boot
```

GCC로 컴파일하고 바이너리로 변환했습니다:

```bash
riscv64-linux-gnu-as -o guest.o guest.S
riscv64-linux-gnu-ld -Ttext=0x100000 -o guest.elf guest.o
riscv64-linux-gnu-objcopy -O binary guest.elf guest.bin
```

#### 메모리 로딩 및 매핑

게스트 코드를 메모리에 로드하고 page table에 매핑합니다.

```rust
fn main() -> ! {
    // ... 초기화 ...
    
    println!("\nBooting hypervisor...");
    
    // 게스트 코드 로드
    let guest_bin = include_bytes!("../guest.bin");
    let kernel_memory = alloc_pages(guest_bin.len());
    unsafe {
        core::ptr::copy_nonoverlapping(
            guest_bin.as_ptr(),
            kernel_memory,
            guest_bin.len()
        );
    }
    
    // Page table 설정
    let guest_entry = 0x100000u64;
    let mut table = GuestPageTable::new();
    table.map(guest_entry, kernel_memory as u64, PTE_R | PTE_W | PTE_X);
    
    println!("map: {:08x} -> {:08x}", guest_entry, kernel_memory as u64);
    
    // 게스트 모드 진입
    unsafe {
        let mut hstatus: u64 = 0;
        hstatus |= 2 << 32;
        hstatus |= 1 << 7;
        asm!("csrw hstatus, {}", in(reg) hstatus);
        
        let sstatus: u64 = 1 << 8;
        asm!("csrw sstatus, {}", in(reg) sstatus);
        
        asm!("csrw hgatp, {}", in(reg) table.hgatp());
        asm!("csrw sepc, {}", in(reg) guest_entry);
        
        asm!("sret");
    }
    
    loop {}
}
```

#### 실행 결과

실행해보니:

```
Booting hypervisor...
map: 00100000 -> 80305000
```

멈췄습니다. 게스트 코드가 무한 루프를 실행하고 있기 때문으로 보입니다.

#### 문제: alloc_pages 길이 처리

튜토리얼과 다르게 한 가지 문제를 발견했습니다. 튜토리얼 코드는 `alloc_pages(1)`을 호출해 1페이지를 할당하는데, 제 구현은 `alloc_pages(size)`가 바이트 단위입니다.

따라서 `alloc_pages(4096)`로 수정해야 했습니다. 아니면 `alloc_pages`의 시그니처를 바꿔서 페이지 수를 받도록 할 수도 있지만, 우선은 그대로 두기로 했습니다.

### Chapter 7: Hello from Guest (Hypercall)

Chapter 7의 목표는 게스트에서 하이퍼바이저로 통신하는 것입니다.

#### 게스트 코드 수정

게스트가 SBI의 "Console Putchar"를 호출하도록 `guest.S`를 수정했습니다:

```asm
.section .text
.global guest_boot
guest_boot:
    li a7, 1        # EID=1 (legacy console)
    li a6, 0        # FID=0 (putchar)
    li a0, 'A'      # Parameter: 'A'
    ecall           # Call SBI

    li a7, 1
    li a6, 0
    li a0, 'B'
    ecall

    li a7, 1
    li a6, 0
    li a0, 'C'
    ecall

halt:
    j halt
```

빌드하고 실행하면 이렇게 트랩이 발생합니다.

```
panic: trap: scause=0xa at 0x100008 (stval=0x0)
```

`scause=0xa`는 "environment call from VS-mode"입니다. 게스트가 `ecall`을 호출한 거라 정확히 우리가 원하는 트랩입니다.

#### VCpu 구조체 도입

Hypercall을 구현하기 전에, 게스트 상태를 관리할 `VCpu` 구조체를 만들겠습니다. 새 파일 `src/vcpu.rs`를 만듭니다.

```rust
use core::{arch::asm, mem::offset_of};
use crate::{allocator::alloc_pages, guest_page_table::GuestPageTable};

#[derive(Debug, Default)]
pub struct VCpu {
    pub hstatus: u64,
    pub hgatp: u64,
    pub sstatus: u64,
    pub sepc: u64,
    pub host_sp: u64,
    pub ra: u64,
    pub sp: u64,
    pub gp: u64,
    pub tp: u64,
    pub t0: u64,
    pub t1: u64,
    pub t2: u64,
    pub s0: u64,
    pub s1: u64,
    pub a0: u64,
    pub a1: u64,
    pub a2: u64,
    pub a3: u64,
    pub a4: u64,
    pub a5: u64,
    pub a6: u64,
    pub a7: u64,
    pub s2: u64,
    pub s3: u64,
    pub s4: u64,
    pub s5: u64,
    pub s6: u64,
    pub s7: u64,
    pub s8: u64,
    pub s9: u64,
    pub s10: u64,
    pub s11: u64,
    pub t3: u64,
    pub t4: u64,
    pub t5: u64,
    pub t6: u64,
}

impl VCpu {
    pub fn new(table: &GuestPageTable, guest_entry: u64) -> Self {
        let mut hstatus: u64 = 0;
        hstatus |= 2 << 32; // VSXL=2
        hstatus |= 1 << 7;  // SPV=1

        let sstatus: u64 = 1 << 8; // SPP=1

        let stack_size = 512 * 1024;
        let host_sp = alloc_pages(stack_size) as u64 + stack_size as u64;
        
        Self {
            hstatus,
            hgatp: table.hgatp(),
            sstatus,
            sepc: guest_entry,
            host_sp,
            ..Default::default()
        }
    }

    pub fn run(&mut self) -> ! {
        unsafe {
            asm!(
                "csrw hstatus, {hstatus}",
                "csrw sstatus, {sstatus}",
                "csrw sscratch, {sscratch}",
                "csrw hgatp, {hgatp}",
                "csrw sepc, {sepc}",

                "mv a0, {sscratch}",
                "ld ra, {ra_offset}(a0)",
                "ld sp, {sp_offset}(a0)",
                "ld gp, {gp_offset}(a0)",
                "ld tp, {tp_offset}(a0)",
                "ld t0, {t0_offset}(a0)",
                "ld t1, {t1_offset}(a0)",
                "ld t2, {t2_offset}(a0)",
                "ld s0, {s0_offset}(a0)",
                "ld s1, {s1_offset}(a0)",
                "ld a1, {a1_offset}(a0)",
                "ld a2, {a2_offset}(a0)",
                "ld a3, {a3_offset}(a0)",
                "ld a4, {a4_offset}(a0)",
                "ld a5, {a5_offset}(a0)",
                "ld a6, {a6_offset}(a0)",
                "ld a7, {a7_offset}(a0)",
                "ld s2, {s2_offset}(a0)",
                "ld s3, {s3_offset}(a0)",
                "ld s4, {s4_offset}(a0)",
                "ld s5, {s5_offset}(a0)",
                "ld s6, {s6_offset}(a0)",
                "ld s7, {s7_offset}(a0)",
                "ld s8, {s8_offset}(a0)",
                "ld s9, {s9_offset}(a0)",
                "ld s10, {s10_offset}(a0)",
                "ld s11, {s11_offset}(a0)",
                "ld t3, {t3_offset}(a0)",
                "ld t4, {t4_offset}(a0)",
                "ld t5, {t5_offset}(a0)",
                "ld t6, {t6_offset}(a0)",
                "ld a0, {a0_offset}(a0)",

                "sret",
                hstatus = in(reg) self.hstatus,
                sstatus = in(reg) self.sstatus,
                hgatp = in(reg) self.hgatp,
                sepc = in(reg) self.sepc,
                sscratch = in(reg) (self as *mut VCpu as usize),
                ra_offset = const offset_of!(VCpu, ra),
                sp_offset = const offset_of!(VCpu, sp),
                gp_offset = const offset_of!(VCpu, gp),
                tp_offset = const offset_of!(VCpu, tp),
                t0_offset = const offset_of!(VCpu, t0),
                t1_offset = const offset_of!(VCpu, t1),
                t2_offset = const offset_of!(VCpu, t2),
                s0_offset = const offset_of!(VCpu, s0),
                s1_offset = const offset_of!(VCpu, s1),
                a0_offset = const offset_of!(VCpu, a0),
                a1_offset = const offset_of!(VCpu, a1),
                a2_offset = const offset_of!(VCpu, a2),
                a3_offset = const offset_of!(VCpu, a3),
                a4_offset = const offset_of!(VCpu, a4),
                a5_offset = const offset_of!(VCpu, a5),
                a6_offset = const offset_of!(VCpu, a6),
                a7_offset = const offset_of!(VCpu, a7),
                s2_offset = const offset_of!(VCpu, s2),
                s3_offset = const offset_of!(VCpu, s3),
                s4_offset = const offset_of!(VCpu, s4),
                s5_offset = const offset_of!(VCpu, s5),
                s6_offset = const offset_of!(VCpu, s6),
                s7_offset = const offset_of!(VCpu, s7),
                s8_offset = const offset_of!(VCpu, s8),
                s9_offset = const offset_of!(VCpu, s9),
                s10_offset = const offset_of!(VCpu, s10),
                s11_offset = const offset_of!(VCpu, s11),
                t3_offset = const offset_of!(VCpu, t3),
                t4_offset = const offset_of!(VCpu, t4),
                t5_offset = const offset_of!(VCpu, t5),
                t6_offset = const offset_of!(VCpu, t6),
            );
        }

        unreachable!();
    }
}
```

`sscratch`에 `VCpu` 구조체 포인터를 저장합니다. Trap handler에서 이 포인터로 게스트 상태를 저장/복원합니다.

#### main() 수정

`src/main.rs`에서 `VCpu`를 사용하도록 수정합니다.

```rust
mod vcpu;

use crate::{
    allocator::alloc_pages,
    guest_page_table::{GuestPageTable, PTE_R, PTE_W, PTE_X},
    vcpu::VCpu,
};

fn main() -> ! {
    // ... 초기화 및 게스트 코드 로드 ...
    
    let mut vcpu = VCpu::new(&table, guest_entry);
    vcpu.run();
}
```

#### Trap Handler 수정

`src/trap.rs`를 `#[naked]` 함수로 수정해서 게스트 상태를 저장하고 복원하도록 했습니다:

```rust
use core::{arch::naked_asm, mem::offset_of};
use crate::vcpu::VCpu;

#[unsafe(link_section = ".text.stvec")]
#[naked]
pub unsafe extern "C" fn trap_handler() {
    naked_asm!(
        // sscratch와 a0 교환 (VCpu 포인터 가져오기)
        "csrrw a0, sscratch, a0",
        
        // 모든 레지스터 저장
        "sd ra, {ra_offset}(a0)",
        "sd sp, {sp_offset}(a0)",
        // ... (생략) ...
        "sd t6, {t6_offset}(a0)",
        
        // CSR 저장
        "csrr t0, sepc",
        "sd t0, {sepc_offset}(a0)",
        
        // sscratch 복구 (원래 a0 값 저장)
        "csrr t0, sscratch",
        "sd t0, {a0_offset}(a0)",
        
        // 하이퍼바이저 스택으로 전환
        "ld sp, {host_sp_offset}(a0)",
        
        // handle_trap 호출
        "call {handle_trap}",
        
        ra_offset = const offset_of!(VCpu, ra),
        sp_offset = const offset_of!(VCpu, sp),
        // ... (생략) ...
        sepc_offset = const offset_of!(VCpu, sepc),
        a0_offset = const offset_of!(VCpu, a0),
        host_sp_offset = const offset_of!(VCpu, host_sp),
        handle_trap = sym handle_trap,
    );
}

fn handle_trap(vcpu: &mut VCpu) -> ! {
    let scause = read_csr!("scause");
    
    if scause == 10 {
        // VS-mode ecall
        println!("SBI call: eid={:#x}, fid={:#x}, a0={:#x} ('{}')",
            vcpu.a7, vcpu.a6, vcpu.a0, vcpu.a0 as u8 as char);
        
        // sepc += 4 (ecall 명령 건너뛰기)
        vcpu.sepc += 4;
        
        vcpu.run();
    }
    
    panic!("trap: scause={:#x} at {:#x}", scause, vcpu.sepc);
}
```

중요한 부분은:
- `csrrw a0, sscratch, a0`로 VCpu 포인터와 a0 교환
- 모든 레지스터를 VCpu 구조체에 저장
- `host_sp`로 스택 전환
- `handle_trap()` 호출
- `sepc += 4`로 ecall 명령 건너뛰기
- `vcpu.run()`으로 게스트 복귀

#### 실행 결과

실행해보니:

```
Booting hypervisor...
map: 00100000 -> 80306000
SBI call: eid=0x1, fid=0x0, a0=0x41 ('A')
SBI call: eid=0x1, fid=0x0, a0=0x42 ('B')
SBI call: eid=0x1, fid=0x0, a0=0x43 ('C')
```

성공했습니다. 게스트에서 하이퍼바이저로 문자를 전달해서 출력되는 모습입니다.

### 튜토리얼 8-10챕터 확인

Chapter 7까지 끝내고, 남은 챕터들을 봤습니다.

#### 나머지 챕터

챕터 8~10까지는 튜토리얼에는 미완성이라고 되어있지만, 튜토리얼 저장소를 보니까 Chapter 8-10 구현이 어느정도 되어있는 것 같았습니다.

그래서 코드를 복사해서 실행해봤는데, Linux 부팅 시도하니까 초기 커널 메시지 몇 개 나오다가 guest page fault 나면서 멈췄습니다.

Chapter 11(MMIO), 12(Interrupt), 13(Outro)는 아예 작업이 되지 않은 것 같습니다.

따라서 우선은 여기까지 하기로 했습니다.

### x86 KVM으로 전환 준비

RISC-V 튜토리얼로 하이퍼바이저의 핵심 구조를 어느 정도 이해했으니, 이제 x86 KVM으로 전환할 계획을 세웠습니다.

RISC-V에서 배운 걸 정리하면:

- OpenSBI가 하이퍼바이저 `boot()` 호출 → 스택/BSS 초기화 → `main()`
- Heap 위에 page allocator (4KB 단위)
- Guest memory는 page allocator에서 할당 → guest page table에 매핑
- 2-stage translation: guest PA → host PA
- Guest 실행: CSR 설정 (`hgatp`, `hstatus`, `sepc`) → `sret`
- VM Exit: trap handler → 레지스터 저장 → Rust 코드 처리 → 레지스터 복원 → `sret`

이걸 x86 KVM에 대응하는 코드로 바꾸면 될 것 같습니다.


### 다음주 todo

- KVM: 간단한 게스트 코드 실행

### 참고 자료

- [RISC-V Hypervisor Tutorial](https://1000hv.seiya.me/en/)
- [Tutorial GitHub Repository](https://github.com/nuta/tutorial-risc-v)

구현한 코드는 `hypervisor/` 폴더에 있습니다.


---

## Week 07 - 교수님 미팅

# 7주차 상담내용 (10/14)

## 진행 상황 검토

- RISC-V 튜토리얼을 통해 하이퍼바이저의 기본적인 구조(2단계 주소 변환, 하이퍼콜 등)를 성공적으로 학습하고 구현함.
- 게스트 OS에서 보낸 문자를 하이퍼바이저가 받아서 출력하는 데 성공.

## 새로운 전략 제안

- 기존의 x86 VMM을 처음부터 다시 개발하는 것은 시간 제약과 복잡성 때문에 현실적으로 어렵다고 판단.
- 교수님께서 과거 연구실 학생이 개발한 x86 기반의 간단한 토이 OS를 제공하기로 함.
  - 이 OS는 베어메탈 부팅, 인터럽트 처리, 페이징, 간단한 메모리 할당 기능이 이미 구현되어 있음.

## 다음 단계 (Action Items)

1.  **제공된 토이 OS 활용**:

    - 교수님께서 제공해주실 Rust로 작성된 x86 토이 OS를 기반으로 프로젝트를 진행.
    - 이 OS의 부팅 및 기본 기능(인터럽트, 페이징)을 분석하고 활용하여 하이퍼바이저의 기반을 다짐.

2.  **QEMU 코드 참고**:

    - 2단계 주소 변환(second-stage translation)과 같이 복잡한 부분은 QEMU의 x86 KVM 구현 코드를 참고하여 이식하는 방향으로 진행.

3.  **하이퍼콜 및 인터럽트 처리**:
    - 토이 OS에 이미 구현된 인터럽트 핸들러를 확장하여, 게스트로부터 발생하는 하이퍼콜과 인터럽트를 처리하는 기능을 구현.

## 향후 일정

- **다음 2주**: 중간고사 기간임을 감안하여, 제공된 토이 OS를 실행해보고 코드 구조를 파악하는 것을 목표로 함.
- **중간 보고서**: 지금까지 작성한 연구 노트를 정리하여 PDF 형태로 제출. (큰 비중을 두지 않아도 됨)
- **다음 미팅**: 2주 후에 진행 상황 검토 예정.

## 참고 자료

- **제공된 토이 OS**: [https://github.com/HLe4s/HLeOs](https://github.com/HLe4s/HLeOs)

## Week 07-08 - 연구 진행 내용

# 7-8주차 연구내용

목표: HLeOs 부팅 및 코드 분석

저번주 todo:

- 중간보고서 작성 및 제출

## 연구 내용

지난주 교수님과의 상담에서, Rust로 작성된 x86 기반 토이 OS인 HLeOs를 제공받았습니다. 이번 주에는 이 OS를 부팅하고 코드를 분석하는 것을 목표로 합니다.

### HLeOs 클론

교수님께서 제공해주신 [HLeOs](https://github.com/HLe4s/HLeOs) 레포지토리를 클론했습니다.

```bash
git clone https://github.com/HLe4s/HLeOs
cd HLeOs
```

레포지토리 구조를 보니, HLeOs 디렉토리 안에 또 HLeOs 디렉토리가 있습니다. 실제 OS 코드는 `HLeOs/HLeOs`에 있는 것 같습니다.

README.md를 읽어보니, 베어메탈 부팅, 인터럽트 처리, 페이징, 메모리 할당 등이 구현되어 있다고 합니다.

### 빌드 환경 설정

README에 Quick Start 가이드가 있어서 그대로 따라가보기로 했습니다. 순서는 다음과 같습니다:

1. Rust toolchain 설치 (cargo, rustup)
2. rust nightly로 override
3. bootimage 설치
4. rust-src 컴포넌트 추가
5. llvm-tools-preview 컴포넌트 추가
6. make

#### Rust 설치

시스템에 Rust의 stable 버전이 설치되어 있어서, rustup을 이용해 nightly로 전환했습니다.

```bash
cd HLeOs/HLeOs
rustup override set nightly
```

#### 필요한 컴포넌트 설치

README에 명시된 대로 필요한 컴포넌트들을 설치했습니다.

```bash
cargo install bootimage
rustup component add rust-src --toolchain nightly-x86_64-unknown-linux-gnu
rustup component add llvm-tools-preview
```

### 첫 번째 빌드 시도

이제 `make` 명령으로 빌드를 시도해봤습니다.

```bash
make
```

그랬더니 다음과 같은 에러가 발생했습니다:

```
error: error loading target specification: target-pointer-width: invalid type: string "64", expected u16 at line 6 column 32
```

#### target specification 파일 문제

에러 메시지를 보니 `x86_64-HLeos.json` 파일의 `target-pointer-width` 필드가 문자열 "64"로 되어 있는데, u16 타입이 필요하다고 합니다.

`x86_64-HLeos.json` 파일을 확인해보니:

```json
{
    "llvm-target": "x86_64-unknown-none",
    ...
    "target-pointer-width": "64",
    ...
}
```

이 부분이 문제였습니다. 아마 HLeOs가 작성될 당시의 Rust 버전에서는 문자열을 허용했지만, 최신 nightly 버전에서는 숫자 타입을 요구하는 것 같습니다.

따라서 `x86_64-HLeos.json`를 다음과 같이 수정했습니다:

```json
"target-pointer-width": 64,
"target-c-int-width": 32,
```

### 두 번째 빌드 시도

다시 `make`를 실행해봤습니다. 그랬더니 이번에는 다른 에러가 발생했습니다:

```
error: error loading target specification: target feature `soft-float` is incompatible with the ABI but gets enabled in target spec
```

#### soft-float 피처 문제

`soft-float` 피처가 ABI와 호환되지 않는다는 에러입니다. `x86_64-HLeos.json`의 features 필드를 보니:

```json
"features": "-mmx,-sse,+soft-float"
```

이것도 Rust 버전이 바뀌면서 생긴 문제인 것 같습니다. x86_64 아키텍처에서 soft-float를 사용하는 것이 이제는 허용되지 않는 것 같습니다.

따라서 `x86_64-HLeos.json`을 다음과 같이 수정했습니다:

```json
"features": "-mmx,-sse"
```

### 세 번째 빌드 시도

다시 `make`를 실행했습니다. 이번에는 Rust 코드는 컴파일이 시작되었지만, C 코드 컴파일에서 에러가 발생했습니다:

```
c_src/dummy_c.c:5:26: error: initialization of 'uint16_t *' {aka 'short unsigned int *'} from 'int' makes pointer from integer without a cast [-Wint-conversion]
    5 |         uint16_t * vgr = 0xb8000;
```

#### C 코드 타입 캐스팅 문제

`c_src/dummy_c.c` 파일을 확인해보니, VGA 텍스트 버퍼 주소를 포인터에 직접 대입하고 있었습니다:

```c
uint16_t * vgr = 0xb8000;
```

찾아보니, 최신 GCC에서는 정수를 포인터로 직접 대입하는 것을 허용하지 않습니다. 명시적으로 캐스팅이 필요해 보입니다.

따라서 `c_src/dummy_c.c`를 다음과 같이 수정했습니다:

```c
uint16_t * vgr = (uint16_t *)0xb8000;
```

### 네 번째 빌드 시도

다시 `make`를 실행했습니다. C 코드는 컴파일되었지만, Rust 코드 컴파일 중에 또 다른 에러가 발생했습니다:

```
rustc-LLVM ERROR: SSE register return with SSE disabled
```

#### SSE 레지스터 문제

SSE를 비활성화했는데 (`"-mmx,-sse"`), SSE 레지스터를 사용하는 리턴이 있다는 에러입니다.

이 문제는 soft-float를 제거했기 때문에 생긴 것 같습니다. SSE를 비활성화하면 부동소수점 연산을 위해 soft-float나 x87 FPU를 사용해야 하는데, 둘 다 설정되지 않은 상태입니다.

생각해보니, 이런 호환성 문제들은 모두 Rust nightly 버전이 문제로 보입니다. 그러면 HLeOs가 개발될 당시의 Rust nightly 버전을 사용하면 이런 문제들이 없을 것 같습니다.

#### 과거 Rust nightly 버전 설치

HLeOs의 마지막 커밋 날짜를 확인해봤더니 2025년 2월 22일이었습니다. 따라서 이 시점에서 한 달 전 정도인 2025-01-15 버전의 Rust nightly를 설치했습니다.

```bash
rustup install nightly-2025-01-15
rustup override set nightly-2025-01-15
rustup component add rust-src --toolchain nightly-2025-01-15-x86_64-unknown-linux-gnu
rustup component add llvm-tools-preview --toolchain nightly-2025-01-15-x86_64-unknown-linux-gnu
```

### 다섯 번째 빌드 시도

2025-01-15 nightly로 빌드를 시도했습니다:

```bash
cargo clean
make
```

이번엔 x86_64 크레이트에서 에러가 발생했습니다. bootloader 0.9.8이 사용하는 x86_64 크레이트 버전이 오래되어, 최신 Rust와 호환되지 않는 것으로 판단했습니다.

#### bootloader 버전 변경

해당 프로젝트 README의 Troubleshooting 부분에서 비슷한 부분을 발견하여, bootloader 버전을 0.9.23으로 올려봤습니다:

```toml
bootloader = "=0.9.23"
```

그랬더니, 빌드 성공했습니다.

```
Created bootimage for `HLeOs` at `/home/seolcu/문서/코드/mini-kvm/HLeOs/HLeOs/target/x86_64-HLeos/debug/bootimage-HLeOs.bin`
```

### HLeOs 실행

#### Makefile 경로 수정

`make run`을 실행하니 Makefile에 하드코딩된 경로 때문에 에러가 발생했습니다. 7번 줄의 `WORKING_DIR`을 수정했습니다:

```makefile
WORKING_DIR=/home/seolcu/문서/코드/mini-kvm/HLeOs/HLeOs
```

#### xauth 에러 해결

다시 `make run`을 실행하니 xauth 에러가 발생했습니다:

```
xauth:  file /home/seolcu/.Xauthority does not exist
xauth: (argv):1:  bad "add" command line
```

Makefile 24번째 줄의 xauth 라인은 X11 forwarding을 위한 것인데, 저는 Wayland를 사용중이라 `.Xauthority` 파일이 없어서 문제가 발생한 것 같았습니다. 따라서 이 라인을 주석 처리했습니다:

```makefile
run : $(IMAGE_DIR)
	# sudo xauth add `xauth -f $(HOME)/.Xauthority list|tail -1`
	sudo qemu-system-x86_64 -drive format=raw,file=$(IMAGE_DIR) -enable-kvm -m 2G
```

이제 `make run`을 했더니, 다음과 같은 결과가 나왔습니다.

![alt text](image.png)

QEMU가 실행되고 화면에 엄청나게 빠른 속도로 `Hello!Hi!Hello!Hi!...` 같은 문자들이 출력되기 시작했습니다. 정상적으로 실행되고 있는 것으로 보입니다.

### 다음주 todo

- 코드 분석, 개발

---

## Week 09 - 교수님 미팅

# 9주차 상담내용 (10/28)

1. 프로젝트 개요

- 목표: RISC-V 아키텍처 기반의 하이퍼바이저(HV) 개발
- 주요 제약:
  - 개발 호스트 환경: x86
  - 프로젝트 기한: 약 1개월 (소프트콘)

2. 기존 계획 (x86 네이티브) 검토

- 내용: x86 호스트에서 직접 실행되는 마이크로 VMM(하이퍼바이저) 개발
- 문제점: x86 아키텍처의 높은 복잡성 (메모리 페이징, 디바이스 및 인터럽트 초기화 등)으로 인해 1개월 내 구현 불가능하다고 판단
- 결론: x86 네이티브 하이퍼바이저 개발 계획 폐기

3. 변경된 아키텍처 (RISC-V 에뮬레이션)

- 구성 (첨부 이미지 참조):
  - Host: x86 (실제 하드웨어)
  - 에뮬레이터: Qemu (RISC-V 환경 에뮬레이션)
  - 에뮬레이트된 OS: Linux (RISC-V 버전, KVM 모듈 포함)
  - 개발 대상: 하이퍼바이저 (HV) (RISC-V 리눅스 상에서 실행)
  - Guest OS: Toy OS 또는 Xv6 (개발 HV 상에서 실행)
- 특징: Qemu로 RISC-V 환경을 구성하고, 그 안의 리눅스 KVM 기능을 활용하는 방식

4. 핵심 개발 과제

- 목표: Guest OS가 발생시킨 하이퍼콜(Hypercall)을 개발 HV가 수신
- 처리: 수신된 하이퍼콜을 Qemu 내 RISC-V 리눅스의 KVM 모듈로 전달 및 처리
- 구현 방안:
  - 개발 HV가 Qemu 내 리눅스의 KVM 인터페이스(/dev/kvm)를 사용.
  - Guest OS의 하이퍼콜을 KVM ioctl 호출 (VCPU 생성/실행, 메모리 설정 등)로 변환하는 '브릿지' 역할 수행.
  - 필요시 Qemu의 KVM API 사용 방식(소스 코드) 참조.

5. 세부 실행 계획 (To-Do)

- 기본 환경 구축: x86 호스트에 Qemu를 설치하고, 그 위에 RISC-V 리눅스(KVM 포함)를 실행.
- Guest OS 구현: 하이퍼콜을 호출하는 간단한 Guest OS (Toy OS) 코드 준비 (기존 작업분 활용).
- HV-KVM 브릿지 구현 (핵심):
  - RISC-V 리눅스 환경에서 /dev/kvm을 열어 VCPU를 생성.
  - Guest OS의 하이퍼콜을 받아 KVM을 통해 VCPU 상태(레지스터 등)를 가져오고 실행을 관리하는 기능 구현.

## Week 09 - 연구 진행 내용

# 9주차 연구내용

목표: KVM 기반 하이퍼바이저로 전환

## 연구 내용

저번주에 교수님과의 상담을 통해 x86 기반 하이퍼바이저 개발에서, RISC-V Linux 위에서 KVM API를 사용하는 방식으로 변경하기로 했습니다.

### 새로운 아키텍처

```
┌─────────────────────────────────────┐
│  Guest OS (Toy OS / xv6-riscv)      │  ← hypercall 전송
├─────────────────────────────────────┤
│  하이퍼바이저 (Rust)                   │  ← 개발 대상
├─────────────────────────────────────┤
│  RISC-V Linux + KVM 모듈             │  ← /dev/kvm 인터페이스 사용
├─────────────────────────────────────┤
│  QEMU (RISC-V 에뮬레이션)             │  ← H-extension
├─────────────────────────────────────┤
│  x86 호스트                          │  ← 개발 머신
└─────────────────────────────────────┘
```

하이퍼바이저는 다음과 같은 역할을 합니다:

- Guest OS로부터 hypercall을 받음
- KVM ioctl 호출로 변환 (VCPU 생성/실행, 메모리 설정 등)
- RISC-V Linux의 `/dev/kvm` 인터페이스 사용

### 환경 설정

#### RISC-V 크로스컴파일러 설치

QEMU는 이미 full 버전으로 설치되어 있습니다.

RISC-V Linux 커널을 빌드하려면 크로스컴파일러가 필요하기에, 다음 패키지들을 설치했습니다:

```bash
sudo pacman -S riscv64-linux-gnu-gcc riscv64-linux-gnu-binutils riscv64-linux-gnu-glibc
```

```bash
riscv64-linux-gnu-gcc --version
```

```
riscv64-linux-gnu-gcc (GCC) 15.1.0
Copyright (C) 2025 Free Software Foundation, Inc.
This is free software; see the source for copying conditions.  There is NO
warranty; not even for MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.
```

정상적으로 설치되었습니다.

#### RISC-V Linux 부팅 테스트

RISC-V Linux 커널을 직접 빌드하면 시간이 오래 걸리므로, 우선 pre-built 이미지로 RISC-V Linux가 정상적으로 부팅되는지 확인해보기로 했습니다.

[Debian RISC-V 위키](https://wiki.debian.org/RISC-V)를 찾아보니 [Debian installer 이미지](wget https://deb.debian.org/debian/dists/sid/main/installer-riscv64/current/images/netboot/debian-installer/riscv64/)가 제공되고 있었습니다.

따라서 Linux과 initrd를 받아서 바로 QEMU로 부팅을 시도했습니다. 이전 bare-metal 하이퍼바이저 프로젝트(1000hv 튜토리얼)에서 QEMU를 사용할 때 `-cpu rv64,h=true` 옵션을 사용했던 것이 기억났습니다. 검색해보니 `h=true`는 RISC-V H-extension (Hypervisor extension)을 활성화하는 옵션이었습니다. KVM도 H-extension을 요구하기에 다음과 같이 옵션을 작성해 부팅해보았습니다.

```bash
qemu-system-riscv64 -machine virt -cpu rv64,h=true -m 2G \
  -kernel linux -initrd initrd.gz -nographic \
  -append "console=ttyS0"
```

![alt text](image-1.png)

성공적으로 인스톨러가 부팅되었습니다.

이후 QEMU 옵션을 조정하며 네트워크 등의 장치를 설정해 정상 설치했으나, 결과적으로 `/dev/kvm`을 찾을 수 없었습니다. 알고보니, 커널에 KVM 모듈이 포함되어 있지 않았습니다. 우분투와 도커도 시도해보았으나, 같은 문제가 발생했습니다.

#### 결론 및 방향 설정

pre-built 이미지들로는 KVM을 테스트할 수 없었습니다:

따라서 직접 Linux 커널을 빌드하기로 결정했습니다.

---

## Week 10 - 교수님 미팅

# 10주차 상담내용

- 이제 3주 가량 남았으니 서둘러야 한다.
- [미니멀한 리눅스 컴파일 옵션](../config-vm)을 슬랙으로 제공했으니 그걸 사용하면 컴파일이 빠를것이다

## Week 10 - 연구 진행 내용

# 10주차 연구내용

목표: Phase 1 완료 (RISC-V Linux + KVM) 및 Phase 2 완전 구현 (x86 KVM VMM)

## 저번주 todo:
- [x] KVM vmm과 guest를 모두 만들고 그들이 통신하는 것을 구현
- [x] 9주차 내용을 정리

## 연구 내용

### RISC-V Linux + KVM 환경 구축

#### 환경 설정

RISC-V 크로스 컴파일 도구체인을 설치했습니다:

```bash
gcc-riscv64-linux-gnu (15.2.1)
gcc-c++-riscv64-linux-gnu
binutils-riscv64-linux-gnu
qemu-system-riscv (10.1.2)
```

#### RISC-V Linux 커널 빌드

Linux 6.17.7 커널을 다음과 같이 빌드했습니다:

```bash
wget https://cdn.kernel.org/pub/linux/kernel/v6.x/linux-6.17.7.tar.xz
tar -xf linux-6.17.7.tar.xz
cd linux-6.17.7

make ARCH=riscv CROSS_COMPILE=riscv64-linux-gnu- defconfig
sed -i 's/CONFIG_KVM=m/CONFIG_KVM=y/' .config
make ARCH=riscv CROSS_COMPILE=riscv64-linux-gnu- -j$(nproc)
```

결과물은 `arch/riscv/boot/Image` (27MB) 입니다.

#### Initramfs 구성

간단한 initramfs를 만들어 부팅 시 KVM 확인이 가능하도록 했습니다:

```bash
cd initramfs
riscv64-linux-gnu-as -o init.o init.S
riscv64-linux-gnu-ld -static -nostdlib -o init init.o
chmod +x init
find . -print0 | cpio --null -o --format=newc | gzip > ../initramfs.cpio.gz
```

결과물은 `initramfs.cpio.gz` (2.1KB) 입니다.

#### QEMU에서 부팅

다음 명령으로 RISC-V Linux를 부팅했습니다:

```bash
qemu-system-riscv64 \
  -machine virt \
  -cpu rv64,h=true \
  -m 2G \
  -nographic \
  -kernel linux-6.17.7/arch/riscv/boot/Image \
  -initrd initramfs.cpio.gz \
  -append "console=ttyS0"
```

부팅 후 다음 메시지로 KVM이 정상 작동함을 확인했습니다:

```
[    0.276728] kvm [1]: hypervisor extension available
[    0.276802] kvm [1]: using Sv57x4 G-stage page table format
```

RISC-V Linux + KVM 환경 구축이 완료되었습니다.

---

### x86 KVM VMM 개발로의 전환

#### 도구체인 문제

RISC-V 타겟으로 KVM VMM을 C로 개발하려 했으나 다음 문제에 직면했습니다:

- Fedora 배포판: RISC-V 크로스 컴파일러(riscv64-linux-gnu-gcc)만 제공
- 표준 라이브러리 부재: glibc, libgcc 등이 없음
- 결과: C 프로그램 컴파일 불가 (stdio.h, libc 등 접근 불가)
- Rust도 동일: `cargo build --target riscv64gc-unknown-linux-gnu` 실패

어셈블리 게스트는 libc 의존성 없이 가능했지만, C로 작성해야 하는 VMM 개발은 불가능했습니다.

이를 해결하기 위해 x86 타겟으로 개발을 진행했습니다. KVM API는 아키텍처 독립적이므로, 가상화 원리와 VMM 구현 방식은 동일하게 적용됩니다.

---

### x86 KVM VMM 구현 및 게스트 프로그램

x86 KVM VMM을 C로 구현했습니다. 주요 구조는:

```c
int main() {
    init_kvm();              // /dev/kvm 열기, VM 생성
    setup_guest_memory();    // 1MB 메모리 할당 및 매핑
    load_guest_binary();     // 게스트 바이너리 로드
    setup_vcpu();            // vCPU 생성, Real Mode 레지스터 설정
    run_vm();                // VM 실행 및 exit 처리
}
```

구현한 기능:
- KVM API: open → CREATE_VM → SET_USER_MEMORY_REGION → CREATE_VCPU → RUN
- Real Mode: Segment × 16 + Offset으로 주소 계산
- VM Exit: HLT, I/O, MMIO 처리
- I/O 에뮬레이션: UART 포트 0x3f8 (COM1) 문자 출력

결과적으로 "Hello, KVM!" 출력과 0-9 카운터가 성공적으로 작동했으며, 약 370줄의 완전한 VMM이 구현되었습니다.

#### Hypercall 시스템 및 복잡한 게스트

게스트-VMM 간 효율적인 통신을 위해 Hypercall 시스템을 설계했습니다.

Hypercall 인터페이스 정의:

```c
#define HYPERCALL_PORT 0x500

#define HC_EXIT       0x00    // Guest requests exit
#define HC_PUTCHAR    0x01    // Output character (BL = char)
#define HC_PUTNUM     0x02    // Output number (BX = decimal number)
#define HC_NEWLINE    0x03    // Output newline
```

게스트 측 사용법 (assembly):

```asm
mov $42, %bx            ; BX = 출력할 숫자
mov $HC_PUTNUM, %al     ; AL = hypercall 번호
mov $HYPERCALL_PORT, %dx
out %al, (%dx)          ; Hypercall 실행
```

VMM 측 처리 (C):

```c
static int handle_hypercall(struct kvm_regs *regs) {
    unsigned char hc_num = regs->rax & 0xFF;

    switch (hc_num) {
        case HC_EXIT:
            return 1;  // Signal guest exit
        case HC_PUTCHAR:
            putchar(regs->rbx & 0xFF);  // BL = character
            break;
        case HC_PUTNUM:
            printf("%u", regs->rbx & 0xFFFF);  // BX = number
            break;
        case HC_NEWLINE:
            putchar('\n');
            break;
    }
    return 0;
}
```

Hypercall 시스템의 장점은 게스트 코드 단순화, 확장성 향상, 그리고 실제 OS의 syscall 메커니즘과 동일한 패턴이라는 점입니다.

---

### 구현한 게스트 프로그램

5개의 게스트 프로그램을 증가하는 복잡도 순서로 구현했습니다.

#### minimal.S (1바이트)

```asm
hlt
```

가장 단순한 게스트로, VMM 기본 기능을 테스트합니다. VM exit는 1회입니다.

#### hello.S (28바이트)

```asm
mov $message, %si
print_loop:
    lodsb
    test %al, %al
    jz done
    mov $0x3f8, %dx
    out %al, (%dx)
    jmp print_loop
done:
    hlt
```

"Hello, KVM!\n"을 출력합니다. 문자열 루핑과 UART I/O를 테스트합니다. VM exit는 13회입니다.

#### counter.S (18바이트)

```asm
mov $0, %cl
print_loop:
    add $0x30, %cl
    mov $0x3f8, %dx
    out %cl, (%dx)
    inc %cl
    cmp $10, %cl
    jl print_loop
```

0부터 9까지 출력합니다. 루프와 산술 연산을 테스트합니다. VM exit는 11회입니다.

#### hctest.S (79바이트)

모든 4가지 Hypercall 타입을 테스트합니다. "Hello!\n42\n1234\n"을 출력합니다. VM exit는 13회입니다.

#### multiplication.S (112바이트)

가장 복잡한 게스트로, 2부터 9까지의 구구단을 출력합니다:

```asm
mov $2, %cl              ; CL = dan (2-9)
outer_loop:
    mov $1, %ch          ; CH = multiplier (1-9)
    inner_loop:
        ; Print dan
        movzx %cl, %bx
        mov $HC_PUTNUM, %al
        mov $HYPERCALL_PORT, %dx
        out %al, (%dx)

        ; Print " x "
        mov $' ', %bl
        mov $HC_PUTCHAR, %al
        out %al, (%dx)
        ; ... (space, x, space 반복)

        ; Print multiplier
        movzx %ch, %bx
        mov $HC_PUTNUM, %al
        out %al, (%dx)

        ; Print " = "
        ; ... (similar)

        ; Calculate & print result
        mov %cl, %al
        mov %ch, %bl
        mul %bl
        movzx %al, %bx
        mov $HC_PUTNUM, %al
        out %al, (%dx)

        ; Print newline
        mov $HC_NEWLINE, %al
        out %al, (%dx)

        ; Continue inner loop
        inc %ch
        cmp $10, %ch
        jl inner_loop

    ; Continue outer loop
    inc %cl
    cmp $10, %cl
    jl outer_loop
```

2×1=2부터 9×9=81까지 18줄을 출력합니다. VM exit는 181회입니다 (18줄 × 10 hypercall/줄 + 1 HLT).

---

### 핵심 기술: 레지스터 압박 해결

x86 16비트 Real Mode에서 필요한 레지스터가 범용 레지스터 4개(AX, BX, CX, DX)보다 많아서 문제가 발생했습니다.

필요한 것:
- CL: 외부 루프 (dan)
- DX: 포트 번호 (0x500)
- AL: hypercall 번호
- BX: hypercall 인자
- AX: MUL 결과

핵심 통찰은 CX와 DX가 독립적인 레지스터라는 것입니다. CX = CL(하위 8비트) + CH(상위 8비트)이고, DX = DL(하위 8비트) + DH(상위 8비트)이므로, CL과 CH를 각각 다른 루프 변수로 사용할 수 있습니다:

```asm
mov $2, %cl             ; CL = dan (외부 루프)
mov $1, %ch             ; CH = multiplier (내부 루프)
mov $HYPERCALL_PORT, %dx ; DX = 포트 번호
; CL 값은 여전히 안전함! DX를 사용해도 영향 없음
```

MUL 명령어도 최적화했습니다. MUL 명령어는 AL × operand → AX이므로, operand로 CL을 사용하면 루프 카운터가 파괴됩니다. 대신 BL을 사용합니다:

```asm
mov %cl, %al            ; AL = dan
mov %ch, %bl            ; BL = multiplier (BH는 사용하지 않음)
mul %bl                 ; AX = AL × BL
                        ; CL은 보호됨
movzx %al, %bx          ; BX = result
```

---

### 빌드 시스템

Makefile에 패턴 룰을 추가하여 모든 .S 파일을 자동으로 빌드할 수 있도록 했습니다:

```makefile
build-%: guest/%.S
    @echo "=== Building guest/$*.S ==="
    as -32 -o $*.o guest/$*.S
    ld -m elf_i386 -T guest/guest.ld -o $*.elf $*.o
    objcopy -O binary -j .text -j .rodata $*.elf $*.bin

run-%: vmm guest/%.bin
    @echo "=== Running VMM (guest/$*.bin) ==="
    ./$(TARGET) guest/$*.bin

multiplication: vmm build-multiplication run-multiplication
counter: vmm build-counter run-counter
hello: vmm build-hello run-hello
hctest: vmm build-hctest run-hctest
minimal: vmm build-minimal run-minimal
```

사용법:

```bash
make multiplication      # 빌드 + 실행
make build-counter      # 빌드만
make run-hello          # 실행만
make clean              # 정리
```

---

### 성능 분석

VM exit 통계:

```
프로그램         크기      Exit 수    시간 (추정)
─────────────────────────────────
minimal          1 byte    1         ~1ms
hello           28 bytes   13        ~5ms
counter         18 bytes   11        ~4ms
hctest          79 bytes   13        ~6ms
multiplication 112 bytes  181       ~200ms
```

구구단의 경우, 18줄 × 10 hypercall/줄 = 180 exits + 1 HLT = 181 exits입니다.

최적화 기회로는 Virtio 모델의 버퍼링이 있습니다. 게스트가 공유 메모리 버퍼에 직접 쓰고, 버퍼 가득 차면 한 번의 notify hypercall을 실행하면, 180 exits가 18 exits로 줄어들어 10배 성능 향상을 기대할 수 있습니다. 이것이 실무에서 Virtio 큐가 사용되는 이유입니다.

---

### 코드 규모

VMM (src/main.c): 450줄
- init_kvm: 25줄
- setup_guest_memory: 30줄
- setup_vcpu: 70줄
- handle_hypercall: 35줄
- run_vm: 100줄
- 기타: 190줄

Guest 프로그램 (총 238바이트):
- minimal.S: 1바이트
- hello.S: 28바이트
- counter.S: 18바이트
- hctest.S: 79바이트
- multiplication.S: 112바이트

---

### 주요 학습 포인트

#### 하드웨어 가상화의 효율성

소프트웨어 에뮬레이션(QEMU TCG)은 모든 명령어를 해석해야 하므로 성능이 약 1/100 네이티브입니다. 반면 하드웨어 지원(KVM VT-x)은 민감한 명령어만 trap하므로 성능이 약 1/1.5 네이티브입니다. 이번 구현을 통해 그 차이를 직접 경험했습니다.

#### 레지스터 최적화의 중요성

x86의 극도로 제한된 레지스터(4개)는 제약이지만, CL/CH 분리와 BL 활용으로 해결할 수 있습니다. 이는 아키텍처를 깊이 있게 이해해야 함을 의미합니다.

#### Hypercall 패턴의 우아함

직접 I/O (각 문자마다 OUT)에서 Hypercall (복잡한 연산은 VMM에)로 전환하면, 게스트 코드가 단순해지고 확장성이 향상됩니다. 이는 실제 OS의 syscall 메커니즘과 동일한 패턴입니다.

---

## 다음 계획 (Week 11-16)

### Week 11: IN 명령어 + UART 입력 지원
- KVM_EXIT_IO 핸들러 확장 (IN 명령어 처리)
- UART 입력 에뮬레이션 (포트 0x3f8)
- Protected Mode 입력 게스트 개발 (echo, add 등)
- 호환성 검증 및 테스트

### Week 12: 토이 OS 포팅 (xv6 또는 HLeOs)
- x86 보호 모드를 활용한 OS 부팅
- 시스템콜 또는 하이퍼콜 기반 기능 통합
- 기본 I/O 및 프로세스 관리 검증

### Week 13: 최적화 및 최종 테스트
- VM exit 수 최적화 (버퍼링, Virtio 패턴)
- 전체 시스템 성능 측정
- 모든 구성 요소 통합 테스트

### Week 14-15: 최종 보고서 및 데모
- 기술 문서 작성
- 데모 영상 제작
- 학습 내용 종합 정리

### Week 16: 최종 제출
- 최종 보고서 및 데모 제출

---

## 결론

**Week 10 완료 내용:**

1. RISC-V Linux + KVM 환경 구축 (커널 빌드, initramfs, 부팅 확인)
2. x86 KVM VMM 완전 구현 (450줄 C 코드)
3. Hypercall 시스템 설계 및 구현 (4가지 operation)
4. Protected Mode 지원 추가 (GDT, IDT, 32비트 게스트)
5. 7개 게스트 프로그램 (1바이트부터 112바이트, 32비트 보호 모드까지)

**프로젝트 상태:**
- Real Mode 게스트: 5개 모두 정상 작동
- Protected Mode 게스트: 2개 정상 작동
- 도구체인 문제 해결 및 아키텍처 선택 완료
- Week 11-16: 입력 기능, OS 포팅, 최종 보고서 (충분한 시간 확보)

---

## 참고 자료

### 공식 문서
- [KVM API Documentation](https://www.kernel.org/doc/html/latest/virt/kvm/api.html)
- [Intel 64 and IA-32 Architectures Software Developer Manual](https://www.intel.com/content/www/us/en/developer/articles/technical/intel-sdm.html)
- [RISC-V Privileged Specification](https://github.com/riscv/riscv-isa-manual)

### 참고 프로젝트
- [kvmtool - Lightweight VMM](https://github.com/kvmtool/kvmtool)
- [QEMU - Full System Emulator](https://www.qemu.org/)
- [rust-vmm - KVM wrappers for Rust](https://github.com/rust-vmm)

### 학습 자료
- KVM 소스: https://github.com/torvalds/linux/tree/master/virt/kvm
- x86 Assembly Guide: http://www.cs.virginia.edu/~evans/cs216/guides/x86.html
- LWN.net - Using the KVM API: https://lwn.net/Articles/658511/

---

## Week 11 - 교수님 미팅

# 11주차 상담내용 (11/11)

## 현황 보고

### 진행 상황

- RISC-V에서 x86 KVM VMM으로 전환 완료
- Real Mode 게스트 5개 정상 작동 (minimal, hello, counter, hctest, multiplication)
- Protected Mode 게스트 2개 추가 구현 (pmode_simple, pmode_test)
- 총 7개 게스트 프로그램 정상 작동 확인

### 구현된 기능

- KVM API를 이용한 VMM (C, 약 450줄)
- Hypercall 시스템 (HC_EXIT, HC_PUTCHAR, HC_PUTNUM, HC_NEWLINE)
- Real Mode 16비트 게스트 지원
- Protected Mode 32비트 게스트 지원
- GDT, IDT 기본 설정

## 기술 논의

### 인터럽트 처리

현재 인터럽트 디스크립터 테이블(IDT)이 기본 설정 상태(빈 상태)로 구현되어 있음.

**현 상황:**

- 인터럽트 없이도 정상 작동 (hypercall 기반 I/O만 사용)
- 키보드 입력 같은 인터럽트 기반 기능은 미지원

**앞으로의 필요성:**

- 더 복잡한 OS를 포팅하려면 인터럽트 처리 필수
- [1K OS](https://operating-system-in-1000-lines.vercel.app/en/) 같은 토이 OS는 인터럽트 처리를 포함

### 페이징 및 메모리

- 게스트 메모리: 간단한 청크 기반 관리 (4MB 연속 메모리)
- 페이징은 현재 불필요 (단순 게스트와 토이 OS용)

## 다음 주(Week 12) 계획

### 1차 목표: 멀티 게스트 지원

- 여러 vCPU를 별도로 생성하여 동시 실행
- 예: 하나의 VMM에서 2-4개의 서로 다른 게스트 병렬 실행
- 구현: KVM_CREATE_VCPU를 여러 번 호출하고 각 게스트별로 초기화
- 데모: 화면을 나눠서 각 게스트의 출력을 동시에 표시

**예시 게스트 조합:**

- vCPU 1: multiplication.S (구구단)
- vCPU 2: counter.S (0-9 카운트)
- vCPU 3: hello.S ("Hello, KVM!")
- vCPU 4: hctest.S (hypercall 테스트)

### 2차 목표: 1K OS 포팅

- Rust로 작성된 x86 토이 OS를 VMM 위에서 실행
- 현재 어셈블리 게스트의 한계를 넘어 더 복잡한 기능 구현 가능
- 인터럽트 처리 예제 포함 (1K OS에 구현되어 있음)

**필요 작업:**

- 1K OS 소스 분석
- x86 KVM 환경에 맞게 부트로더 수정 (필요시)
- 게스트 메모리 레이아웃 조정

### 3차 목표: 성능 비교 분석

매트릭스 곱셈 또는 대규모 계산을 통해 성능 차이 측정:

**비교 대상:**

1. **KVM 모드 (현재):** 호스트 CPU 직접 사용
2. **QEMU 에뮬레이션 모드:** 호스트가 x86 명령어를 완전히 에뮬레이션

**측정 방식:**

- 10×10 또는 그 이상 크기의 매트릭스 곱셈 구현
- KVM과 에뮬레이션 모드 모두에서 실행
- 시간 측정 및 성능 비율 비교 (예: 10배, 100배 차이)
- 결과를 최종 발표 자료에 포함

## 최종 발표 자료 방향

### 구성

1. **개념 설명**

   - KVM과 에뮬레이션의 차이
   - 가상화 기법 개요

2. **구현 과정**

   - RISC-V 도구체인 문제 → x86으로 아키텍처 변경
   - 반복된 설계 재검토 (3-4회)
   - Real Mode → Protected Mode로의 발전

3. **성능 분석**

   - KVM vs 에뮬레이션 성능 비교
   - 벤치마크 결과 및 해석

4. **기술 시연**
   - 멀티 게스트 동시 실행 영상
   - 1K OS 부팅 및 실행 영상

## 요점 정리

### 교수 피드백

- "생각보다 빨리 진행됐다" - 1주일 만에 기초 완성
- "AI 도구 효과적" - 설정 및 복잡한 부분을 잘 처리함
- "멀티 게스트 지원은 상대적으로 쉽다" - KVM_CREATE_VCPU 여러 번 호출로 가능
- "1K OS는 포팅이 빨리 될 것으로 예상"

### 앞으로의 강점

- 이미 기본 VMM이 완성되어 있어 확장이 용이
- Protected Mode 지원으로 복잡한 OS 포팅 가능
- 문서와 코드가 잘 정리되어 있음

### 도전 과제

- 인터럽트 처리는 상대적으로 복잡
- 1K OS의 특정 기능이 x86 KVM에서 미지원될 가능성
- 성능 비교 시 동일한 환경에서의 측정 필요

## 다음 미팅 예정

- Week 12 중 (멀티 게스트 진행 상황 보고)
- 최종 발표 준비 상황 논의

## Week 11 - 연구 진행 내용

# 11주차 연구내용

목표: 1K OS 포팅 준비 - Protected Mode with Paging 지원 구현

저번주 todo:
- [x] Multi-vCPU 지원 구현 (2-4 guests 동시 실행)
- [x] 1K OS 포팅 시작 (Protected Mode + Paging)

## 연구 내용

### Multi-vCPU 지원 구현

Week 11의 첫 번째 목표는 여러 게스트 프로그램을 동시에 실행할 수 있도록 멀티 vCPU 지원을 추가하는 것이었습니다.

#### 아키텍처 설계

기존 단일 vCPU 구조를 확장하여 최대 4개의 vCPU를 동시에 실행할 수 있도록 설계했습니다:

```
┌─────────────────────────────────────┐
│  Host (Linux x86_64)                │
│  ┌───────────────────────────────┐  │
│  │ VMM Process (kvm-vmm)         │  │
│  │  ┌─────────┬─────────┬─────┐ │  │
│  │  │Thread 0 │Thread 1 │ ... │ │  │
│  │  │vCPU 0   │vCPU 1   │     │ │  │
│  │  └─────────┴─────────┴─────┘ │  │
│  └───────────────────────────────┘  │
│  ┌───────────────────────────────┐  │
│  │ KVM (/dev/kvm)                │  │
│  │  VM (single VM, multiple vCPUs)│ │
│  └───────────────────────────────┘  │
└─────────────────────────────────────┘

Memory Layout:
  vCPU 0: GPA 0x00000 - 0x3FFFF (256KB)
  vCPU 1: GPA 0x40000 - 0x7FFFF (256KB)
  vCPU 2: GPA 0x80000 - 0xBFFFF (256KB)
  vCPU 3: GPA 0xC0000 - 0xFFFFF (256KB)
```

각 vCPU는 독립적인 메모리 영역과 레지스터 상태를 가지며, pthread를 통해 병렬 실행됩니다.

#### 구현 상세

**1. Per-vCPU Context 구조체**

```c
typedef struct {
    int vcpu_id;                  // vCPU index (0-3)
    int vcpu_fd;                  // KVM vCPU file descriptor
    struct kvm_run *kvm_run;      // Per-vCPU run structure
    void *guest_mem;              // Per-guest memory region
    size_t mem_size;              // Memory size (256KB)
    const char *guest_binary;     // Binary filename
    char name[256];               // Display name
    int exit_count;               // VM exit counter
    bool running;                 // Execution state
} vcpu_context_t;
```

각 vCPU가 독립적인 상태를 유지하도록 모든 정보를 context 구조체에 캡슐화했습니다.

**2. 메모리 할당 및 매핑**

```c
static int setup_vcpu_memory(vcpu_context_t *ctx) {
    ctx->mem_size = 256 * 1024;  // 256KB per vCPU
    ctx->guest_mem = mmap(NULL, ctx->mem_size, PROT_READ | PROT_WRITE,
                          MAP_PRIVATE | MAP_ANONYMOUS, -1, 0);

    struct kvm_userspace_memory_region mem_region = {
        .slot = ctx->vcpu_id,
        .flags = 0,
        .guest_phys_addr = ctx->vcpu_id * ctx->mem_size,  // Offset GPA
        .memory_size = ctx->mem_size,
        .userspace_addr = (uint64_t)ctx->guest_mem,
    };

    ioctl(vm_fd, KVM_SET_USER_MEMORY_REGION, &mem_region);
}
```

각 vCPU의 메모리를 서로 다른 GPA에 매핑하여 격리를 보장합니다.

**3. Thread-safe 출력**

여러 vCPU가 동시에 출력할 때 섞이지 않도록 mutex와 ANSI 색상 코드를 사용했습니다:

```c
static pthread_mutex_t stdout_mutex = PTHREAD_MUTEX_INITIALIZER;

static void vcpu_printf(vcpu_context_t *ctx, const char *fmt, ...) {
    pthread_mutex_lock(&stdout_mutex);

    // vCPU별 색상: red, green, yellow, blue
    const char *colors[] = {"\033[31m", "\033[32m", "\033[33m", "\033[34m"};

    printf("%s[vCPU %d:%s]%s ", colors[ctx->vcpu_id],
           ctx->vcpu_id, ctx->name, "\033[0m");

    va_list args;
    va_start(args, fmt);
    vprintf(fmt, args);
    va_end(args);

    pthread_mutex_unlock(&stdout_mutex);
}
```

**4. Pthread 기반 실행**

```c
static void *vcpu_thread(void *arg) {
    vcpu_context_t *ctx = (vcpu_context_t *)arg;

    while (ctx->running) {
        ioctl(ctx->vcpu_fd, KVM_RUN, 0);
        handle_vm_exit(ctx);
    }

    return NULL;
}

int main(int argc, char **argv) {
    pthread_t threads[MAX_VCPUS];

    // Spawn vCPU threads
    for (int i = 0; i < num_vcpus; i++) {
        pthread_create(&threads[i], NULL, vcpu_thread, &vcpus[i]);
    }

    // Wait for all vCPUs
    for (int i = 0; i < num_vcpus; i++) {
        pthread_join(threads[i], NULL);
    }
}
```

#### 테스트 결과

**2-vCPU 테스트:**
```bash
./kvm-vmm guest/multiplication.bin guest/counter.bin
```

출력:
```
[vCPU 0:multiplication] 2 x 1 = 2
[vCPU 1:counter] 0
[vCPU 0:multiplication] 2 x 2 = 4
[vCPU 1:counter] 1
...
```

두 게스트가 병렬로 실행되면서 출력이 인터리빙되는 것을 확인했습니다.

**4-vCPU 테스트:**
```bash
./kvm-vmm guest/multiplication.bin guest/counter.bin \
          guest/hello.bin guest/hctest.bin
```

4개의 게스트가 동시에 실행되며, 각각 색상으로 구분된 출력을 생성했습니다.

#### 주요 도전과제

**1. Static Buffer 버그**

초기 구현에서 `extract_guest_name()` 함수가 static buffer를 사용하여 여러 스레드에서 동시에 호출될 때 race condition이 발생했습니다. 이를 per-context buffer로 변경하여 해결했습니다.

**2. Real Mode CS:IP 설정**

각 vCPU는 서로 다른 GPA에서 시작해야 하므로, CS:IP를 적절히 설정해야 했습니다:
- vCPU 0: CS=0x0000, IP=0x0 (물리 주소 0x00000)
- vCPU 1: CS=0x4000, IP=0x0 (물리 주소 0x40000)
- vCPU 2: CS=0x8000, IP=0x0 (물리 주소 0x80000)
- vCPU 3: CS=0xC000, IP=0x0 (물리 주소 0xC0000)

**3. VM Exit 동기화**

각 vCPU의 VM exit을 독립적으로 처리하면서도 출력이 섞이지 않도록 mutex로 보호했습니다.

### 1K OS 프로젝트 분석

Week 11에서는 [Operating System in 1000 Lines](https://operating-system-in-1000-lines.vercel.app/en/) 프로젝트를 x86 KVM VMM으로 포팅하는 작업을 시작했습니다. 이 프로젝트는 원래 RISC-V RV32 아키텍처를 대상으로 작성된 교육용 OS로, 약 1000줄의 코드로 다음 기능을 구현합니다:

- Paging을 통한 메모리 관리
- 멀티태스킹 및 프로세스 관리
- Tar 파일시스템
- 간단한 Shell

RISC-V에서 x86으로 포팅하기 위해서는 먼저 x86 Protected Mode with Paging 환경을 구축해야 했습니다.

### Phase 2: Protected Mode with Paging 구현

#### 아키텍처 설계

x86 Protected Mode with Paging을 지원하기 위해 다음과 같은 메모리 레이아웃을 설계했습니다:

```
Physical Memory:
  0x00000000 - 0x003FFFFF  (4MB)

Virtual Memory (with paging):
  0x00000000 - 0x003FFFFF  (Identity mapping)
  0x80000000 - 0x803FFFFF  (Kernel space → 물리 0x0)

Page Directory:
  GPA 0x00100000 (1MB offset)
  - PDE[0]   = 0x00000083  (0x0-0x3FFFFF 매핑)
  - PDE[512] = 0x00000083  (0x80000000 가상 → 0x0 물리)
```

이 구조는 다음과 같은 장점이 있습니다:
- Identity mapping으로 부트 코드 실행 가능
- Higher-half kernel (0x80000000 이상)로 일반적인 OS 구조 유사
- 4MB PSE 페이지로 페이지 테이블 구조 단순화

#### VMM 확장

기존 Real Mode 전용 VMM을 확장하여 Protected Mode with Paging을 지원하도록 수정했습니다.

**1. 명령줄 인자 파싱 추가**

```c
bool enable_paging = false;
uint32_t entry_point = 0x80001000;
uint32_t load_offset = 0x1000;

// --paging 플래그 처리
if (strcmp(argv[i], "--paging") == 0) {
    enable_paging = true;
}
```

**2. 페이지 테이블 설정 함수**

```c
static int setup_page_tables(vcpu_context_t *ctx) {
    const uint32_t page_dir_offset = 0x00100000;
    uint32_t *page_dir = (uint32_t *)(ctx->guest_mem + page_dir_offset);

    memset(page_dir, 0, 4096);

    // Identity map: 0x0-0x3FFFFF
    page_dir[0] = 0x00000083;  // Present, R/W, 4MB page

    // Kernel space: 0x80000000-0x803FFFFF → 0x0-0x3FFFFF
    page_dir[512] = 0x00000083;

    return page_dir_offset;
}
```

**3. CR 레지스터 설정**

```c
sregs.cr3 = page_dir_offset;
sregs.cr0 |= 0x00000001;  // PE bit (Protected Mode)
sregs.cr0 |= 0x80000000;  // PG bit (Paging enabled)
sregs.cr4 |= 0x00000010;  // PSE bit (4MB pages)
```

**4. Flat Segment 설정**

GDT 없이 KVM의 unrestricted guest 모드를 활용하여 세그먼트를 직접 설정:

```c
sregs.cs.base = 0;
sregs.cs.limit = 0xFFFFFFFF;
sregs.cs.selector = 0x08;
sregs.cs.type = 0x0B;  // Execute/Read
sregs.cs.present = 1;
sregs.cs.dpl = 0;      // Ring 0
sregs.cs.db = 1;       // 32-bit
```

#### 테스트 커널 개발

Protected Mode with Paging을 테스트하기 위한 간단한 커널을 개발했습니다.

**1. boot.S - 부트 코드**

```asm
.code32
.section .text.boot
.global _start

_start:
    # VMM이 이미 Protected Mode + Paging 설정 완료
    # 세그먼트 레지스터 재로드 금지 (GDT 없음)

    # 스택 설정
    movl $__stack_top, %esp
    movl $0, %ebp

    # BSS 클리어
    movl $__bss, %edi
    movl $__bss_end, %ecx
    subl %edi, %ecx
    xorl %eax, %eax
    rep stosb

    # kernel_main 호출
    call kernel_main

    # Halt
1:  hlt
    jmp 1b
```

**2. kernel.ld - 링커 스크립트**

```ld
OUTPUT_FORMAT("elf32-i386")
ENTRY(_start)

SECTIONS {
    . = 0x80001000;  /* 가상 주소 */

    __kernel_base = .;

    .text.boot : { *(.text.boot) }
    .text : { *(.text*) }
    .rodata : ALIGN(4) { *(.rodata*) }
    .data : ALIGN(4) { *(.data*) }

    .bss : ALIGN(4) {
        __bss = .;
        *(.bss*)
        __bss_end = .;
    }

    __stack_bottom = .;
    . += 0x2000;  /* 8KB stack */
    __stack_top = .;

    __free_ram = .;
    __free_ram_end = 0x80400000;
}
```

**3. test_kernel.c - 테스트 코드**

Hypercall을 통한 출력 및 메모리 접근 테스트:

```c
static inline void hypercall_putchar(char c) {
    __asm__ volatile(
        "mov %0, %%bl\n\t"
        "mov %1, %%al\n\t"
        "mov %2, %%dx\n\t"
        "outb %%al, %%dx"
        :
        : "q"((uint8_t)c), "i"(0x01), "i"(0x500)
        : "al", "bl", "dx"
    );
}

void kernel_main(void) {
    puts("\n=== 1K OS x86 Test Kernel ===\n");

    // 커널 정보 출력
    print_hex((uint32_t)__bss);

    // 메모리 테스트
    volatile uint32_t *test_addr = (volatile uint32_t *)0x80010000;
    *test_addr = 0xDEADBEEF;
    if (*test_addr == 0xDEADBEEF) {
        puts("Memory test passed\n");
    }

    while (1) __asm__ volatile("hlt");
}
```

#### 트러블슈팅: Triple Fault 문제

초기 테스트에서 커널이 즉시 triple fault로 종료되는 문제가 발생했습니다.

**문제 발견 과정:**

1. **증상**: VM이 시작 직후 `KVM_EXIT_SHUTDOWN`으로 종료
2. **디버깅 1**: 단순한 어셈블리 코드는 정상 작동 확인
3. **디버깅 2**: CPU 상태 덤프 추가
   ```
   SHUTDOWN at RIP=0xfff0, RSP=0x0
   CR0=0x60000010  (PE, PG 비트 꺼짐!)
   CR3=0x0
   CS=0xf000  (Reset vector!)
   ```

4. **원인 분석**: CPU가 reset vector로 돌아감 → 설정이 적용 안됨

**근본 원인:**

boot.S의 다음 코드가 문제였습니다:

```asm
movl $0x10, %eax
movl %eax, %ds  # 세그먼트 레지스터 재로드!
```

Protected Mode에서 세그먼트 레지스터를 로드하면 GDT를 참조합니다. 하지만 우리는 GDT를 설정하지 않고 KVM의 unrestricted guest 모드로 세그먼트를 직접 설정했기 때문에, 이 명령이 protection fault를 발생시켰습니다.

**해결책:**

VMM이 이미 세그먼트 레지스터를 올바르게 설정했으므로, boot.S에서 세그먼트 레지스터 재로드 코드를 제거했습니다:

```asm
_start:
    # VMM이 이미 설정 완료 - 세그먼트 재로드 금지!
    movl $__stack_top, %esp  # 스택만 설정
    # ...
```

이 수정 후 커널이 정상적으로 실행되었습니다.

### 테스트 결과

최종 테스트 커널 실행 결과:

```
=== 1K OS x86 Test Kernel ===
Protected Mode with Paging Enabled

Kernel base:   0x80001000
BSS start:     0x80001624
BSS end:       0x80001624
Free RAM:      0x80004000 - 0x80400000

Testing memory access...
Memory test passed: 0x80010000 is writable

Test kernel completed successfully!
Halting CPU...
Guest halted after 306 exits
```

**주요 메트릭:**
- 커널 크기: 1.6KB (바이너리)
- VM exits: 306회 (대부분 hypercall)
- 메모리: 4MB 할당
- 실행 모드: Protected Mode with Paging

### 1K OS 포팅 시작 및 Hypercall 이슈

Week 11에서 1K OS를 x86 KVM VMM으로 포팅하는 작업을 시작했습니다. RISC-V 버전의 1K OS를 분석하고 x86 32-bit Protected Mode로 변환하는 과정에서 몇 가지 중요한 문제를 발견하고 해결했습니다.

#### 1K OS 아키텍처 분석

1K OS는 다음과 같은 구조로 구성됩니다:

```
Kernel (kernel.c):
- Process management (create_process, yield, switch_context)
- Memory allocator (alloc_pages, map_page)
- Syscall handling
- Tar filesystem (fs_init, fs_lookup)

User Space (user.c, shell.c):
- Shell process with command line parsing
- Basic commands: hello, echo, readfile, exit
- Syscall wrappers (getchar, putchar, etc.)
```

#### GDT/IDT 설정

x86 Protected Mode에서는 GDT(Global Descriptor Table)와 IDT(Interrupt Descriptor Table)가 필수입니다. 게스트 메모리 내부에 이들을 직접 구성했습니다:

**GDT 설정 (물리 주소 0x500):**
```c
static void setup_gdt(void) {
    struct gdt_entry *gdt = (struct gdt_entry *)(guest_mem + GDT_ADDR);
    
    // Null descriptor
    gdt[0] = (struct gdt_entry){0};
    
    // Kernel code segment (selector 0x08)
    gdt[1] = (struct gdt_entry){
        .limit_low = 0xFFFF,
        .base_low = 0,
        .base_mid = 0,
        .access = 0x9A,  // Present, Ring 0, Code, Readable
        .granularity = 0xCF,  // 4KB pages, 32-bit
        .base_high = 0
    };
    
    // Kernel data segment (selector 0x10)
    gdt[2] = (struct gdt_entry){
        .limit_low = 0xFFFF,
        .base_low = 0,
        .base_mid = 0,
        .access = 0x92,  // Present, Ring 0, Data, Writable
        .granularity = 0xCF,
        .base_high = 0
    };
    
    // User code/data segments 추가...
}
```

**IDT 설정 (물리 주소 0x528):**
```c
static void setup_idt(void) {
    struct idt_entry *idt = (struct idt_entry *)(guest_mem + IDT_ADDR);
    
    // 256 entries 모두 null로 초기화
    memset(idt, 0, 256 * sizeof(struct idt_entry));
}
```

KVM의 unrestricted guest 모드와 달리, 게스트 메모리에 실제 GDT/IDT를 구성하여 일반적인 Protected Mode 환경을 제공했습니다.

#### User Space 프로세스 생성

1K OS의 핵심 기능 중 하나는 user space 프로세스를 생성하고 관리하는 것입니다. Shell을 별도의 user process로 실행하기 위한 구현:

**1. 프로세스 페이지 테이블 설정**

User process는 다음과 같은 가상 메모리 레이아웃을 가집니다:
```
0x00000000 - 0x003FFFFF: Identity mapping (커널 공유)
0x01000000 - 0x01003FFF: User space (16KB)
0x80000000 - 0x803FFFFF: Kernel space (커널만 접근)
```

**2. Shell 바이너리 임베딩**

Shell 프로그램을 컴파일한 후 objcopy로 커널에 임베드:
```makefile
objcopy -I binary -O elf32-i386 -B i386 shell.bin shell.bin.o
ld -m elf_i386 -T kernel.ld -o kernel.elf \
    boot.o kernel.o shell.bin.o
```

링커가 제공하는 심볼을 통해 접근:
```c
extern char _binary_shell_bin_start[];
extern char _binary_shell_bin_size[];

struct process *shell_proc = create_process(
    _binary_shell_bin_start, 
    (size_t)_binary_shell_bin_size
);
```

**3. Context Switch 구현**

User space로 전환하기 위한 어셈블리 코드:
```c
__attribute__((naked)) void user_entry(void) {
    __asm__ volatile(
        "movl $0x01000000, %%eax\n\t"  // USER_BASE
        "jmp *%%eax\n\t"
        ::: "eax"
    );
}

// Bootstrap into shell
current_proc = shell_proc;
__asm__ volatile(
    "movl %0, %%cr3\n\t"      // Load shell's page table
    "movl %1, %%esp\n\t"      // Set stack pointer
    "jmp user_entry\n\t"
    :
    : "r" ((uint32_t)shell_proc->page_table),
      "r" (shell_proc->sp)
    : "memory"
);
```

#### 트러블슈팅: 스택 페이지 누락

Shell 프로세스가 시작 직후 Page Fault로 종료되는 문제가 발생했습니다.

**증상:**
```
Created shell process (pid=2)
Starting shell process...
Page Fault! addr=0x01003ffc
```

**원인 분석:**

Shell의 스택은 0x01003FFC에서 시작하는데 (아래로 성장), 처음에는 user code page (0x01000000-0x01000FFF)만 매핑하고 스택 페이지를 매핑하지 않았습니다.

**해결:**

User space에 총 4개 페이지를 매핑하도록 수정:
```c
void map_user_pages(struct process *proc) {
    // Page 0: Code/Data (0x01000000-0x01000FFF)
    map_page(proc, 0x01000000, phys_base, PAGE_PRESENT | PAGE_WRITABLE | PAGE_USER);
    
    // Page 1-3: Stack (0x01001000-0x01003FFF)
    for (int i = 1; i < 4; i++) {
        uint32_t vaddr = USER_BASE + i * 0x1000;
        uint32_t paddr = phys_base + i * 0x1000;
        map_page(proc, vaddr, paddr, PAGE_PRESENT | PAGE_WRITABLE | PAGE_USER);
    }
}
```

이 수정 후 Shell이 정상적으로 부팅되었습니다:
```
=== Kernel Initialization Complete ===
Starting shell process (PID 2)...

> 
```

#### Hypercall 문제: IN 명령어가 Trap되지 않음

Shell이 정상적으로 부팅된 후, getchar() syscall을 통해 사용자 입력을 받으려 할 때 심각한 문제가 발견되었습니다.

**증상:**

```
> [ch=0] [ch=0] [ch=0] [ch=0] [ch=0]
command line too long
```

Shell이 입력을 기다리지 않고 ch=0을 계속 읽어들여 즉시 "command line too long" 에러를 발생시켰습니다.

**원인 조사 1: VM Exit 로깅**

모든 VM exit의 종류를 로깅하는 코드를 추가했습니다:

```c
static int handle_vm_exit(vcpu_context_t *ctx) {
    static int io_count = 0;
    if (io_count++ < 100) {
        vcpu_printf(ctx, "IO[%d]: dir=%s port=0x%x size=%d\n",
                   io_count,
                   (ctx->kvm_run->io.direction == KVM_EXIT_IO_OUT) ? "OUT" : "IN",
                   ctx->kvm_run->io.port,
                   ctx->kvm_run->io.size);
    }
    // ...
}
```

**결과:**
```
IO[1]: dir=OUT port=0x500 size=1
IO[2]: dir=OUT port=0x500 size=1
...
IO[100]: dir=OUT port=0x500 size=1
GETCHAR[1] request, setting pending_getchar
GETCHAR[2] request, setting pending_getchar
...
```

**중요한 발견:** 10,001개의 VM exit가 모두 `KVM_EXIT_IO`였고, 그 중 **단 하나의 IN 명령어도 trap되지 않았습니다**. 모든 I/O exit이 `dir=OUT`이었습니다.

**원인 분석:**

원래 getchar() 구현은 다음과 같았습니다:

```c
long getchar(void) {
    long ch;
    __asm__ volatile(
        "movb $2, %%al\n\t"        // HC_GETCHAR
        "movw $0x500, %%dx\n\t"    // Port 0x500
        "outb %%al, %%dx\n\t"      // Trigger GETCHAR hypercall (OUT)
        "inb (%%dx), %%al\n\t"     // Read character from port (IN)
        "movsbl %%al, %0"          // Sign-extend AL to long
        : "=r"(ch)
        :
        : "al", "dx"
    );
    return ch;
}
```

OUT 명령어는 정상적으로 VM exit를 발생시키지만, **IN 명령어는 전혀 trap되지 않았습니다**. 이는 아마도 다음 이유 때문으로 추정됩니다:

1. **AMD SVM의 I/O 처리 방식:** AMD-V (SVM)에서는 IN 명령어가 특정 조건에서 trap되지 않을 수 있음
2. **CPL=0 권한:** 커널 코드가 Ring 0에서 실행되므로 IOPL 체크 없이 직접 I/O 실행 가능
3. **KVM 구현 세부사항:** 특정 I/O port range에 대한 최적화

**Workaround 시도 1: 메모리 기반 통신**

IN 명령어를 사용하는 대신, VMM이 결과를 게스트 메모리에 직접 쓰는 방식을 시도했습니다:

```c
// Guest kernel
#define HYPERCALL_RESULT_ADDR ((volatile int *)0x4000)

long getchar(void) {
    // Request via OUT
    __asm__ volatile(
        "movb $2, %%al\n\t"
        "movw $0x500, %%dx\n\t"
        "outb %%al, %%dx\n\t"
        ::: "al", "dx", "memory"
    );
    
    // Read result from memory
    return *HYPERCALL_RESULT_ADDR;
}
```

```c
// VMM
case HC_GETCHAR: {
    // Read character from stdin
    int ch = -1;
    if (select(...) > 0) {
        ch = getchar();
    }
    
    // Write to guest memory at physical 0x4000
    volatile int32_t *result_ptr = 
        (volatile int32_t *)(ctx->guest_mem + 0x4000);
    *result_ptr = (int32_t)ch;
    break;
}
```

**결과:**

VMM은 성공적으로 값을 쓰고 읽을 수 있었습니다:
```
Verification: VMM wrote -1, readback -1 at phys 0x4000
```

하지만 게스트는 여전히 0을 읽었습니다:
```
> [ch=0] [ch=0] [ch=0] ...
```

**Workaround 시도 2: TLB Invalidation**

캐시 일관성 문제를 의심하여 TLB invalidation을 추가했습니다:

```c
long getchar(void) {
    // Request via OUT
    __asm__ volatile(
        "movb $2, %%al\n\t"
        "movw $0x500, %%dx\n\t"
        "outb %%al, %%dx\n\t"
        ::: "al", "dx", "memory"
    );
    
    // Invalidate TLB for result address
    __asm__ volatile(
        "invlpg (%0)"
        :: "r" (HYPERCALL_RESULT_ADDR)
        : "memory"
    );
    
    return *HYPERCALL_RESULT_ADDR;
}
```

**결과:** 여전히 동일한 문제 발생

**근본 원인 추정:**

VMM userspace와 게스트 실행 컨텍스트 간의 **메모리 일관성 문제**입니다:

1. VMM이 `mmap`으로 매핑한 메모리에 값을 씀
2. 게스트 CPU는 자체 캐시/TLB를 가지고 있음
3. 일반적인 x86 캐시 일관성 프로토콜이 이 두 컨텍스트 사이에 적용되지 않음
4. 게스트는 stale value를 계속 읽음

#### 미해결 문제 및 향후 계획

현재 getchar() hypercall은 작동하지 않으며, 다음 해결 방법을 검토 중입니다:

**옵션 1: CPUID-based Hypercalls**
- CPUID 명령어는 항상 VM exit를 발생시킴
- 레지스터를 통한 양방향 통신 가능

**옵션 2: VMCALL/VMMCALL 명령어**
- 명시적 hypercall 명령어
- 표준적인 가상화 인터페이스

**옵션 3: kvm_run I/O data 영역 활용**
- KVM API의 공유 메모리 영역 사용
- 확실한 메모리 일관성 보장

**옵션 4: Interrupt 기반 Syscall**
- Port I/O 대신 INT 0x80 스타일 syscall
- IDT를 통한 trap

**옵션 5: KVM dirty page tracking**
- KVM의 메모리 동기화 메커니즘 활용
- 명시적 cache flush API 사용

이 문제는 x86 가상화의 저수준 메커니즘과 KVM의 메모리 세맨틱스에 대한 깊은 이해가 필요합니다. Week 12에서 이 문제를 해결하여 완전히 작동하는 1K OS를 구현할 계획입니다.

### 주요 학습 포인트

#### 1. KVM Unrestricted Guest Mode

KVM의 unrestricted guest 기능을 사용하면 GDT 없이 세그먼트 레지스터를 직접 설정할 수 있습니다. 이는 VMM에서 유용하지만, 게스트 코드가 세그먼트 레지스터를 재로드하려고 시도하면 문제가 발생합니다.

**핵심 교훈:**
- VMM이 unrestricted guest로 세그먼트 설정 시, 게스트는 세그먼트 재로드 금지
- 전통적인 Protected Mode 진입 시퀀스와 다른 초기화 방식
- 부트 코드 작성 시 VMM 환경 고려 필수

#### 2. x86 Page Table Structure

4MB PSE 페이지를 사용하면:
- 페이지 디렉토리만으로 충분 (페이지 테이블 불필요)
- PDE 플래그: bit 0 (Present), bit 1 (R/W), bit 7 (PS=Page Size)
- 간단한 구조로 디버깅 용이

#### 3. Higher-Half Kernel

Higher-half kernel 구조 (0x80000000 이상)의 장점:
- User space와 kernel space 명확한 분리
- 일반적인 OS 구조와 유사
- 향후 프로세스 관리 구현 시 유리

### 코드 통계

**추가된 파일:**
- `kvm-vmm-x86/src/main.c`: 897줄 추가/수정 (페이징 지원)
- `kvm-vmm-x86/os-1k/boot.S`: 52줄 (부트 코드)
- `kvm-vmm-x86/os-1k/kernel.ld`: 65줄 (링커 스크립트)
- `kvm-vmm-x86/os-1k/test_kernel.c`: 135줄 (테스트 커널)
- `kvm-vmm-x86/os-1k/Makefile`: 62줄 (빌드 시스템)
- `kvm-vmm-x86/os-1k/DESIGN.md`: 설계 문서

**VMM 기능:**
- Real Mode 지원 (기존)
- Protected Mode with Paging 지원 (신규)
- 멀티 vCPU 지원 (Week 10)
- 총 라인 수: ~1,200줄

## 다음 계획

### Week 12 목표

1. **1K OS 컴포넌트 포팅**
   - [ ] Process management (create_process, yield, switch_context)
   - [ ] Memory allocator (alloc_pages, map_page)
   - [ ] Exception/interrupt handling
   - [ ] Syscall 인터페이스

2. **Filesystem 구현**
   - [ ] Embedded tar filesystem
   - [ ] File I/O 함수들

3. **Shell 및 User Programs**
   - [ ] 간단한 shell 포팅
   - [ ] Echo, ls 등 기본 프로그램

### 추가 고려사항

- Performance 분석: KVM vs QEMU 비교
- Interrupt/Exception handling 개선
- 더 많은 테스트 케이스 작성

## 결론

**Week 11 완료 내용:**
1. 1K OS 프로젝트 분석 및 포팅 전략 수립
2. VMM에 Protected Mode with Paging 지원 추가
3. 4MB PSE 페이징 구현 (Identity + Higher-half mapping)
4. 테스트 커널 개발 및 검증 완료
5. Triple fault 디버깅 및 해결

**프로젝트 상태:**
- **완료**: Phase 0 (Bare-metal), Phase 1 (RISC-V KVM), Phase 2 (x86 Protected Mode + Paging)
- **진행 중**: Phase 3 (1K OS 포팅)
- **남은 기간**: 5주 (Week 12-16)

Week 11에서 1K OS 포팅의 핵심 기반인 Protected Mode with Paging 지원을 성공적으로 구현했습니다. 가장 큰 도전은 KVM unrestricted guest 모드에서의 세그먼트 처리 방식을 이해하는 것이었습니다. Triple fault 문제를 해결하면서 x86 Protected Mode의 세그먼트 메커니즘과 KVM의 가상화 방식에 대한 깊은 이해를 얻을 수 있었습니다.

다음 주에는 실제 1K OS 컴포넌트들을 포팅하여 완전한 작동 가능한 OS를 만드는 작업에 집중할 예정입니다.

## 참고 자료

- [Operating System in 1000 Lines](https://operating-system-in-1000-lines.vercel.app/en/)
- [Intel® 64 and IA-32 Architectures Software Developer's Manual](https://www.intel.com/content/www/us/en/developer/articles/technical/intel-sdm.html)
- [KVM API Documentation](https://www.kernel.org/doc/html/latest/virt/kvm/api.html)
- [OSDev Wiki - Paging](https://wiki.osdev.org/Paging)
- [OSDev Wiki - Higher Half Kernel](https://wiki.osdev.org/Higher_Half_Kernel)

---

## Week 12 - 교수님 미팅

# 12주차 상담내용 (11/18)

## 현황 보고

### 진행 상황

- Multi-instance 구현 완료 (최대 4개 VM 동시 실행)
- 각 vCPU별로 다른 게스트 프로그램 로드 가능
- 색상 코드로 출력 구분 (red, green, yellow, blue)
- 1K OS x86 포팅 진행 중 (Protected Mode with Paging 완료)

### 구현된 기능

- Pthread 기반 멀티 vCPU 실행
- Per-vCPU 메모리 격리 (각 256KB)
- Thread-safe 출력 (mutex 기반)
- Protected Mode + Paging 지원

### 데모 가능 구성

```bash
./kvm-vmm guest/multiplication.bin guest/counter.bin \
          guest/hello.bin guest/hctest.bin
```

4개의 서로 다른 프로그램이 병렬로 실행되며 색상으로 구분된 출력 생성

## 기술 논의

### 현재 이슈: Keyboard Input 처리

**문제 상황:**

- 1K OS에서 키보드 입력이 동작하지 않음
- IN 명령어가 VM exit를 발생시키지 않음
- OUT 명령어는 정상 동작 (hypercall 출력)
- 메모리 기반 통신도 cache coherency 문제로 실패

**근본 원인:**

호스트에서 키보드를 누르면 인터럽트가 Hypervisor로 전달되지만, Guest VM으로 injection되지 않음

### 해결 방안: Interrupt Injection

**아키텍처:**

```
Host Keyboard → Hypervisor Interrupt Handler
              → VM 선택 (VM 0번)
              → Guest로 Interrupt Injection
              → Guest Interrupt Handler 실행
```

**구현 단계:**

1. Hypervisor에서 키보드 인터럽트 감지 (select/poll)
2. 입력 문자를 버퍼에 저장
3. 대상 VM 결정 (Multi-instance 환경에서 VM 0번만 입력 받기)
4. KVM interrupt injection API 사용하여 Guest에 전달
5. Guest IDT를 통해 interrupt handler 실행

**참고 구현:**

- RISC-V hypervisor에 유사한 입력 포워딩 코드 존재
- 폴링 방식을 인터럽트 방식으로 변환 필요

### 구현할 인터럽트 종류

교수님 조언: "키보드 인터럽트랑 타이머 인터럽트 두 개만 일단 만들어 보시면 나머지는 다 구현이거든요. 나머지는 일의 양이 많은 거지 기술적 난이도는 낮을 거예요."

#### 1. Keyboard Interrupt

- Host stdin에서 키 입력 감지
- KVM_SET_INTERRUPT로 Guest에 전달
- 용도: Shell 명령어 입력, Echo 프로그램

#### 2. Timer Interrupt

- setitimer/timerfd로 주기적 인터럽트 생성
- Guest에 timer interrupt injection
- 용도: 시간 기반 애니메이션 (예: "Hello, World"를 1초마다 한 글자씩 출력)

이 두 인터럽트만 구현하면 나머지(disk I/O, network 등)는 같은 패턴을 따르면 됨

## 자율연구 포스터 발표 준비

### 일정

- **마감:** 11월 24일 (일요일)
- **리뷰:** 주말에 교수님께 초안 전송

### 포스터 구성

#### 1. 스크린샷 (전체의 1/3 ~ 1/2)

교수님 강조: "사람들이 지나가다 잘 모르거든요. 화면을 이쁘게 잘 만드셔야 돼요."

**포함할 내용:**

- Multi-instance 실행 화면 (4개 VM, 색상 구분)
- 1K OS 메뉴 시스템
- 각 프로그램 실행 결과

#### 2. 아키텍처 다이어그램

- Hypervisor 전체 구조
- Memory layout
- Interrupt flow

#### 3. 프로세스 설명

- VM 인스턴스 생성 과정
- Multi-instance 실행
- Interrupt injection 프로세스

#### 4. 기본 아이디어 (나머지 1/3 ~ 1/2)

- 프로젝트 목표
- 주요 기능
- 기술적 특징
- 응용 가능성

## 1K OS 데모 시나리오

### 메뉴 시스템 (VM 0에서 실행)

```
=== 1K OS on x86 KVM ===

Select program to run:
  1. Multiplication Table (2x1 ~ 9x9)
  2. Number Counter (0-9)
  3. Echo (repeat your input)
  4. Timer Hello (animated)

Choice: _
```

### I/O 라우팅

- VM 0번만 키보드 입력 받기
- 나머지 VM 1, 2, 3은 자동 실행
- 모든 출력은 하나의 콘솔에 색상 구분

교수님 조언: "그런 거는 문제 그런 거 하지 마시고 핵심적인 필요한 게 나은 게 중요하니까" (복잡한 화면 전환 구현하지 말 것)

### 구현 방식

- 1K OS 내부에 4개 모듈 구현
- Multiprocessing 없이 단일 프로세스로 처리
- 메뉴 선택에 따라 해당 함수 호출

## 성능 비교 실험

### 목표

KVM 기반 VMM vs QEMU emulation 성능 비교

### 주의사항

교수님 경고: "QEMU가 인스트럭션 바이 인스트럭션으로 에뮬레이션 하지 않고 블록 단위로 JIT 컴파일하기 때문에 성능 차이가 안 날 수도 있다"

**해결 방안:**

- QEMU에서 JIT 비활성화 옵션 찾기
- instruction-by-instruction mode 활성화
- 옵션이 없으면 결과에 설명 추가

## 이번 주(Week 12) 목표

### 필수 구현

1. **Keyboard Interrupt**
   - Host stdin 감지
   - KVM interrupt injection
   - Guest keyboard interrupt handler
   - 1K OS getchar() 동작 확인

2. **Timer Interrupt**
   - Host timer 설정
   - 주기적 Guest interrupt injection
   - Timer 기반 애니메이션 데모

3. **1K OS 메뉴 시스템**
   - 4가지 옵션 구현
   - 메뉴 선택 로직
   - 각 프로그램 모듈화

### 자율연구 포스터

- 스크린샷 촬영 및 편집
- 아키텍처 다이어그램 작성
- 프로세스 플로우 작성
- 레이아웃 디자인
- 교수님 리뷰 및 수정

### 선택 사항

- 성능 비교 실험 (KVM vs QEMU)

## 다음 미팅 예정

- 다음 주도 미팅 예정
- 포스터 발표 후 추가 개발 계획 논의
- 남은 몇 주간 추가 기능 구현 가능

## 요점 정리

### 교수 피드백

- "요거 한 번 키보드 인터럽트랑 타이머 인터럽트 두 개만 일단 만들어 보시면 나머지는 다 구현이거든요"
- "핵심적인 필요한 게 나은 게 중요하니까" (최소 기능 구현 집중)
- "화면을 이쁘게 잘 만드셔야 돼요" (포스터 시각적 요소 중요)

### 핵심 과제

- Interrupt injection 메커니즘이 전체 프로젝트의 핵심
- 이것만 성공하면 나머지는 같은 패턴으로 쉽게 확장
- 포스터는 기술적 세부사항보다 시각적 명확성 중요

### 우선순위

1. Keyboard + Timer interrupt 구현 (최우선)
2. 1K OS 메뉴 시스템 완성
3. 포스터 작성 및 리뷰
4. 성능 비교 (시간 되면)

## Week 12 - 연구 진행 내용

# 12주차 연구내용

## 이번 주 목표

### Week 12 상담 내용 (11/18)
- Keyboard/Timer interrupt 구현
- 1K OS 메뉴 시스템 완성
- 포스터 발표 준비 (11/24 마감)
- 성능 비교 실험 (선택)

## 구현 완료 사항

### 1. 1K OS 키보드 입력 단순화 (11/22)
**변경 내용**:
- 복잡한 keyboard interrupt handler 제거
- Hypercall 기반 `getchar()` 구현으로 전환
- Polling 방식으로 간소화

**코드 변경**:
```c
// 이전: Interrupt handler + circular buffer (100+ lines)
// 현재: Simple hypercall-based getchar (20 lines)

long getchar(void) {
    long ch;
    while (1) {
        __asm__ volatile(
            "movb $2, %%al\n\t"         // HC_GETCHAR
            "movw $0x500, %%dx\n\t"
            "outb %%al, %%dx\n\t"
            "movl %%eax, %0"
            : "=r"(ch)
            :
            : "eax", "edx"
        );
        if (ch != -1 && ch != 0xFFFFFFFF) {
            return ch & 0xFF;
        }
        for (volatile int i = 0; i < 10000; i++);
    }
}
```

**장점**:
- 코드 복잡도 감소 (114 lines → 20 lines)
- 유지보수 용이
- 안정성 향상 (interrupt race condition 제거)

**단점**:
- Polling 방식이라 CPU 사용률 증가
- Interrupt-driven보다 반응성 저하

### 2. 빌드 시스템 완성
**완료 항목**:
- VMM 빌드 (main.c → kvm-vmm)
- Guest 프로그램 빌드 (hello, counter, multiplication, fibonacci, hctest)
- 1K OS 빌드 (boot.S + kernel.c + shell.c → kernel.bin)

**빌드 명령어**:
```bash
# VMM
cd kvm-vmm-x86 && make vmm

# Guest programs
cd guest && ./build.sh

# 1K OS
cd os-1k && make
```

### 3. 성능 테스트 완료
**측정 결과**:
| 프로그램 | 실행 시간 | VM Exits | 비고 |
|---------|----------|----------|------|
| Hello World | < 10 ms | 13 | 빠른 I/O |
| Counter (0-9) | 24 ms | ~100 | 반복 hypercall |
| Multi-vCPU (4개) | < 50 ms | ~400 | 병렬 실행 |

**핵심 발견**:
- VM 초기화 시간: < 10ms
- Hypercall 오버헤드: ~0.8ms per call
- Near-native execution speed

### 4. 문서 작성 완료
**완성된 문서**:
1. **POSTER.md** (포스터 발표 자료)
   - 시스템 아키텍처 다이어그램
   - 주요 구현 기능 설명
   - 실행 예시 및 성능 측정 결과
   - 교육적 가치 강조

2. **performance_test.md** (성능 측정 결과)
   - 실제 측정 데이터
   - KVM vs QEMU 비교 (추정)
   - 성능 병목 분석
   - 최적화 방향 제시

3. **AGENTS.md** (빌드 가이드)
   - 빌드 명령어 정리
   - 코드 스타일 가이드
   - 프로젝트 노트

4. **README.md** (프로젝트 개요)
   - Quick Start 가이드
   - 완성 상태 표시
   - 문서 링크

## 기술적 결정 사항

### Keyboard Input 방식 변경
**초기 계획**: Interrupt injection 방식
- VMM에서 stdin 모니터링 → keyboard buffer 채움
- Guest에 keyboard interrupt injection
- Guest interrupt handler가 buffer에서 읽기

**최종 구현**: Hypercall 방식
- Guest가 hypercall로 직접 요청
- VMM이 keyboard buffer 확인 후 반환
- Polling loop로 blocking 구현

**변경 이유**:
1. 코드 복잡도 대폭 감소
2. Interrupt handler의 race condition 제거
3. 교육용 프로젝트에 적합한 간결함
4. 기능적으로는 동일한 결과

### Protected Mode 지원 현황
**완료**:
-  GDT/IDT 설정
-  Paging (4MB PSE pages)
-  Kernel/User mode separation
-  Process management basics
-  Timer interrupt handler

**미완료** (시간 부족):
-  Keyboard interrupt handler (hypercall로 대체)
-  Full syscall table
-  Disk I/O

## 프로젝트 통계

### 코드 규모
```
kvm-vmm-x86/src/main.c:      ~1,500 LOC
os-1k/kernel.c:               ~900 LOC (interrupt handler 제거 후)
os-1k/shell.c:                ~100 LOC
os-1k/common.c:               ~200 LOC
guest/*.S:                    ~500 LOC
────────────────────────────────────
Total:                       ~3,200 LOC
```

### Git 커밋 내역
```bash
$ git log --oneline --since="2025-11-17" --until="2025-11-24"
063ab2c Simplify 1K OS keyboard input to use HC_GETCHAR hypercall
fb7dc40 Add final report and performance test framework
c0d721f Phase 1-3: Implement keyboard/timer interrupts and 1K OS menu system
```

## 포스터 발표 준비 (11/24)

### 포스터 구성
1. **프로젝트 개요** (20%)
   - 목표: 교육용 초소형 VMM
   - 주요 특징: Multi-vCPU, Interrupt, 1K OS

2. **시스템 아키텍처** (30%)
   - 전체 구조 다이어그램
   - Host-Guest 상호작용
   - Memory layout

3. **실행 화면** (30%)
   - Multi-vCPU 실행 스크린샷
   - 1K OS 메뉴 시스템
   - 각 프로그램 출력

4. **성과 및 결론** (20%)
   - 성능 측정 결과
   - 교육적 가치
   - 향후 계획

### 핵심 메시지
> "간결하지만 완전한 기능을 갖춘 교육용 VMM"

**강조할 점**:
- 2,800 라인으로 완전한 VMM 구현
- Multi-vCPU 동시 실행
- Near-native performance (< 1ms hypercall overhead)
- 1K OS 포팅 성공 (Protected Mode + Paging)

## 남은 작업

### 필수 (11/23까지)
- [x] 포스터 초안 작성 (POSTER.md)
- [x] 성능 테스트 결과 정리
- [x] README 업데이트
- [ ] 최종 코드 정리
- [ ] Git push 및 원격 저장소 정리

### 선택 (시간 되면)
- [ ] 1K OS 메뉴 시스템 개선
- [ ] 추가 guest 프로그램 작성
- [ ] QEMU 성능 비교 실험

## 학습 내용

### KVM API 활용
1. **Multi-vCPU 관리**
   - 각 vCPU는 독립적인 pthread
   - Per-vCPU memory isolation
   - KVM_CREATE_VCPU로 vCPU 생성

2. **Interrupt Injection**
   - KVM_CREATE_IRQCHIP으로 interrupt controller 초기화
   - KVM_INTERRUPT로 interrupt injection
   - Guest IDT를 통해 handler 실행

3. **Memory Management**
   - KVM_SET_USER_MEMORY_REGION으로 메모리 매핑
   - GPA (Guest Physical Address) ↔ HVA (Host Virtual Address)
   - 4MB 단위 격리

### x86 Protected Mode
1. **Segmentation**
   - GDT 구성 (Null, Code, Data segments)
   - Selector 설정
   - LGDT 명령어

2. **Paging**
   - 2-level paging (Page Directory + Page Table)
   - 4MB PSE (Page Size Extension) pages
   - Identity mapping (0x0 - 0x400000)
   - CR0.PG, CR3 설정

3. **Interrupt Handling**
   - IDT 구성
   - Interrupt gate 설정
   - Naked function으로 handler 구현

## 시도했다가 되돌린 기능들

### Split-Screen 터미널 출력 (11/22 시도 → 11/22 되돌림)

**목표**: Multi-vCPU 출력을 ANSI escape codes로 터미널 화면 분할하여 표시

**구현 시도**:
- `kvm-vmm-x86/src/main.c`에 ~200 LOC 추가
- `--split` 플래그로 split-screen 모드 활성화
- 각 vCPU에 독립적인 터미널 영역 할당 (ASCII 박스 테두리)
- Per-vCPU 출력 버퍼링 및 ANSI cursor positioning

**문제점**:
1. **vCPU 0만 출력 표시**: pthread 동기화 문제로 다른 vCPU 출력 손실
2. **디버깅 복잡도**: Terminal escape codes로 디버그 출력 추적 어려움
3. **터미널 호환성**: 모든 터미널에서 ANSI codes 동작 보장 불가
4. **코드 복잡도 급증**: Main function이 1,391 LOC → 1,591 LOC

**결정: 되돌림 (Revert)**
- 이유: ANSI escape codes 방식이 적합하지 않음 판단
- Git 상태: `backup/split-screen-attempt` 브랜치로 백업 후 `main`을 f830456으로 리셋
- 결과: 코드는 clean state로 복구, 기존 color-coded interleaved 출력 유지

**대안 방안** (논의만, 미구현):
1. **Sequential output**: vCPU별 출력 버퍼링 후 순차 표시 (간단, 15분)
2. **External tmux**: 스크립트로 각 vCPU를 tmux pane에 표시 (VMM 수정 불필요)
3. **Enhanced color mode**: 현재 방식 개선 (prefix 추가 등)
4. **File-based output**: 각 vCPU를 파일로 출력, 별도 tail

**교훈**:
- Terminal UI는 VMM의 핵심 기능이 아님
- 간결함 > 화려한 시각화 (교육용 프로젝트)
- 기존 color-coded 출력도 충분히 multi-vCPU 병렬 실행을 보여줌

**커밋 히스토리**:
```
f830456 - Update AGENTS.md (현재 HEAD - clean state)
[split-screen 관련 6개 커밋 - backup/split-screen-attempt 브랜치에만 존재]
```

## 추가 개선 사항 (11/23)

### 5. Real Mode Guest Hang 버그 수정

**문제 발견**:
- Real Mode 게스트가 HLT 명령 실행 후 종료되지 않고 무한 대기
- Timer interrupt (IRQ0)가 100ms마다 게스트를 깨움
- 모든 모드에서 IRQCHIP이 활성화되어 있었음

**근본 원인**:
- Real Mode는 인터럽트를 처리할 수 없는데도 IRQ0 주입됨
- HLT 상태에서 깨어났지만 처리할 수 없어 다시 HLT 진입 반복
- 타이머 스레드가 계속 실행되어 프로그램이 종료되지 않음

**해결 방법**:
```c
// init_kvm()에 need_irqchip 파라미터 추가
bool init_kvm(bool need_irqchip) {
    // ... VM 초기화 ...
    
    // Protected Mode에서만 IRQCHIP 생성
    if (need_irqchip) {
        if (ioctl(vm_fd, KVM_CREATE_IRQCHIP) < 0) {
            perror("KVM_CREATE_IRQCHIP");
            return false;
        }
    }
    return true;
}

// main()에서 모드에 따라 분기
if (paging_mode) {
    init_kvm(true);   // Protected Mode: IRQCHIP 활성화
    start_timer_thread();
    start_keyboard_thread();
} else {
    init_kvm(false);  // Real Mode: IRQCHIP 비활성화
}
```

**효과**:
- Real Mode 게스트 즉시 종료 (< 100ms)
- Protected Mode는 기존처럼 인터럽트 기반 동작
- 모드별 명확한 책임 분리

관련 커밋: `13c71fe`

---

### 6. 출력 시스템 개선

#### vCPU별 색상 구분 개선
**변경 사항**:
- vCPU 0 색상: 빨강(Red) → 청록(Cyan)으로 변경
- 이유: 빨강은 오류 메시지로 오인될 수 있음
- 최종 색상: 청록/초록/노랑/파랑 (중립적이고 구분 명확)

**단일/다중 vCPU 출력 조건부 처리**:
```c
if (num_vcpus > 1) {
    // 다중 vCPU: 색상으로 구분
    const char *colors[] = {"\033[36m", "\033[32m", "\033[33m", "\033[34m"};
    printf("%s[vCPU %d:%s]%s ", colors[id], id, name, "\033[0m");
} else {
    // 단일 vCPU: 깔끔한 출력 (색상 없음)
    printf("[%s] ", name);
}
```

**버퍼링 제거**:
- Character-by-character 즉시 출력
- 다중 vCPU 병렬 실행 효과가 더 명확히 보임

관련 커밋: `4472af7`, `b67ed68`

---

### 7. Verbose 모드 추가

**기능**:
```bash
./kvm-vmm --verbose guest/hello.bin       # Real Mode + verbose
./kvm-vmm --paging --verbose os-1k/kernel.bin  # Protected Mode + verbose
```

**출력 내용**:
- VM exit 원인 및 횟수
- I/O 포트 접근 (IN/OUT)
- 하이퍼콜 상세 정보 (번호, 인자, 반환값)

**예시**:
```
[hello] VM exit #1: KVM_EXIT_IO (port=0x500, size=1, dir=OUT)
[hello] Hypercall: HC_PUTCHAR (0x01), arg=0x48 ('H')
[hello] VM exit #2: KVM_EXIT_IO (port=0x500, size=1, dir=OUT)
[hello] Hypercall: HC_PUTCHAR (0x01), arg=0x65 ('e')
```

**특수 처리**:
- `HC_GETCHAR` (IN) 하이퍼콜은 verbose 모드에서도 억제
- 이유: 1K OS에서 너무 빈번하게 호출되어 출력 범람 방지

관련 커밋: `befd8ac`

---

### 8. 1K OS 입력 시스템 대폭 개선

#### readline() 함수 추가
**기능**:
- 즉각적인 문자 에코 (사용자 피드백)
- Backspace/DEL 지원 (0x08, 0x7F)
- Buffer overflow 방지
- 자동 입력 truncation 및 경고

**구현** (`user.c`):
```c
int readline(char *buf, int bufsz) {
    int pos = 0;
    while (pos < bufsz - 1) {
        int ch = getchar();
        if (ch == '\n' || ch == '\r') {
            putchar('\n');
            buf[pos] = '\0';
            return pos;
        } 
        else if (ch == 0x08 || ch == 0x7F) {  // Backspace
            if (pos > 0) {
                pos--;
                putchar('\b'); putchar(' '); putchar('\b');  // Erase
            }
        }
        else if (ch >= 0x20 && ch < 0x7F) {  // Printable
            putchar((char)ch);
            buf[pos++] = (char)ch;
        }
    }
    // ... buffer full handling ...
}
```

#### Echo 프로그램 리팩터링
**변경 전** (30+ 줄):
- 수동 character-by-character 루프
- 경계 검사 누락
- Backspace 미지원

**변경 후** (15 줄):
```c
static void echo_demo(void) {
    printf("\n=== Echo Program (type 'quit' to exit) ===\n");
    while (1) {
        printf("$ ");
        char line[64];
        int len = readline(line, sizeof(line));
        
        if (strcmp(line, "quit") == 0) {
            printf("Exiting echo program\n");
            break;
        } else {
            printf("Echo: %s\n", line);
        }
    }
}
```

#### Calculator 프로그램 리팩터링
**변경 전**:
- 복잡한 character-by-character 파싱 (80+ 줄)
- 입력 버퍼 문제로 "10 + 5" 입력 시 "10"만 읽고 에러

**변경 후**:
- `readline()`로 전체 줄 읽기
- 한 번에 파싱 (더 간단하고 안전)
- 표현식 정상 처리: "10 + 5" → Result: 15

#### 메뉴 입력 개선
**문제**: 메뉴 선택 후 남은 줄바꿈이 다음 입력으로 전달됨

**해결**:
```c
char choice = getchar();
putchar(choice);  // 즉시 에코

// 남은 문자 모두 소비
char ch;
while ((ch = getchar()) != '\n' && ch != -1) {
    putchar(ch);
}
printf("\n");
```

**효과**:
- 일관된 입력 동작
- 향상된 사용자 경험
- 안전한 버퍼 처리
- 유지보수 용이

관련 커밋: `630b688`

---

### 9. 문서 업데이트

**업데이트된 문서**:
1. `README.md`: 새 실행 방식 및 개선사항 반영
2. `docs/데모가이드.md`: 색상 변경, 실행 명령어 업데이트
3. `docs/최종보고서.md`: 최근 개선사항 섹션 추가
4. 모든 마크다운 파일에서 이모지 제거 (전문성 향상)

관련 커밋: `c5b67e6`, `b67ed68`, `b2e396b`, `bb24e1f`

---

### 기술적 결정: 실행 방식 개선

**변경 전**:
```bash
make run-hello
make run-multi-2
make run-1k-os-multiplication
```

**변경 후**:
```bash
./kvm-vmm guest/hello.bin
./kvm-vmm guest/multiplication.bin guest/counter.bin
./kvm-vmm --paging os-1k/kernel.bin
./kvm-vmm --verbose --paging os-1k/kernel.bin
```

**장점**:
- 더 직관적이고 유연함
- 플래그 조합 가능 (`--verbose`, `--paging`)
- 임의의 게스트 조합 실행 가능
- Makefile에 덜 의존적

---

## 다음 주 계획 (Week 14: 12/1)

### 최종 정리
1. 코드 정리 및 주석 추가
2. 최종 보고서 작성
3. 시연 영상 녹화 (선택)
4. Git 저장소 정리

### 추가 개선 (선택)
1. Quiet mode 추가 (디버그 출력 비활성화)
2. 추가 guest 프로그램
3. Documentation 개선

## 결론

**Week 12 성과**:
-  1K OS 입력 시스템 단순화 완료
-  전체 빌드 시스템 안정화
-  성능 테스트 및 문서화 완료
-  포스터 발표 자료 작성 완료

**프로젝트 상태**: **Feature Complete** 

**남은 작업**: 최종 정리 및 문서화 (Week 14-16)

---

**작성일**: 2025-11-22  
**다음 미팅**: 2025-11-25 (포스터 발표 후)

---

## Week 13 - 교수님 미팅

# 13주차 상담내용 (11/25)

## 현황 보고

### 진행 상황

- **프로젝트 완성 확인**: 교수님께 모든 기능 구현 완료 보고
- Protected Mode + Paging 완전 구현 (4KB 페이징)
- 키보드 입력 처리 완료 (인터럽트 기반)
- 성능 측정 결과: 네이티브 실행 대비 거의 동일한 속도
- 최종 발표 준비 완료 (다음 주 포스터 발표)

### 구현된 기능

- Protected Mode with 4KB Paging
- 키보드 인터럽트 처리 (에코 기능 포함)
- 타이머 인터럽트 (기본 구현)
- Hypercall 시스템 (포트 0x500 사용)
- 대화형 실행 환경 (`make` 명령어로 즉시 실행)

### 데모 가능 구성

```bash
make              # 대화형으로 1K OS 실행
./kvm-vmm --paging os-1k/kernel.bin
```

## 기술 논의

### 페이징 구현 완료

**구현 내용:**

- 4KB 페이징 사용 (x86 표준)
- 2-level page table 구조
- Identity mapping 방식

**확인 사항:**

- "4MB 페이지는 x86에서 안 쓴다" (문서 오타 지적)
- Protected Mode 완전 구현 확인
- 성능이 네이티브 수준임 확인

### 키보드 입력 처리 구현

**문제 상황 (Week 12):**

- IN 명령어가 VM exit 발생시키지 않음
- 키보드 입력이 게스트에 전달 안 됨

**최종 구현:**

- 인터럽트 기반 키보드 입력 처리
- Echo 기능 구현 (화면에 입력 표시)
- Flushing 및 동적 버퍼링 문제 해결

**남은 이슈:**

- 타이머 인터럽트는 "hack 수준" 구현
- 완전한 구현보다는 동작하는 수준에 우선순위

### Hypercall 인터페이스

**구현 방식:**

- 포트 0x500 사용
- "Hypervisor in 1K lines" 프로젝트 컨벤션 따름
- 상수 기반 통신 시스템
- 표준 컨벤션 준수
- 간결하고 효율적인 설계

## Linux 부팅 가능성 논의

### 교수님 질문

"페이징이 되면 리눅스도 뜨지 않아요?"

**Linux 부팅에 필요한 추가 인프라:**

1. **APIC** (Advanced Programmable Interrupt Controller)
   - 인터럽트 라우팅 및 관리
   - 멀티코어 환경 지원

2. **ACPI** (Advanced Configuration and Power Interface)
   - 전원 관리 및 하드웨어 설정
   - Device tree 생성 및 로딩 필요

3. **디바이스 드라이버 인프라**
   - 각종 하드웨어 에뮬레이션
   - 상당한 추가 구현 필요

### 결정 사항

**현 시점 결론:**

- Linux 부팅은 프로젝트 범위를 넘어섬
- 현재 구현으로도 충분한 교육적 가치 달성
- 향후 확장 과제로 남겨둠
- 현재 구현으로 발표 및 보고서 진행

## 포스터 발표 준비

### 발표 형식

- 포스터 형태 (라이브 데모 컴퓨터 없음)
- 스크린샷 및 결과물 기반 발표
- 다음 주 발표 예정

## 이번 주(Week 13) 목표

### 완료된 작업

- [x] 프로젝트 완성 확인
- [x] 교수님 데모 시연
- [x] 성능 측정 및 검증
- [x] 최종 발표 자료 준비

### 남은 작업

- [ ] 발표 준비 (다음 주)
- [ ] 최종 보고서 정리
- [ ] 소스 코드 주석 보완
- [ ] 문서화 마무리

## 이번 주(Week 13) 완료 사항

- [x] 모든 핵심 기능 구현 완료
- [x] 성능 목표 달성 (네이티브 수준)
- [x] 실제 동작하는 1K OS 완성
- [x] Multi-vCPU 병렬 실행 가능
- [x] 프로젝트 완성 확인

### 최종 단계

- 최적화 및 문서화
- 코드 주석 보완
- 발표 및 보고서 마무리

## 다음 미팅 예정

발표 후 (12월 초 예정)

## Week 13 - 연구 진행 내용

# 13주차 연구내용

## 이번 주 목표

### Week 13 상담 내용 (11/25)
- 프로젝트 완성 확인 및 데모
- 최종 발표 준비 (포스터 영상 제출)
- 문서화 및 코드 정리
- 버그 수정 및 사용성 개선

## 구현 완료 사항

### 1. 치명적 버그 수정: Multi-vCPU DS Segment 설정 오류 (11/28)

**문제 상황**:
- Multi-vCPU 환경에서 vCPU 1-3의 출력이 누락됨
- `hello.bin` 실행 시 vCPU 0만 출력되고 나머지는 출력 없음
- 메모리 접근은 정상이지만 데이터 세그먼트 접근 실패

**근본 원인**:
```c
// 이전 코드 (버그)
vcpu->sregs.ds.base = 0;  // vCPU 1-3도 base가 0
vcpu->sregs.ds.limit = 0xFFFFFFFF;

// CS는 제대로 설정됨
vcpu->sregs.cs.base = vcpu_id * mem_size;  // vCPU별로 다름
```

**해결 방법**:
```c
// 수정된 코드
vcpu->sregs.ds.base = vcpu_id * mem_size;  // CS base와 동일하게
vcpu->sregs.ds.limit = 0xFFFFFFFF;
vcpu->sregs.es.base = vcpu_id * mem_size;
vcpu->sregs.ss.base = vcpu_id * mem_size;
```

**영향**:
- Multi-vCPU 데모가 완전히 동작하게 됨
- 모든 vCPU의 출력이 정상적으로 표시됨
- Color-coded output이 제대로 작동

**커밋**: `a40a33c` - fix(vmm): Fix multi-vCPU output and improve usability

### 2. Fibonacci Guest 프로그램 수정 (11/28)

**문제 상황**:
- Fibonacci 결과값이 부정확
- Hypercall 호출 규약 불일치
- 출력 형식이 다른 guest와 달라 가독성 저하

**수정 내용**:
```assembly
# 이전: 레지스터 관리 불일치
movb $1, %al          # HC_PUTCHAR
movw $0x500, %dx
# EDX 값이 보존되지 않음

# 수정 후: 레지스터 올바르게 보존
pushl %edx
movb $1, %al
movw $0x500, %dx
outb %al, %dx
popl %edx
```

**개선 사항**:
- 올바른 피보나치 수열 출력 (0, 1, 1, 2, 3, 5, 8, 13, 21, 34...)
- 숫자 → ASCII 변환 로직 수정
- 줄바꿈 형식 통일

**커밋**: `360b3e9` - fix(guest): Fix fibonacci guest output

### 3. 키보드 입력 안정화: Interrupt Injection 제거 (11/28)

**문제 상황**:
- `KVM_INTERRUPT` ioctl 사용 시 triple fault 발생
- IRQCHIP 없이 interrupt injection 시도하여 guest 크래시
- Protected mode에서 간헐적 hang 발생

**근본 원인**:
```c
// 잘못된 코드
if (ioctl(vcpu->fd, KVM_INTERRUPT, &irq) < 0) {
    // IRQCHIP이 없는데 interrupt를 inject하려 함
}
```

**해결 방법**:
- IRQ injection 코드 완전 제거
- Hypercall 기반 polling 방식으로 통일
- stdin monitoring thread는 유지 (버퍼 채우기용)

**장점**:
- Triple fault 문제 해결
- 모든 게스트에서 안정적 동작
- 코드 복잡도 감소 (39줄 → 15줄)

**커밋**: `0f51f43` - fix(vmm): Enable stdin thread and remove interrupt injection

### 4. Arch Linux 빌드 문제 해결 (11/27)

**문제 상황**:
- Fedora에서 빌드한 `kernel.bin`은 정상 동작
- Arch Linux에서 빌드한 버전은 부팅 실패 (triple fault)
- 동일한 코드, 동일한 Makefile인데 결과가 다름

**원인 분석**:
```bash
# Fedora GCC: i686 기본값
$ gcc -m32 -march=native → i686 코드 생성

# Arch GCC: i386 기본값
$ gcc -m32 -march=native → i386 코드 생성 (일부 명령어 미지원)
```

**해결 방법**:
```makefile
# os-1k/Makefile에 명시적 아키텍처 지정
CFLAGS += -march=i686

# 추가 링커 플래그
LDFLAGS += --no-pie --build-id=none -z norelro
```

**결과**:
- Arch Linux에서도 정상 빌드 및 실행
- Binary 크기: 13K → 12K (더 최적화됨)
- 백업: `backups/working-fedora-build/` 디렉토리 생성

**문서화**: `docs/investigations/arch_vs_fedora_build_issue.md` 추가

**커밋**: `f1832b9` - fix(os-1k): Fix Arch Linux build to work on Zen 5 KVM

### 5. 사용성 개선: 확장자 제거 (11/28)

**변경 내용**:
- Guest 바이너리 파일에서 `.bin` 확장자 제거
- Tab completion이 더 편리하게 개선

**이전**:
```bash
./kvm-vmm guest/hello.bin guest/counter.bin guest/multiplication.bin
```

**개선 후**:
```bash
./kvm-vmm guest/hello guest/counter guest/multiplication
# Tab으로 자동완성 시 더 빠름
```

**추가 기능**:
- Multi-vCPU 실행 시 컬러 범례 자동 출력
- vCPU 0: Cyan, vCPU 1: Green, vCPU 2: Yellow, vCPU 3: Blue

**커밋**: `a40a33c` - fix(vmm): Fix multi-vCPU output and improve usability

### 6. 4KB Paging으로 전환 (11/27)

**변경 내용**:
- 기존 4MB PSE (Page Size Extension) 페이징에서 4KB 표준 페이징으로 변경
- x86 표준 2-level page table 구조 사용

**이유**:
- 4MB 페이지는 x86 표준이 아님
- 교육적 목적: 표준 페이징 메커니즘 학습
- 더 세밀한 메모리 관리 가능

**구현**:
```c
// Page Directory Entry (4KB 페이지 사용)
pde = page_table_addr | 0x07;  // Present, R/W, User (PSE 비트 없음)

// Page Table Entry
for (int i = 0; i < 1024; i++) {
    page_table[i] = (i * 4096) | 0x07;  // 4KB 단위 매핑
}
```

**커밋**: `f1832b9`의 일부

## 버그 수정 및 마이너 개선

### 7. 터미널 입출력 개선 (11/23)

**문제점들**:
1. Raw mode에서 줄바꿈이 제대로 출력되지 않음
2. 사용자 입력이 중복으로 echo됨
3. Shell 프롬프트가 바로 표시되지 않음 (버퍼링 문제)

**해결**:
```c
// 1. Raw mode에서 OPOST 유지 (줄바꿈 처리)
termios.c_oflag |= OPOST;  // 출력 후처리 유지

// 2. Echo 비활성화
termios.c_lflag &= ~(ECHO | ICANON);

// 3. flush_output() 함수 추가
void flush_output(void) {
    fflush(stdout);
}
```

**커밋들**: `b486d0f`, `e0e35fe`, `a911f08`, `6f9e072`

### 8. 코드 정리 및 리팩토링 (11/23)

**작업 내용**:
- 큰 함수들을 작은 함수로 분리
- 모든 마크다운 파일에서 이모지 제거 (지침 준수)
- `.gitignore` 업데이트
- Guest 빌드 스크립트 수정

**개선**:
- 코드 가독성 향상
- 유지보수 용이
- 빌드 시스템 안정화

**커밋들**: `bd3b57f`, `bb24e1f`, `84fce1c`

### 9. 문서 작업 (11/23 - 11/28)

**최종 문서 완성**:
1. `docs/최종보고서.md` - 프로젝트 전체 요약
2. `docs/벤치마크_결과.md` - 성능 측정 및 분석
3. `docs/기술평가.md` - 코드 품질 및 설계 결정
4. `docs/데모가이드.md` - 실행 방법 및 예제
5. `docs/1K_OS_설계.md` - 1K OS 아키텍처 설명

**문서 구조 개편**:
- 모든 문서를 `docs/` 폴더로 이동
- README 간소화
- 인덱스 페이지 추가 (`docs/INDEX.md`)

**커밋**: `cf5d47d`, `7dd2909`, `1d2e66c`

## Week 13 이후 작업 (11/30 - 12/02)

### 10. Long Mode (64-bit) 지원 추가 (12/01)

**동기**:
- 교수님이 Linux 부팅 가능성 언급
- Linux는 64-bit Long Mode 필요
- 교육 확장 과제로 시도

**구현 내용**:
```c
// 1. CR4.PAE 활성화
vcpu->sregs.cr4 |= X86_CR4_PAE;

// 2. IA32_EFER.LME 설정
struct kvm_msrs *msrs;
msrs->entries[0].index = MSR_EFER;
msrs->entries[0].data = EFER_LME | EFER_LMA;

// 3. 4-level page table 구조 생성
setup_long_mode_paging();

// 4. 64-bit GDT 설정
setup_long_mode_gdt();
```

**현재 상태**:
- 인프라 구현 완료
- `KVM_SET_SREGS` 오류 해결
- 64-bit guest 바이너리 로드 가능

**남은 작업**:
- Guest 64-bit 코드 테스트
- 실제 Linux 부팅 시도 (Phase 2)

**커밋들**: `59a9d93`, `26b37d0`, `a4dbb00`

### 11. Linux Boot Protocol 준비 (12/02)

**Phase 2 작업 시작**:
- Linux boot protocol 헤더 파싱
- 초기 RAM disk (initrd) 로딩 구조
- Zero page 설정

**새로운 파일들**:
- `kvm-vmm-x86/src/linux_boot.c` - Boot protocol 처리
- `kvm-vmm-x86/src/linux_boot.h` - 구조체 정의

**현재 상태**: 초기 인프라만 구현, 실제 부팅 미완성

**커밋들**: `b870a62`, `0054786`

### 12. 디버깅 인프라 추가 (12/01)

**목적**:
- Long Mode 및 Linux 부팅 디버깅 지원
- 레지스터 상태 덤프
- 메모리 내용 검사

**기능**:
```c
void dump_registers(struct kvm_vcpu *vcpu);
void dump_memory(void *guest_mem, uint64_t addr, size_t len);
void dump_segment(struct kvm_segment *seg, const char *name);
```

**커밋**: `8554e3b` - feat(debug): Add comprehensive debugging infrastructure

## 프로젝트 통계

### 최종 코드 규모
```
kvm-vmm-x86/src/main.c:       ~1,500 LOC
kvm-vmm-x86/src/debug.c:        ~200 LOC (새로 추가)
kvm-vmm-x86/src/paging_64.c:   ~150 LOC (새로 추가)
kvm-vmm-x86/src/linux_boot.c:  ~100 LOC (새로 추가)
os-1k/kernel.c:                 ~900 LOC
os-1k/shell.c:                  ~100 LOC
os-1k/common.c:                 ~200 LOC
guest/*.S:                      ~600 LOC (fibonacci 수정 포함)
────────────────────────────────────
Core (Week 13까지):           ~3,300 LOC
Extension (Long Mode):          ~450 LOC
────────────────────────────────────
Total:                         ~3,750 LOC
```

### Git 활동 통계
```bash
$ git log --since="2025-11-23" --until="2025-12-02" --oneline | wc -l
78

$ git log --since="2025-11-23" --until="2025-11-26" --oneline | wc -l
41  # 11/23-26: 프로젝트 완성 및 최종 정리

$ git log --since="2025-11-27" --until="2025-11-28" --oneline | wc -l
25  # 11/27-28: 버그 수정 및 발표 준비

$ git log --since="2025-11-30" --until="2025-12-02" --oneline | wc -l
7   # 12/01-02: Long Mode 확장 작업
```

### 주요 커밋들 (Week 13)

**11/23 (프로젝트 완성)**:
- `773f737` - chore: Final polish and cleanup for project completion
- `13c71fe` - Fix VMM hang issue and improve single/multi-vCPU output
- `630b688` - feat: Add readline function with echo and backspace support
- `08ab07d` - docs: Add comprehensive benchmark results and performance analysis
- `36b27c5` - docs: Add technical evaluation document

**11/24 (포스터 준비)**:
- `86b4342` - feat: Add final poster and supporting assets
- `e9dbffe` - chore: Remove poster creation guide files after submission

**11/27 (주요 버그 수정)**:
- `f1832b9` - fix(os-1k): Fix Arch Linux build to work on Zen 5 KVM
- `fb367a6` - Add triple fault investigation report and debug test files

**11/28 (사용성 개선)**:
- `a40a33c` - fix(vmm): Fix multi-vCPU output and improve usability
- `360b3e9` - fix(guest): Fix fibonacci guest output
- `0f51f43` - fix(vmm): Enable stdin thread and remove interrupt injection
- `cf5d47d` - docs: Reorganize and standardize documentation structure

**12/01-02 (확장 작업)**:
- `59a9d93` - feat(long-mode): Implement 64-bit Long Mode support infrastructure
- `26b37d0` - feat(long-mode): Integrate 64-bit Long Mode setup into main VMM
- `a4dbb00` - fix(long-mode): Resolve KVM_SET_SREGS issue for 64-bit Long Mode

## 기술적 결정 사항

### Interrupt Injection 포기 결정

**초기 계획 (Week 12)**:
- KVM_INTERRUPT ioctl로 키보드/타이머 인터럽트 전달
- Guest IDT handler로 처리

**실제 구현 (Week 13)**:
- Hypercall 기반 polling 방식 유지
- IRQ injection 코드 완전 제거

**변경 이유**:
1. **안정성**: Triple fault 문제 완전 해결
2. **단순성**: 코드 복잡도 대폭 감소 (39줄 → 15줄)
3. **충분성**: 교육용 목적에 적합한 수준
4. **이식성**: IRQCHIP 의존성 제거

### 4KB vs 4MB Paging

**교수님 피드백**:
- "4MB 페이지는 x86에서 안 쓴다"
- 문서 오타 지적

**최종 결정**:
- 4KB 표준 페이징으로 변경
- 2-level page table 구조
- 더 표준적이고 교육적인 구현

### Long Mode 확장 작업

**동기**:
- Linux 부팅 가능성 탐색
- 프로젝트 완성 후 개인 학습 목표

**접근 방식**:
- Phase 0: 디버깅 인프라 (12/01 완료)
- Phase 1: Long Mode 기본 (12/01 완료)
- Phase 2: Linux Boot Protocol (12/02 진행 중)
- Phase 3: Linux 실제 부팅 (미완성)

**현실적 평가**:
- APIC/ACPI 구현 필요 (상당한 작업량)
- Device tree 생성 및 로딩 필요
- 교육 과제 범위를 넘어섬

## 학습 내용

### 1. x86 Segmentation의 중요성

**발견**:
- CS base만 설정하고 DS base를 잊으면 데이터 접근 실패
- 모든 세그먼트 레지스터(CS, DS, ES, SS)의 base가 일치해야 함

**교훈**:
- x86 아키텍처의 역사적 복잡성
- Real mode에서 Protected mode 전환 시 주의사항
- KVM API의 저수준 특성

### 2. Toolchain 차이의 영향

**발견**:
- 동일한 소스 코드도 컴파일러 설정에 따라 다르게 동작
- Arch GCC vs Fedora GCC의 기본 타겟 차이

**교훈**:
- 크로스 플랫폼 빌드의 어려움
- 명시적 아키텍처 지정의 중요성
- 바이너리 호환성 문제

### 3. KVM API의 제약사항

**발견**:
- IRQCHIP 없이 `KVM_INTERRUPT` 사용 시 triple fault
- Long Mode 설정 시 MSR과 SREGS 순서 중요

**교훈**:
- Low-level virtualization API 사용의 복잡성
- 에러 메시지만으로 디버깅이 어려움
- 문서와 레퍼런스 구현 병행 필요

### 4. 점진적 개발의 중요성

**Week 13의 접근 방식**:
1. 버그 수정 → 안정화
2. 사용성 개선 → 완성도
3. 문서화 → 전달력
4. 확장 기능 → 학습 심화

**교훈**:
- 완성도 높은 작은 시스템이 불완전한 큰 시스템보다 가치 있음
- 각 단계에서 동작하는 버전 유지
- 실험적 기능은 별도 브랜치/플래그로 관리

## 남은 작업

### 프로젝트 완성 기준 (11/30까지)

- [x] 모든 핵심 기능 구현
- [x] 주요 버그 수정
- [x] 문서화 완료
- [x] 데모 및 확인 완료
- [x] 포스터 발표 자료 제출

### 확장 작업 (선택, 12월 이후)

- [x] Long Mode 인프라 (Phase 0-1)
- [x] 디버깅 도구
- [ ] Linux Boot Protocol 완성 (Phase 2)
- [ ] APIC/ACPI 구현 (Phase 3)
- [ ] 실제 Linux 커널 부팅 (Phase 4)

## 결론

**Week 13 성과**:
- 프로젝트 완성 확인
- 치명적 버그 수정 (Multi-vCPU DS segment)
- 크로스 플랫폼 빌드 문제 해결 (Arch/Fedora)
- 사용성 및 문서 개선
- 네이티브 수준 성능 달성 확인
- 확장 작업 시작 (Long Mode)

**프로젝트 상태**: 완성 (Feature Complete)

**향후 방향**:
- Linux 부팅 지원 (개인 학습 목표)
- 코드 리뷰 및 최적화
- 추가 guest 프로그램 개발
- 교육 자료로 활용

---

## Week 14 - 교수님 미팅

# 14주차 상담내용 (12/02)

## 진행 상황 검토

비디오 출력 성공: 텍스트 모드 비디오 출력까지는 구현에 성공했으나, 이후 리눅스 부팅 프로세스 구현에서 난항을 겪고 있음.

하드웨어 의존성 문제: 리눅스 커널 부팅을 위해 ACPI, APIC, Timer, Disk Driver(ATA), File System 등 방대한 하드웨어 의존성을 한 번에 해결해야 하는 상황에 직면함.

Initramfs 구현의 어려움: 초기 램 디스크(initramfs)와 디바이스 드라이버 로딩 방식에 대한 개념적 혼란 및 구현 복잡도 증가.

## 기술적 피드백 및 조언

교수님께서는 현재 범용 리눅스를 목표로 하는 접근 방식이 현실적으로 어렵다고 판단하시고, 타겟 하드웨어를 좁히고 우선순위를 재조정하는 전략을 제시하셨습니다.

## 타겟 하드웨어 축소

모든 하드웨어를 지원하는 범용 OS가 아닌, QEMU 가상 머신의 특정 스펙에 딱 맞춘 OS를 목표로 해야 함.

복잡한 최신 하드웨어 스펙 대신, QEMU가 제공하는 레거시(Legacy) 인터페이스나 시리얼 포트 등을 활용하여 난이도를 낮출 것.

## 드라이버 로딩 전략 수정

Initramfs를 통한 동적 모듈 로딩은 현재 단계에서 불필요한 복잡성을 야기함.

타겟 디바이스(QEMU)가 정해져 있으므로, 필요한 드라이버를 커널 이미지 안에 정적으로(Static) 포함시켜 initramfs 구현 단계를 건너뛰는 것을 권장.

## 개발 우선순위 재설정

현재 가장 시급한 것은 스케줄러의 기반이 되는 **타이머(Timer)**와 인터럽트(Interrupt) 구현임.

메모리 맵 확인 -> 인터럽트/타이머 활성화 -> 스케줄러 -> 쉘(Shell) 진입 순으로 로드맵을 단순화할 것.

## 향후 계획

맨땅에 헤딩하는 현재 방식보다는, 교육용으로 검증된 PintOS 프로젝트를 통해 OS의 핵심 원리(스케줄러, 메모리 관리 등)를 체계적으로 학습하기로 결정.

## Week 14-15 - 연구 진행 내용

# 14-15주차 연구내용

## 쉬어가는 기간

작지만 상도 받았고, 미팅도 끝났고, 목표도 어느정도 이뤘으니 14주와 15주는 쉬어가는 기간으로 했습니다.
---

## Week 16 - 연구 진행 내용

# 16주차 연구노트 (마지막 주)

목표: Mini-KVM 위에서 Linux(bzImage) 부팅 로그를 **UART(COM1)** 로 출력하고, initramfs까지 연결해 “부팅이 진행된다”는 데모를 만든다.

저번주(week14-15) 방향:

- 범용 하드웨어 지원은 포기하고 타겟을 극단적으로 축소한다.
- initramfs “구현”을 피하고, 가능한 한 정적 포함/최소 장치로 간다.
- 타이머/인터럽트 기반(IRQCHIP)부터 안정화한다.

관련 계획 문서:

- `research/week16/linux_boot_plan.md`

---

## 이번 주에 실제로 한 일(구현)

### 1) Linux 부팅 모드 확장

- Linux 모드에서 게스트 메모리를 256MB로 확장 (`kvm-vmm-x86/src/main.c`)
- Linux 모드에서 `KVM_CREATE_IRQCHIP` 강제 활성화 (IRQ 라우팅/PIT 에뮬 기대)
- UART COM1(0x3f8~0x3ff) I/O 처리 추가
  - OUT(THR)로 나오는 바이트를 stdout으로 출력
  - IN(LSR)은 항상 “송신 가능(THRE|TEMT)” 반환

### 2) Linux boot protocol 파라미터 처리 보강

- bzImage에서 파싱한 setup header(`boot_params->hdr`)를 `setup_linux_boot_params()`가 덮어쓰지 않도록 수정
  - 기존에는 `memset(boot_params, 0, ...)` 후 header가 날아가서 `code32_start`가 0으로 보이는 문제가 있었음
- `--initrd <file>` 옵션 추가:
  - initrd를 게스트 메모리 0x04000000(64MB)에 적재
  - `boot_params->hdr.ramdisk_image/ramdisk_size/initrd_addr_max` 갱신

### 3) 디버깅을 위한 VM exit 핸들링 추가

- Linux 경로에서 보이던 `KVM_EXIT_MMIO`를 최소 처리(읽기는 0으로 채움, 쓰기는 무시)
- 일부 exit reason 추가 처리(무조건 실패하지 않도록)

---

## 실험 기록(재현 가능한 커맨드)

### 준비: 호스트 커널/이니트램프를 프로젝트 폴더로 복사

권한 문제(`/boot`의 initramfs는 root-only)가 있어서 파일을 프로젝트 쪽으로 복사해서 사용했다.

```bash
cd kvm-vmm-x86
sudo cp /boot/vmlinuz-6.17.11-300.fc43.x86_64 ./bzImage
sudo cp /boot/initramfs-6.17.11-300.fc43.x86_64.img ./initrd.img
sudo chmod 644 ./initrd.img
```

### 실행(부팅 시도)

```bash
cd kvm-vmm-x86
make vmm
./kvm-vmm --debug 2 --linux ./bzImage --initrd ./initrd.img --cmdline \
  "console=ttyS0 earlycon=uart,io,0x3f8,115200 rd.shell rd.debug loglevel=8 root=/dev/ram0 acpi=off noapic nolapic pci=off nokaslr panic=-1"
```

관찰:

- KVM/VM 생성 및 bzImage 로드, boot params/E820 세팅, initrd 적재까지는 정상 로그가 출력됨.
- 실행 직후에는 아래 두 가지 케이스를 관찰했다.
  1) (초기 실모드 진입 시도) 의미 없는 포트 I/O(예: `port=0xfff1` 등)가 반복되며 진행이 멈춘 듯 보임(earlycon 출력 없음).
  2) (코드32 진입 실험 시도) 대량의 `KVM_EXIT_MMIO`가 발생하고 최종적으로 `KVM_EXIT_INTERNAL_ERROR(suberror=0x3)`로 종료.

---

## 문제 분석(현재 막힌 지점)

### A) 실모드 진입 경로에서 “BIOS 의존/레거시 포트” 가능성

- bzImage의 setup 코드는 일부 레거시 초기화(A20, 8042, CMOS 등)를 기대할 수 있다.
- 현재 VMM은 UART 외 포트 I/O를 대부분 무시하며, **IN에 대한 기본값도 제한적**이라 부팅 코드가 polling에서 멈출 가능성이 있다.

추가 관찰:

- `objdump`로 bzImage setup 일부를 확인해보니 실제로 `sti` 이후 `int 0x0` 같은 인터럽트 호출이 들어있었다.
- 이는 IVT(0x0000)와 BIOS 인터럽트 핸들러가 존재한다는 전제를 깔고 동작할 가능성이 커서, “펌웨어/BIOS 없이 KVM만으로 setup 코드를 그대로 실행”하는 접근은 리스크가 크다.

### B) 코드32 진입 경로에서 “부팅 상태/boot_params 전달” 불일치 가능성

- 코드32로 바로 점프하면 Linux가 기대하는 레지스터/세그먼트/boot_params 전달 방식이 정확해야 한다.
- boot_params 포인터/내용이 어긋나면 커널이 잘못된 물리주소로 접근해 MMIO 폭주로 이어질 수 있다(관찰된 0xbdfe... 대 물리주소 write).

---

## 다음 계획(개선안) + 바로 할 일

1) **포트 I/O 기본값 보강**
   - 알 수 없는 IN은 0으로 채워서 “대기 루프가 풀릴 여지”를 만든다.
   - A20(0x92), 8042(0x60/0x64), PIC(0x20/0x21/0xa0/0xa1), POST(0x80), CMOS(0x70/0x71) 최소 에뮬을 추가한다.
2) **MMIO/IO 로깅 제한**
   - 현재는 MMIO가 폭주하면 로그가 과도해 디버깅 자체가 어려워진다(상한선을 둔다).
3) **실모드(setup 0x90200) 경로 우선 안정화**
   - “setup 코드가 code32_start로 넘어가서 earlycon을 찍는 단계”까지를 1차 목표로 한다.
4) (플랜 B) 코드32 직접 진입 재시도
   - 위가 막히면, boot protocol 문서 기준으로 보호모드 진입 상태를 더 정확히 맞춰서 재시도한다.

---

## 추가 진행(업데이트)

### 1) 메모리 레이아웃/boot_params 처리 수정

- bzImage setup 코드를 `0x90000`이 아니라 `0x10000`으로 적재하도록 변경
  - 기존에는 `boot_params`를 `0x90000`에 통째로 `memcpy` 하면서 setup 코드 영역을 덮어써서, 실모드/코드32 둘 다 불안정해질 가능성이 높았음.
- `boot_params`는 guest RAM의 `0x90000`(zero page) 위치에 직접 구성하도록 변경(호스트 malloc → 게스트 memcpy 제거).
- 로컬 테스트용 `bzImage/initrd.img`는 **커밋 대상에서 제외**되도록 `.gitignore` 추가.

### 2) Linux 진입 전략을 실험할 수 있도록 옵션 추가

- `--linux-entry setup|code32` 추가
  - `code32`: 보호모드(페이징 OFF)에서 `code32_start`로 직접 진입(기본값).
- `--linux-rsi base|hdr` 추가
  - Linux 초기 진입 코드가 `ESI`를 어떻게 해석하는지 실험하기 위한 옵션.
  - 관찰상 `base`는 즉시 MMIO 폭주(`0xbdfdf...`) 패턴으로 이어지는 경우가 많았고,
    `hdr`는 단일 스텝 디버그에서 더 “멀리” 진행(대신 특정 지점에서 triple fault).

### 3) 디버그/관찰 개선

- `--debug 3`에서 KVM single-step 디버그를 개선:
  - 보호모드에서는 `RIP` 전체로 선형주소를 계산하도록 수정(기존 real-mode 가정으로 잘못된 주소를 찍던 문제 수정).
  - SHUTDOWN(triple fault) 시점에 “마지막 single-step 상태”를 함께 출력하도록 개선.

### 4) 최신 관찰(가장 최근 실패 지점)

- `--linux-entry code32 --linux-rsi hdr --debug 3` 조합에서,
  Linux decompressor가 진행되다가 `rep stosd`(`bytes=f3 ab ...`) 수행 중 triple fault로 리셋되는 패턴을 관찰.
- 마지막 single-step 상태 예시(요지):
  - `RIP=0x1000a9`, `RCX=0x1800`, `RDI=0xfdf000`, `IDT.base=0x11e4280`, `IDT.limit=0xff`
  - 즉, 커널이 자체적으로 IDT를 구성/설정한 뒤, 메모리 초기화 루틴(추정) 진행 중 예외로 리셋되는 형태.

---

## 다음 계획(개선)

1) **“왜 triple fault가 나는지” 원인 식별을 최우선으로**
   - 단일 스텝에서 마지막 상태만으로는 예외 번호를 확정하기 어려움.
   - 커널이 설정한 IDT(예: `IDT.base=... limit=...`)가 실제로 유효한지(엔트리/핸들러 주소) 확인하는 로깅을 추가한다.
2) **Linux entry 조건을 boot protocol에 더 엄밀히 맞추기**
   - `ESI` 의미(boot_params vs setup_header), stack/segment 초기 상태를 문서 기준으로 다시 대조.
   - 필요하면 `--linux-rsi` 외에 관련 레지스터/세그먼트 값을 로그로 고정 출력.
3) **테스트 커널을 바꿔서 변수 줄이기**
   - 호스트 최신 배포판 커널 대신, (가능하면) 부팅 경로가 단순한 “작은 bzImage”로 재현을 시도해 원인 범위를 좁힌다.

---

## 최종 업데이트(부팅 성공)

### 1) root cause: boot_params(zero page) 레이아웃이 틀려서 커널이 헤더 필드를 잘못 읽고 있었음

- 기존 `struct boot_params`가 실제 Linux boot protocol의 오프셋(`hdr@0x1f1`, `e820_map@0x2d0`)과 달라서,
  커널이 `boot_params->hdr.init_size` 같은 필드를 **깨진 값으로 해석**하고 비정상 주소 접근/트리플폴트로 이어졌음.
- `kvm-vmm-x86/src/linux_boot.h`에서 zero page를 4KB로 고정하고, 오프셋을 `_Static_assert`로 검증하도록 수정.

### 2) initrd 적재 주소를 “고정 64MB”에서 “상단 메모리”로 변경

- Fedora 커널의 `init_size`가 큰 편이라, initrd를 64MB에 고정 적재하면 **커널 footprint와 겹쳐 initrd가 덮어써짐**.
- `kvm-vmm-x86/src/linux_boot.c`에서 initrd를 `mem_size` 상단 쪽(4KB align-down)으로 배치하고,
  `KERNEL_LOAD_ADDR + init_size` 이후에만 놓이도록 체크를 추가.

### 3) userspace 콘솔 출력/입력 지원: COM1 IRQ4 최소 동작 추가

- 커널 printk는 polling으로도 출력되지만, userspace(`/init`/쉘)는 8250 드라이버의 **IRQ 기반 TX/RX**에 의존해 출력이 막힐 수 있었음.
- `kvm-vmm-x86/src/main.c`에서
  - UART RX: stdin 입력을 버퍼링하고 `IRQ4`를 pulse
  - UART TX: `IER.THRE` 활성화 시점/THR write 시점에 `IRQ4`를 pulse
  - `IIR/LSR`에 RX-ready/THRE 상태를 최소 반영
  을 추가해서 `/bin/sh` 프롬프트까지 출력됨을 확인.

### 4) initramfs 빌드 스크립트 추가(커밋에 바이너리 포함 X)

- `/boot`의 initramfs는 권한(0600)으로 읽을 수 없는 경우가 있어, 프로젝트 내부에서 재현 가능한 최소 initramfs를 생성하도록 함.
- `kvm-vmm-x86/tools/mkinitramfs.sh`가
  - `kvm-vmm-x86/initramfs/init.c`를 빌드하여 `/init`로 넣고
  - 호스트의 `bash` + 필요한 공유 라이브러리를 포함한 `initramfs.cpio`를 생성한다.
- 추가로 `kvm-vmm-x86/initramfs/miniutils.c`를 빌드해 `/bin/{ls,cat,uname}` 등을 제공하고,
  `/init`에서 `PATH=/bin:...` 설정 + 쉘 종료 시 respawn 하도록 해서 데모 UX를 안정화했다.

### 재현 커맨드(현재 성공 조합)

```bash
cd kvm-vmm-x86
make vmm
./tools/mkinitramfs.sh initramfs.cpio
./kvm-vmm --linux ./bzImage --initrd ./initramfs.cpio --linux-entry code32 --linux-rsi base --cmdline \
  "console=ttyS0 earlycon=uart,io,0x3f8,115200 loglevel=4 nokaslr"
```

관찰(요지):

- 커널 부팅 로그 출력 후, userspace에서 아래 메시지와 함께 쉘 프롬프트 진입:
  - `[mini-kvm] userspace init started`
  - `sh-5.3#`

### 남은 한계(범위 밖으로 둔 것)

- 디스크/virtio/PCI/ACPI 테이블 없음 → “완전한 배포판 부팅”은 목표 범위 밖.
- initramfs는 최소 구성(현재는 `bash` 기반)이라 배포판 수준 유틸리티는 없고, 데모용으로 `miniutils`(uname/ls/cat 등)만 포함.
